D:\omniverse\pkg\isaac-lab\isaaclab\_isaac_sim\exts\omni.isaac.ros2_bridge\humble\lib

NVIDIA Edify能否让您一夜之间成为3D视觉大师
NVIDIA推出Edify：3D AI视觉内容创作的革命
- NVIDIA Edify推出尖端的3D AI技术，可转变数字创意，并为视觉内容创作提供直观的控制。

演示NVIDIA audio2face2d该模型能够生成一个完全动画的视频与嘴部运动从只是一个肖像图片输入和语音输入。这可以扩展到与文本到语音和maxine语音技术一起工作，以从大型语言模型（LLM）创建文本到动画内容。

export isaac_sim_package_path=$HOME/omniverse/pkg/isaac-sim-4.1.0
export RMW_IMPLEMENTATION=rmw_fastrtps_cpp
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$isaac_sim_package_path/exts/omni.isaac.ros2_bridge/humble/lib


## NVIDIA Omniverse Stream | Getting Started - Deformable Bodies + Cloth Simulation

https://docs.omniverse.nvidia.com/extensions/latest/ext_physics/physics-particles.html

在这个直播中，您将了解Omniverse如何支持可变形物体和布料模拟，因为我们演示了几个有趣的场景，并提供了即将推出的功能预览。

NVIDIA Omniverse™是一个开放式平台，专为虚拟协作和实时逼真模拟而构建。当用户和团队连接设计工具、资产和项目以在虚拟世界中进行协作迭代时，复杂的创建者、设计师和工程可视化工作流将发生转变。

## Speed Up Your Workflow With the Marvelous Designer Omniverse Connector | Omniverse Live
For all of you fashion designers & creators out there, this livestream at 11am PST is going to be a lot of fun as we show and tell Marvelous Designer with very special guests from CLO Virtual Fashion (creators of Marvelous Designer) and the Marvelous Designer community!

对于所有的时装设计师创作者在那里，这个直播在太平洋标准时间上午11点将是一个很大的乐趣，因为我们显示，并告诉奇妙的设计师与非常特殊的客人从CLO虚拟时尚（奇妙的设计师的创作者）和奇妙的设计师社区！

NVIDIA Omniverse是一个可扩展的开发平台，用于构建和操作自定义3D管道和工业虚拟世界应用程序。基于通用场景描述（OpenUSD），Omniverse从根本上改变了复杂的3D工作流程，允许个人和团队连接不同的3D工具和数据集，并为工业和科学用例模拟大规模、物理精确的虚拟世界。

## GPU-Based Simulation of Cloth Wrinkles at Submillimeter Levels
Huamin Wang. 2021. GPU-Based Simulation of Cloth Wrinkles at Submillimeter Levels. ACM Transactions on Graphics (SIGGRAPH), vol. 40, no. 4, pp. 169:1--169:14.

In this paper, we study physics-based cloth simulation in a very high resolution setting, presumably at submillimeter levels with millions of vertices, to meet perceptual precision of our human eyes. State-of-the-art simulation techniques, mostly developed for unstructured triangular meshes, can hardly meet this demand due to their large computational costs and memory footprints. We argue that in a very high resolution, it is more plausible to use regular meshes with an underlying grid structure, which can be highly compatible with GPU acceleration like high-resolution images. Based on this idea, we formulate and solve the nonlinear optimization problem for simulating high-resolution wrinkles, by a fast block-based descent method with reduced memory accesses. We also investigate the development of the collision handling component in our system, whose performance benefits greatly from the grid structure. Finally, we explore various issues related to the applications of our system, including initialization for fast convergence and temporal coherence, gathering effects, inflation and stuffing models, and mesh simplification. We can treat our system as a quasistatic wrinkle synthesis tool, run it as a standalone dynamic simulator, or integrate it into a multi-resolution solver as an additional component. The experiment demonstrates the capability, efficiency and flexibility of our system in producing a variety of high-resolution wrinkles effects.
王华民。2021.基于GPU的亚毫米级织物褶皱模拟。ACM Transactions on Graphics（SIGGRAPH），vol. 40，no. 4，pp. 169：1 169：14

在本文中，我们研究了基于物理的布料模拟在一个非常高的分辨率设置，大概在亚毫米级与数百万个顶点，以满足我们的人眼的感知精度。最先进的模拟技术，主要是为非结构化三角形网格，很难满足这一需求，由于其巨大的计算成本和内存占用。我们认为，在非常高的分辨率下，使用具有底层网格结构的规则网格更合理，这可以像高分辨率图像一样与GPU加速高度兼容。基于这一思想，我们制定和解决的非线性优化问题，模拟高分辨率皱纹，通过一个快速的基于块的下降方法，减少内存访问。我们还调查了我们的系统中的冲突处理组件，其性能受益于网格结构的发展。 最后，我们探讨了与我们的系统的应用程序相关的各种问题，包括快速收敛和时间一致性，聚集效应，膨胀和填充模型，网格简化的初始化。我们可以把我们的系统作为一个准静态皱纹合成工具，运行它作为一个独立的动态模拟器，或集成到一个多分辨率求解器作为一个额外的组件。实验证明了我们的系统在产生各种高分辨率皱纹效果方面的能力，效率和灵活性。

## Omniverse Tutorial: Animate a Wind Turbine
How to set up joints (revolute joints/prismatic joints) in Omniverse?

Download Asset for this tutorial:

https://github.com/DigitalBotLab/WindMill

Creators of Omniverse Tools, Captivating Simulations and Robot AI training

Visit our website
https://digitalbotlab.com/

NVIDIA Omniverse is a scalable, end-to-end platform for building and operating metaverse applications. Based on Universal Scene Description (USD), Omniverse fundamentally transforms complex 3D workflows, allowing individuals and teams to connect and customize 3D pipelines and simulate large-scale, physically accurate virtual worlds for industrial and scientific use cases.

Download Omniverse today → https://nvda.ws/3gNZsu0
如何在Omniverse中设置关节（旋转关节/平移关节）？

下载本教程的资源：

https://github.com/DigitalBotLab/Wind网站... 

Omniverse工具，迷人的模拟和机器人AI培训的创造者

访问我们的网站
https://digitalbotlab.com/

NVIDIA Omniverse是一个可扩展的端到端平台，用于构建和操作Metaverse应用程序。Omniverse基于通用场景描述（USD），从根本上改变了复杂的3D工作流程，允许个人和团队连接和自定义3D管道，并模拟工业和科学用例的大规模物理精确的虚拟世界。

## Omniverse Tutorial: Build a Rotating Moon
https://www.youtube.com/watch?v=vf0fVs4B1tM&list=PLjHofDVCYx5DXLb8iKrHsQ_oJJgBb8tse&index=3

Creators of Omniverse Tools, Captivating Simulations and Robot AI training
Download Asset for this tutorial:

https://github.com/DigitalBotLab/MoonToy


NVIDIA Omniverse is a scalable, end-to-end platform for building and operating metaverse applications. Based on Universal Scene Description (USD), Omniverse fundamentally transforms complex 3D workflows, allowing individuals and teams to connect and customize 3D pipelines and simulate large-scale, physically accurate virtual worlds for industrial and scientific use cases.

Download Omniverse today → https://nvda.ws/3gNZsu0
Omniverse工具，迷人的模拟和机器人AI培训的创造者

## Omniverse Tutorial: Explore fluid scenes
What are the fluid set-up in Omniverse?
Creators of Omniverse Tools, Captivating Simulations and Robot AI training

## Omniverse Tutorial: Build Your First Scene - Toy Clock
https://youtu.be/_9s6sweh91Y?list=PLjHofDVCYx5DXLb8iKrHsQ_oJJgBb8tse

Creators of Omniverse Tools, Captivating Simulations and Robot AI training
资源下载地址：
https://github.com/DigitalBotLab/ToyClock

## Omniverse Tutorial: Make a Pendulum Clock
资源下载地址：
https://github.com/DigitalBotLab/ToyClock
NVIDIA Omniverse is a scalable, end-to-end platform for building and operating metaverse applications. Based on Universal Scene Description (USD), Omniverse fundamentally transforms complex 3D workflows, allowing individuals and teams to connect and customize 3D pipelines and simulate large-scale, physically accurate virtual worlds for industrial and scientific use cases.

## Omniverse Tutorial: Set up a USD scene using the Asset Store
How to set up a USD using the assets from Omniverse Asset Store?

Creators of Omniverse Tools, Captivating Simulations and Robot AI training

## Omniverse Tutorial: How to animate an articulated body/robot
What to animate an articulated body/ robot with joints in Omniverse?

Creators of Omniverse Tools, Captivating Simulations and Robot AI training


## 3 Easy VFX to Get Started with Simulations in OMNIVERSE!
https://www.youtube.com/watch?v=NyjD3EBjDnc

Mike walks us through 3 VFX simulations you can do for FREE in NVIDIA Omniverse.

Download NVIDIA Omniverse today, for free: https://nvda.ws/3QvMhyn

Learn more about the HP ZBook Studio G9: https://nvda.ws/3RRomKA

ProductionCrate's Browser Extension (FREE!) http://bit.ly/ProductionCrateConnect
Learn To Rotoscope Pro Course: http://bit.ly/31ybOi2
Subscribe to The Craters (our second channel): https://bit.ly/2XFgbHC

Not a part of the ProductionCrate Community yet? We got your back -
https://bit.ly/2ADcR4O Pro Membership gets you full access!

FOLLOW US
  / productioncrate  
  / productioncrate  
  / productioncrate  
  / productioncrate  

Check Out our DISCORD:   / discord  

All VFX can be found at https://footagecrate.com
All Music and Sound FX can be found at https://SoundsCrate.com

If you need original music for your project contact our friends at https://www.suitetracks.com

Saturday Morning Tutorials Episode Guide: https://news.productioncrate.com/satu...

VFX simulations，即视觉特效模拟，是Visual Effects（VFX）领域的一个重要组成部分。VFX，即视觉特效，是指通过创造图像和处理真人拍摄范围以外的镜头，结合传统SFX技术与数码化特殊效果，将CGI（三次元计算机图形）与DIP（二次元实际影像）混合使用，以创造出逼真传神的特技场面。这些特技场面包括但不限于火灾、废土、烟雾、爆炸、撞击等，这些在现实生活中无法看到或在拍摄时无法布置的场景。

在VFX simulations中，模拟技术起着至关重要的作用。通过先进的计算机图形软件和算法，艺术家们能够模拟出各种复杂的物理现象和自然现象，如流体动力学、刚体破碎、粒子系统等。这些模拟技术不仅能够帮助艺术家们创造出逼真的特效场景，还能够提高制作效率，减少制作成本。

具体来说，VFX simulations可能包括以下几个方面：

物理模拟：通过物理引擎模拟物体的运动、碰撞、破碎等物理现象。例如，在模拟爆炸场景时，可以使用物理引擎来计算爆炸产生的冲击力对周围物体的影响，以及物体在受到冲击后的运动轨迹和破碎效果。
流体模拟：模拟水、火、烟雾等流体的运动和形态。这些流体模拟技术可以创造出逼真的水流、火焰、烟雾等效果，为电影、游戏等视觉媒体增添震撼力。
粒子系统：使用粒子系统来模拟尘埃、烟雾、雨滴等微小物体的运动和分布。粒子系统可以根据预设的规则和参数生成大量的粒子，并通过计算粒子的运动轨迹和相互作用来模拟出复杂的效果。
光照和渲染：在VFX simulations中，光照和渲染技术也是至关重要的。通过精确的光照模拟和高质量的渲染技术，可以使得特效场景更加逼真、生动。
综上所述，VFX simulations是视觉特效制作中的一个重要环节，它利用先进的计算机图形技术和算法来模拟各种复杂的物理现象和自然现象，为电影、游戏等视觉媒体创造出逼真的特效场景。


## Exporting Simulations from Houdini to Nvidia Omniverse (USD Composer)
In this Nvidia Omniverse tutorial, I'll show you how to export your simulations / animations from Houdini to Nvidia Omniverse - USD Composer (formerly Create) for real time rendering using the latest Houdini Connector. Previously when exporting simulations using Alembic Sequences from 3ds Max / TyFlow or Houdini, the file sizes would be massive (in the hundreds of GB's) and fairly slow to playback, but using the Nvidia Omniverse Houdini Connector we can export directly to USD and have our color data and simulation data brought directly into Omniverse with a much smaller file size. In this case, one frame of this lego sim exported as an FBX was 25 Million Polygons for reference - so it's a super fast workflow for high poly geometry.

I'm currently working on more Houdini simulations to showcase this workflow for an upcoming Nvidia Omniverse Live Stream on Wed 24th, 2023, so stay tuned!
   • 3D Scene Setup With SideFX Houdini an...  

For more Houdini Tutorials, 3ds Max, and TyFlow content, visit:
https://edstudios.ca/

I'm going to make these Houdini project files available on my Patreon here:
  / edstudios  

## Simulating cloth with TyFlow & rendering with Nvidia Omniverse Create in real time
https://www.youtube.com/watch?v=jL6oYSIQVWg

Using 3ds Max and TyFlow, I created this basic cloth simulation, and then exported the simulation via USD into Nvidia Omniverse Create for real time RTX rendering. Stay tuned for a TyFlow tutorial on how to create the cloth simulation, and how to set up lighting and how to render out the animation in Nvidia Omniverse Create.

## Creating cloth simulations with TyFlow & 3ds Max - Tutorial
In this 3ds Max & Tyflow tutorial I'll show you how to create cloth simulations and export your simulation via USD into Nvidia Omniverse Create for real time rendering. First, we'll create the cloth pieces using box geometry. Then using the birth objects operator and a cloth bind operator in TyFlow, we can convert that geometry into cloth particles, and can apply different forces to create abstract animations of cloth pieces floating around in our scene affected by wind, gravity, and noise. We can then export our TyFlow simulation, by using the export particles operator to export a tycache of our cloth simulation, apply a turbosmooth modifier, and export as USD. In Nvidia Omniverse Create, I'll show you how to open the USD file, add lighting, and a camera. I'll also show you how to use post-production effects like bloom, motion blur, chromatic abberation, and global atmospheric effects like fog. Using a few keyframes to animate the lighting and camera, we can then finish off our scene. Lastly, I'll show you how to export your final motion graphics animation using the Render Queue Dialog. I rendered my animation as an image sequence in the EXR format, and brought it into After Effects for final compositing.

This tutorial demonstrates how to create a cloth simulation in 3ds Max using TyFlow and then export it to Nvidia Omniverse for rendering. The video covers setting up the simulation, adding forces, exporting as a USD file, and then importing it into Omniverse to add lighting, cameras, and materials.

https://youtu.be/iPZHOjK32xk

## NVIDIA Omniverse Beginner Tutorial
Some of yall have been asking for one so here ya go! Enjoy!

00:00 - 00:59 Installing Omniverse Launcher
1:00-1:29 Installing Machinima
1:30-2:38 Installing Nucleus
2:39 - 12:46 - NVIDIA Omniversre Machinima UI Layout, Movements Etc.
12:47 - 14:02 Paint Tool
14:03 - 15:34 Importing Characters 
15:35 - 19:21 Video to AI Body Animation
19:22 - 23:15 Light options
23:16 - End Audio to AI Body Animation

## NVIDIA Omniverse Path Tracing Car Animation
In this video we will be taking a look at a quick car animation i made in a couple of hours using NVIDIA Omniverse Path Tracing

## Google's RT-2-X Generalist AI Robots_ 500 Skills, 150,000 Tasks, 1,000,000+ Workflows.webm
谷歌 RT-2-X 通用型人工智能机器人：500 种技能、150,000 项任务、1,000,000 多个工作流程
谷歌 DeepMind 与学术合作伙伴共同发布了一种人工智能，利用 "Open X-Embodiment "数据集训练机器人执行通用任务。另一方面，ConceptGraphs 提供了一种新的三维场景表示法，通过将视觉和语言相结合，提高了机器人的感知和规划能力。

## TILOS Seminar: Large Datasets and Models for Robots in the Real World
TITLE: Large Datasets and Models for Robots in the Real World

SPEAKER: Nicklas Hansen, UC San Diego

ABSTRACT: Recent progress in AI can be attributed to the emergence of large models trained on large datasets. However, teaching AI agents to reliably interact with our physical world has proven challenging, which is in part due to a lack of large and sufficiently diverse robot datasets. In this talk, I will cover ongoing efforts of the Open X-Embodiment project–a collaboration between 279 researchers across 20+ institutions–to build a large, open dataset for real-world robotics, and discuss how this new paradigm is rapidly changing the field. Concretely, I will discuss why we need large datasets in robotics, what such datasets may look like, and how large models can be trained and evaluated effectively in a cross-embodiment cross-environment setting. Finally, I will conclude the talk by sharing my perspective on the limitations of current embodied AI agents, as well as how to move forward as a community.

BIO: Nicklas Hansen is a Ph.D. student at University of California San Diego advised by Prof. Xiaolong Wang and Prof. Hao Su. His research focuses on developing generalist AI agents that learn from interaction with the physical and digital world. He has spent time at Meta AI (FAIR) and University of California Berkeley (BAIR), and received his B.S. and M.S. degrees from Technical University of Denmark. He is a recipient of the 2024 NVIDIA Graduate Fellowship, and his work has been featured at top venues in machine learning and robotics.

## What Makes USD Unique in NVIDIA Omniverse
<!-- This video introduces the Pixar's Universal Scene Description (USD) file format and explains the basics of its structure and introduces layers, references, and sublayers. We explore how larger scenes can be represented using these components.

NVIDIA Omniverse™ is an open platform built for virtual collaboration and real-time photorealistic simulation. Complex creator, designer, and engineering visual workflows are transformed as users and teams connect design tools, assets, and projects for collaborative iteration in a virtual world.

深度解析 USD：在 NVIDIA Omniverse 中构建高效的虚拟协作与仿真

本视频详细介绍 Pixar 的 Universal Scene Description (USD) 文件格式及其在 NVIDIA Omniverse 平台中的独特应用。
作为一款专为虚拟协作和实时逼真模拟设计的开放平台，Omniverse 支持用户和团队连接设计工具、资产和项目，在虚拟世界中实现协同迭代。
视频首先介绍了 USD 的基本结构，包括层（Layers）、引用（References）和子层（Sublayers），并探讨了如何使用这些组件表示复杂场景。
USD 支持多种文件格式，包括高效的二进制格式 USDC、人类可读的文本格式 USDA、封装资源的 USDZ 以及通过插件扩展的其他格式，以满足不同场景下的需求。
随后，重点讲解了 USD 中的 LIVRPS（Local, Inherits, Variants, References, Payloads, Specializes）规则，这一规则决定了属性意见值的优先级顺序，确保在多层堆栈中对属性或元数据的意见进行一致性和可预测性的解析。
通过可视化决策树，观众可以直观了解意见值在何处及何时被获取，以及递归评估过程中 S(pecializes) 的特殊处理方式。此视频将帮助开发者更好地理解和利用 USD 的强大功能，优化其在 Omniverse 中的工作流。

https://resources.nvidia.com/en-eu-omniverse-usd/usd-unique -->

## Advancing Humanoid Robots with Foundation Model NVIDIA Project Groot
NVIDIA announces Project GR00T, a general purpose foundation model for humanoid robot learning. GR00T is trained in NVIDIA Isaac Lab, a robot learning application built in NVIDIA Omniverse Isaac Sim.
Learn more about NVIDIA GR00T: https://developer.nvidia.com/project-gr00t
https://youtu.be/h0R5aumX_Uo

## Breathing Life Into Disney’s Robotic Characters With Deep Reinforcement Learning

迪士尼工程师讲述如何使用强化学习

At Disney, we are fundamentally rethinking what entertainment robotics means to this world. In this talk, we will provide insight into the robotic character platform developed by Walt Disney Imagineering that enables the rapid design of legged robotic characters that learn to imitate artist-specified animations with a combination of deep reinforcement learning and GPU-accelerated simulation. We’ll also showcase live for attendees the first characters developed with this platform – our BD-X series droids.

https://www.nvidia.com/en-us/on-demand/session/siggraph2024-sigg2409

## A Smart Robot Born in the Industrial Metaverse, Enabled by OpenUSD
Cristian Sartori 克里斯蒂安·萨托利, Country Business Segment Manager, Siemens
，西门子国家业务部门经理
Vincenzo De paola 德保拉, AI Technical Sales Support, Siemens
，AI技术销售支持，Siemens

In robotic applications, the arduous and costly process of optimizing the vision system for top-notch performance is a significant challenge. Siemens Italy Digital Industries' pre-sales consulting team, known for its strong collaboration with local original equipment manufacturers, delves into cutting-edge robotic feeding systems, exemplified by 'Supata,' a groundbreaking solution tailored for the efficient handling of small-batch assembly and insertion tasks. The pivotal goal here is to augment the robot's adaptability by transitioning from the traditional vision system to an artificial intelligence-driven one. This transformation empowers the robot to adeptly identify and adjust to various objects, facilitated by the powerful Omniverse framework. The journey commences with 3D models of objects, where the robot is visually trained to distinguish these diverse objects, including those synthetically generated via Isaac Sim Replicator.
在机器人应用中，优化视觉系统以获得一流性能的艰巨而昂贵的过程是一项重大挑战。西门子意大利数字工业的售前咨询团队以与当地原始设备制造商的密切合作而闻名，他们深入研究尖端的机器人进料系统，例如“Supata”，这是一种为高效处理小批量装配和插入任务而量身定制的突破性解决方案。这里的关键目标是通过从传统的视觉系统过渡到人工智能驱动的视觉系统来增强机器人的适应性。这种转换使机器人能够熟练地识别和适应各种物体，并由强大的Omniverse框架提供便利。旅程从物体的3D模型开始，机器人在视觉上接受训练，以区分这些不同的物体，包括通过Isaac Sim Replicator合成生成的物体。

https://www.nvidia.com/en-us/on-demand/session/gtc24-s62687

## NVIDIA Research Presents AI and the Next Frontier of Graphics
Clement Fuji Tsang, Research Scientist, NVIDIA
Yashraj Narang, Research Manager and Senior Research Scientist, NVIDIA
Ming-Yu Liu, Vice President of Generative AI Research, NVIDIA
Aaron Lefohn, VIce President of Graphics Research, NVIDIA
Francis Williams, Senior Research Scientist, NVIDIA

Get an exclusive sneak peek into the latest research coming from NVIDIA Research in graphics, 3D, simulation, and robotics labs. Join research luminaries as they present their latest fields and discoveries in a lightning round of presentations.

独家预览NVIDIA research在图形、3D、模拟和机器人实验室的最新研究。加入研究名人的行列，在闪电般的演讲中展示他们的最新领域和发现。

https://www.nvidia.cn/on-demand/session/siggraph2024-sigg2401

## Robotics in the Age of Generative AI

<!-- Robotics in the Age of Generative AI
Vincent Vanhoucke, Distinguished Scientist and Senior Director of Robotics, Google DeepMind

Generative AI is taking automated common-sense reasoning, task planning, and perception to a new level. It is also revolutionizing synthetic data generation, human-computer interaction, and multimodal understanding. Collectively, these are some of the key capabilities required for robots to understand our world and provide humanity with accessible, versatile physical assistance for day-to-day tasks. The key missing ingredient is for generative AI to also understand physical interaction. I'll sketch a future in which embodied AI is a natural extension of the revolution that large multimodal models are ushering, and its implications for the future of collaborative robotics and human-centered AI at large.

https://www.nvidia.com/en-us/on-demand/session/gtc24-s61182 -->

## Human-Level Performance with Autonomous Vision-Based Drones
Davide Scaramuzza, Professor, University of Zurich, Director, Robotics and Perception Group

Autonomous drones play a crucial role in search-and-rescue, delivery, and inspection missions, and promise to increase productivity by a factor of 10. However, they're still far below human pilots in terms of speed, versatility, and robustness. What does it take to fly autonomous drones as agile as, or even better than, human pilots? Autonomous, agile navigation through unknown, GPS-denied environments poses several challenges for robotics research in perception, learning, planning, and control. I'll show how the combination of both model-based and machine learning methods, united with the power of new, low-latency sensors (such as event cameras), can allow drones to achieve unprecedented speed and robustness by relying solely on onboard computing. This can result in better productivity and safety of future autonomous aircraft.

https://www.nvidia.com/en-us/on-demand/session/gtc24-s62254

## Deploying AI in Real-World Robots

<!-- Aaron Saunders, Chief Technology Officer, Boston Dynamics

Boston Dynamics is excited about a new opportunity in the research world. Along with NVIDIA, Boston Dynamics has been working toward an exciting new robot capability that integrates hardware and software. Come learn how the NVIDIA AGX Orin took the robotic ecosystem to a new level, unlocking opportunities in the AI and cloud spaces.

Spot机器人在现实世界中的挑战与成就（波士顿动力）
加入波士顿动力首席技术官Aaron Saunders，深入了解移动机器人在现实世界中的AI部署。在这次演讲中，Aaron将分享波士顿动力如何通过与NVIDIA的合作，将硬件和软件完美结合，推动机器人技术的前沿发展。
Aaron将分享波士顿动力如何通过强化学习技术，克服Spot机器人在滑溜表面上的打滑问题，确保其在复杂环境中的可靠性和稳定性。从早期的实验到实际应用，波士顿动力展示了如何通过高度集成的硬件和软件，实现机器人的高效能和高可靠性。通过具体的案例，您将看到Spot机器人如何在复杂环境中自主完成任务，为各行各业带来显著的价值。
https://www.nvidia.com/en-us/on-demand/session/gtc24-s62602

Spot RL Researcher Kit介绍：
https://bostondynamics.com/reinforcement-learning-researcher-kit/ -->


## Parkour and More: How Simulation-Based RL Helps to Push the Boundaries in Legged Locomotion

<!-- Marco Hutter, Professor for Robotics, ETH Zurich

Recent years have brought tremendous progress in the field of legged robotics and applying quadrupedal systems in the real world. Besides the massive improvement of the hardware systems to rugged and certified products by several companies, recent developments in perception, navigation planning, and reinforcement learning (RL) for locomotion control have unleashed a new level of robot mobility and autonomy to operate in challenging terrain. We'll show the joint efforts of NVIDIA and ETH Zurich to leverage RL with NVIDIA simulation tools to achieve full-body locomotion maneuvers of quadrupedal robots in parkour-like environments. We'll talk about novel ideas to handle perception and how these methods can be extended to autonomous navigation.

https://www.nvidia.com/en-us/on-demand/session/gtc24-s63140 -->


## Robotics and the Role of AI: Past, Present, and Future
Dieter Fox, Senior Director of Robotics Research, NVIDIA
Marc Raibert, Executive Director, The AI Institute

Advances in artificial intelligence have enabled breakthroughs in several fields, including computer vision and natural language processing in both academia and industry. In this fireside conversation, NVIDIA’s senior director of robotics research, Dieter Fox, will be joined by Marc Raibert, the executive director of The AI Institute, to discuss how artificial intelligence has impacted robotics, from the traditional controls era to today.

https://www.nvidia.com/en-us/on-demand/session/gtc24-s62315

## Narrowing the Gap Between Simulation and Reality With OpenUSD
Rich Goldman, Director of Strategic Partnerships, Ansys

Join us to learn how Ansys is using OpenUSD to unlock new capabilities in physics-based simulation and visualization, enhancing the accuracy, efficiency, and realism of virtual prototypes across industries. Discover how Ansys is contributing its AI-enabled simulation expertise to the Alliance for OpenUSD to build an open, extensible.

https://www.nvidia.com/en-us/on-demand/session/siggraph2024-sigg2415

## Accelerating Autonomous Vehicle Development With OpenUSD Virtual Worlds
<!-- Hai Loc Lu, Sr. Engineering Manager, NVIDIA

We’ll demonstrate how OpenUSD enhances AV workflows through creating worlds with NeRF, testing AV scenarios with LLMs, and generating synthetic training data with free space and occupancy voxel GTs for perception at scale.

OpenUSD 如何帮大家加速自动驾驶车的开发

借助 OpenUSD 虚拟世界加速自动驾驶汽车开发
Hai Loc Lu，NVIDIA 高级工程经理

我们将演示 OpenUSD 如何通过使用 NeRF 创建世界、使用 LLM 测试 AV 场景以及使用可用空间和占用体素 GT 生成合成训练数据以进行大规模感知来增强 AV 工作流。

https://www.nvidia.com/en-us/on-demand/session/siggraph2024-sigg2406 -->

## AI Robotics: Driving Innovation for the Future of Automation
Deepu Talla, Vice President, Robotics and Edge Computing, NVIDIA

Transforming the world of robotics, AI is fundamentally altering the way we build and deploy robots. Join Deepu Talla, Vice President of Robotics and Edge Computing, to uncover insights into cutting-edge technologies from NVIDIA Robotics that span from the cloud to the edge. These groundbreaking innovations are poised to advance the entire robotics ecosystem by harnessing the power of AI and GPU-accelerated simulation.

https://www.nvidia.cn/on-demand/session/gtc24-s63287/

## Improving Robot Uptime: Nav2 Autonomous Docking with NVIDIA Isaac ROS
Nico Zhou, Senior Software Engineer, NVIDIA
Steve Macenski, Founder, Open Navigation LLC
Renjie Shao, Senior Software Engineer, NVIDIA

NVIDIA and Open Navigation have developed a solution that lets a robot perceive, navigate, and dock with a charging station to help keep it online continuously. The new Nav2 docking feature uses the advanced perception enabled by NVIDIA Isaac™ ROS to improve and extend your software so you can use your own pose detection algorithms, along with docking to other infrastructure.

NVIDIA Isaac ROS, built on the open-source ROS 2™ framework, is designed to help ROS developers expedite AI robot development and deployment using NVIDIA-accelerated libraries and AI models.

https://www.nvidia.cn/on-demand/session/other2024-rosnav2/

## cuRobo: a CUDA Accelerated Robot Motion Generation Toolkit
Balakumar Sundaralingam, Senior Research Scientist, NVIDIA
Siva Hari, Senior Research Scientist, NVIDIA

Global motion generation for manipulators remains a very slow process, taking anywhere from a few seconds to minutes, predominantly done on CPU. We will present cuRobo, a CUDA-accelerated library containing a suite of robotics algorithms that can generate minimum-jerk, collision-free trajectories within 30 milliseconds leveraging parallel compute on a NVIDIA RTX 4090. cuRobo leverages GPU compute to run optimization over many seeds in parallel, converging to good solutions for many robotics optimization problems, including collision-free inverse kinematics and trajectory optimization.

cuRobo provides fast implementations of kinematics, collision-free inverse kinematics, trajectory optimization, graph planning, batched numerical optimization solvers (L-BFGS, MPPI), and global motion generation. cuRobo also provides fast collision functions to query signed distance between a robot and the world represented by cuboids, meshes (warp), and depth images (nvblox), leveraging several NVIDIA technologies, all on the GPU. cuRobo is integrated with PyTorch, so you can integrate either the full stack for motion generation or only sub modules as needed. We'll explain the approach that cuRobo takes to solve the global motion generation problem, the API design of the toolkit, and provide example integrations with NVIDIA Isaac Sim.

https://www.nvidia.cn/on-demand/session/gtc24-s62122/

## Training an Autonomous Mobile Race Car with OpenUSD and Isaac Sim
Eric Bowman, Omniverse Connection Evangelist, NVIDIA

Autonomous mobile robots (AMRs) are a key element of industrial automation. Training AMRs in the real world is often a difficult, expensive process and prevents many from taking advantage of their tremendous potential. Combining remote control car platforms such as F1Tenth and JetRacer with synthetic data allow virtually anyone to build and train their own AMR. In this session you will use OpenUSD and Isaac Sim to train a virtual F1Tenth vehicle on a digital twin of a track. This will be done using ROS 2 and Isaac Sim in such a way that you can deploy your trained model directly to a real-world robot. Finally, there will be a race to see whose car is the best-trained. The fastest car will get a prize and bragging rights, but with such a fun, informative session we are all winners!Prerequisite(s):
None.

Explore more training options offered by the NVIDIA Deep Learning Institute (DLI). Choose from an extensive catalog of self-paced, online courses or instructor-led virtual workshops to help you develop key skills in AI, HPC, graphics & simulation, and more.
Ready to validate your skills? Get NVIDIA certified and distinguish yourself in the industry.

https://www.nvidia.cn/on-demand/session/gtc24-dlit61797/


## Accelerating Automotive Workflows With Large Language Models
Bryan Goodman, Director, Artificial Intelligence, Ford Motor Co.

Large language models (LLM) are revolutionizing the way we interact with information, making it easy to pinpoint information from a large source of data, such as vehicle owner’s manuals or manufacturing machinery manuals. However, ensuring these agents operate accurately, or without hallucinating, presents a variety of challenges. To date,the best solutions use LLMs through the retrieval augmented generation (RAG) architecture, with solutions benefiting from fine-tuning LLMs for performance, scalability, and domain knowledge. This session will demonstrate and discuss such LLM solutions for vehicle engineering, connected vehicle analytics, manufacturing, legal, vehicle service and repair, customer support, and employee support.

https://www.nvidia.cn/on-demand/session/gtc24-s62804

## Training Robot Behavior at Scale in the AWS Cloud With the NVIDIA Isaac Robotics Platform
Abhishek Srivastav, Senior Solutions Architect, Amazon
Matt Hansen, Principal Solutions Architect, Amazon
Shaun Kirby, Principal IoT Architect, Amazon

In this training lab, you will learn the benefits of using reinforcement learning to train robot behaviors. Then you'll learn how to train robots quickly and easily with NVIDIA Isaac Sim in AWS Cloud. Finally, you'll learn to scale training for robots across multiple GPU nodes to expedite performance using AWS Batch.Prerequisite(s):
Some familiarity with robotic simulation and ML training concepts.

Explore more training options offered by the NVIDIA Deep Learning Institute (DLI). Choose from an extensive catalog of self-paced, online courses or instructor-led virtual workshops to help you develop key skills in AI, HPC, graphics & simulation, and more.
Ready to validate your skills? Get NVIDIA certified and distinguish yourself in the industry.

https://www.nvidia.cn/on-demand/session/gtc24-dlit63259/

## Empowering Collaborative Robots: The Future of AI Vision With Digital Twins
Chung-Hsien Huang, Director, Techman Robot, Inc.

Incorporating visual recognition capabilities into collaborative robot arms helps them adapt to existing production line configurations, making them more flexible to small-volume-large-variety production lines. Mature AI technology significantly expands the scope of robotics vision in positioning and inspection. Additionally, digital twin technology allows for more efficient generation of diverse data for training AI models, thereby accelerating AI model development and robustness.

We'll explain how collaborative robots with built-in AI vision systems create value in positioning and inspection compared to traditional robots. Through simulation in a digital twin environment, we'll illustrate how robots learn to optimize motion trajectories, object recognition, defect identification, and other aspects, fundamentally altering current robot programming methods.

https://www.nvidia.cn/on-demand/session/gtc24-s62409/


## Elevate Your Robotics Game: Unleash High Performance with Isaac ROS & Isaac SIM
Chitoku Yato, Senior Technical Product Marketing Manager, NVIDIA
Asawaree Bhide, Technical Marketing Engineer, NVIDIA
Piyush Medikeri, Senior Systems Software Engineer, NVIDIA

Our introduction to the Isaac Robot Operating System will include:

A hands-on tutorial showing how to use the Isaac ROS dev container;
How to run Isaac ROS perception packages on Jetson; and
A demo of how to test a robot in simulation with hardware in the loop, using Isaac SIM and Isaac ROS.

Prerequisite(s):

Knowledge of ROS.


Explore more training options offered by the NVIDIA Deep Learning Institute (DLI). Choose from an extensive catalog of self-paced, online courses or instructor-led virtual workshops to help you develop key skills in AI, HPC, graphics & simulation, and more.
Ready to validate your skills? Get NVIDIA certified and distinguish yourself in the industry.

https://www.nvidia.cn/on-demand/session/gtc24-dlit61534/

## Transforming Agriculture with AI and Computer Vision
Chris Padwick, Director of Machine Learning and Computer Vision, John Deere

Leveraging AI and Computer Vision, John Deere is building products that help farmers grow food more efficiently. AI-equipped robot sprayers can reduce herbicide usage by only targeting weeds. Autonomous driving tractors can free up hours for the farmer to do higher value tasks. In either case, these technologies are improving profitability and increasing sustainability. Deploying these perception technologies in the hard and dusty world of a farm introduces unique challenges. Join this session for a deep dive into this tech stack, including how John Deere uses synthetic data to improve AI model performance.

https://www.nvidia.cn/on-demand/session/gtc24-s63033/

## Next Phase of Industrial Robot Skills with AI
Masahiro Ogawa, Representative Director, President, Yaskawa Electric Corp.

So far, robot technology and products have served as integral components of automation across many manufacturing industries. Nevertheless, against the backdrop of societal challenges including environmental concerns, an aging population, economic inequalities, food shortages, and energy supply issues, the demand for automation encompasses a broad spectrum, leaving numerous diverse automation tasks unresolved. Automating such tasks requires actively adapting to the environment and performing tasks beyond the purview of the passive robots we've seen thus far. Through integration with NVIDIA's technology, robots are undergoing a new phase of evolution, heralding a fresh era of automation. We'll introduce this new era of automation, highlighting practical use cases in collaboration with our partners.

https://www.nvidia.cn/on-demand/session/gtc24-s61403/

## Simulating Custom Robots: A Hands-On Lab Using Isaac Sim and ROS2
Elie Youssef, Lead Robotics Engineer, Inmind.ai
Liila Torabi, Director, Isaac Sim, NVIDIA

Robotics applications have grown in popularity recently, with robots being used in industries including manufacturing, healthcare, and agriculture (among others) to increase production, quality, efficiency, and safety. And with the help of other technologies such as artificial intelligence, sensor technologies, and hardware developments, it's now possible to build a robot capable of operating independently of other devices.
Learn how to simulate a custom robot, how to import custom messages from the ROS2 application, and allow Isaac Sim to subscribe to and publish those messages. You'll create the ROS2 package with the appropriate Python version that's compatible with your Isaac Sim version, then create an extension that uses those messages. By the end of this workshop, you should be able to import your custom robot, build the ROS2 packages, and run a simulation in which Isaac Sim communicates with the ROS2 app and vice versa.
Prerequisite(s):

Basic knowledge in ROS2, including familiarity with messages, services, and actions, colcon build, CMake.
Intermediate knowledge in Python 3.


Explore more training options offered by the NVIDIA Deep Learning Institute (DLI). Choose from an extensive catalog of self-paced, online courses or instructor-led virtual workshops to help you develop key skills in AI, HPC, graphics & simulation, and more.
Ready to validate your skills? Get NVIDIA certified and distinguish yourself in the industry.

https://www.nvidia.cn/on-demand/session/gtc24-dlit61899/

## Autonomous Mobility Robotics is the Key Driver Accelerating the Era of AI (Presented by MSI)
Albert Uy, Assistant Vice President, Micro-Star International Co., Ltd.

The maturation of the autonomous mobile robot (AMR) is pivotal in propelling the age of AI forward by enabling advanced and self-driven mobility, ultimately contributing to the rapid evolution of AI. AI technology helps with the complexity of the AMR in computing the best and shortest way to target, provides timely inference to ensure the efficiency and productivity, and increase the level of safety.

Our AMR AI Cobot is equipped with NVIDIA Isaac Sim, an extensible robotics simulator giving you a better way to design and train AI robots, powered by Omniverse to deliver scalable, photorealistic, and accurate virtual environments for making high-fidelity simulations. For the robotic arm, it also incorporates visual recognition run by NVIDIA GPU A2000 for inference and model training. Effective flexible manufacturing systems depend deeply on AI inference to simultaneously organize and manage the mission to optimize the logistics flow; communications between AMR, robot arms, doors, etc; and user friendliness.

https://www.nvidia.cn/on-demand/session/gtc24-expt63127/

## Tired of Household Chores? Teach Robots to Perform 1,000 Everyday Activities With BEHAVIOR
Chengshu Li, Ph.D. Student, Stanford University
Josiah Wong, Ph.D. Student, Stanford University

Meet BEHAVIOR: a simulation benchmark focused on robot tasks that matter to humans. Guided by the question: "what should robots do for you?", we defined 1,000 daily activities set in 50 everyday scenes (think homes, offices, etc.) with over 7,000 curated objects. Through OmniGibson, our simulation environment based on NVIDIA Omniverse, these activities come to life with realistic physics and visuals, involving everything from rigid and flexible bodies to liquids. Learn how BEHAVIOR can accelerate your AI and robot learning research!

https://www.nvidia.cn/on-demand/session/gtc24-s62698/

## People and Robot Navigation Simulation for Synthetic Data Generation
Krishna Kunadharaju, Senior Software Engineer, NVIDIA
Xunlei Wu, Senior Manager of AI Software, NVIDIA

People detection, tracking, as well as robot navigation in the wild are challenging problems. Training AI perception models and robot navigation controller models demands large amounts of labelled data. The cost of acquiring such data in the real world is overwhelmingly high. Synthetic data generation provides a scalable solution with greater flexibility, reduced bias, and improved data security.

https://www.nvidia.cn/on-demand/session/gtc24-s62573/

## Isaac Sim/Robotics Weekly Livestream: From Single to Multi Robot Environment
Ossama Samir Ahmed, NVIDIA

In this session we will show how to program a pick and place controller for one of the Isaac Sim Robot assets and then scale the environment to multiple robots doing the same

https://www.nvidia.cn/on-demand/session/issac-ovk1614/

## Accelerating Autonomous Mobile Robot Development from the Ground Up
Amit Gupta, Associate Director, eInfochips
Gurpreet Singh, Associate Director, eInfochips

The use of robots, and specifically application-specific autonomous mobile robots (AMRs) — most of them with unique functionality and specifications — will soon be ubiquitous. This necessitates creating almost near-product-like minimum viable products to get customer and venture capital attention at speed and scale. We present grounds-up development of AMR and showcase the accelerated development using AGX Orin and tools like ISAAC SIM, Replicator, Omniverse, TAO toolkit/Tensor RT, ROS GEMS, and Triton. This work involves integration and development of multiple technologies, such as VSLAM, sensor fusion, navigation, chassis and hardware design, simulation, sensor integration on ROS2, and automated testing. We achieved all of this in a short time, including the team ramp-up. This fundamental AMR template (i.e., the ability to navigate from point A to point B autonomously while performing object identification and avoiding obstacles) opens opportunities for customizing it for multiple applications and use cases in the future.

https://www.nvidia.cn/on-demand/session/gtcspring23-s51232/

## Next-Generation Robotics: Training Agile Loco-Manipulation and Human-Machine Interaction
Julian Eßer, Research Associate, Fraunhofer IML
Sören Kerner, Head of AI and Autonomous Systems, Fraunhofer IML

Nowadays no industry can thrive without efficient logistics. With rising complexity, logistics is moving away from pure automation toward more flexible mobile robots, and consequently human-robot interaction. Fraunhofer IML is known for pushing the boundaries of logistics automation with projects like the highly dynamic LoadRunner or the recently presented O³dyn pallet transport robot.

We'll preview what the next generation of autonomous transport robots will look like — agile, flexible, cooperative, and intelligent. The evoBOT is a bio-inspired robot combining complex skills such as agile locomotion and dexterous object manipulation for a wide range of potential logistics applications. To scope with such a range of powerful robotic skills, novel development concepts such as simulation-based AI need to be introduced, which provides a systematic approach to accelerate training and improve performance in real-world robotics settings by incorporating additional knowledge sources. Holistic physics simulations are a key technology for such outstanding developments.

A logistic future without humans is unthinkable. Thus, learning to interact with humans is highly needed for future applications. But this can be dangerous to humans, especially in the early stages of development. Thus, we’ll show how this development, interleaved with Isaac SIM, can pave the way to an industrial application of AI-based robots, both in simulation and reality, and provide deeper insights into the underlying technologies.

https://www.nvidia.cn/on-demand/session/gtc24-s62865/

## Isaac Sim/Robotics Weekly Livestream: Importing Robot, Assembly Setup, Simulation Tuning
Renato Gasoto, Robotics & AI Engineer, NVIDIA

NVIDIA Omniverse is a scalable, end-to-end platform for building and operating metaverse applications. Based on Universal Scene Description (USD), Omniverse fundamentally transforms complex 3D workflows, allowing individuals and teams to connect and customize 3D pipelines and simulate large-scale, physically accurate virtual worlds for industrial and scientific use cases.

https://www.nvidia.cn/on-demand/session/issac-ovk1608/

## Isaac Sim/Robotics Weekly Livestream: Bringing a new robot to Isaac Sim & making custom extension
Avi Rudich, NVIDIA

This is part of the weekly Isaac Sim / Robotics livestream series that you can add to your calendar here: https://www.addevent.com/event/jQ1666...

In this episode, NVIDIA's Avi Rudich covers: Bringing a new robot to Isaac Sim, starting with its URDF file. https://docs.omniverse.nvidia.com/app... 

Configuring the new robot to use our "Motion Generation" stack: 
1) Creating a Robot Description File https://docs.omniverse.nvidia.com/app... 
2) Configuring RMPflow https://docs.omniverse.nvidia.com/app... 

Writing a standalone extension from scratch using Isaac Sim Extension Templates https://docs.omniverse.nvidia.com/app...
 
NOTE: The issue Avi encountered in this stream at 14:30 with paths ended up being a very simple fix. In the file name he was trying to import, there was the word "issac" and he typed the name as "isaac". I got the path to the file right, but when I typed the file name I didn't notice the misspelling.

https://www.nvidia.cn/on-demand/session/issac-ovk1616/

## Amazon Robotics Deploys First Fully Autonomous Robot With NVIDIA Isaac Sim

Amazon Robotics has manufactured and deployed the world's largest fleet of mobile industrial robots. The newest member of this robotic fleet is Proteus—Amazon's first fully autonomous mobile robot. Amazon uses NVIDIA Isaac Sim, built on Omniverse, to create high-fidelity simulations to accelerate Proteus deployments across its fulfillment centers.

Explore NVIDIA Isaac Sim: https://developer.nvidia.com/isaac-sim

#Simulation
#Robotics
#DigitalTwin

## Generate Synthetic Data of Dynamic Environments With a Custom Isaac Sim-Based Framework
Elia Bonetto, Ph.D. Student, Max Planck Institute for Intelligent Systems

Robots need to be simulated before they can be deployed in the real world. Unfortunately, this is usually done in static scenarios even though the real world is dynamic. High-quality simulations of dynamic environments will help us build robots that can be safely deployed in the real world. These simulations need to be photorealistic and physically accurate, with sensor noise, and be flexible enough to adapt to different sensors and setups. However, no system currently works like that out of the box. We'll show how this can be done using NVIDIA Isaac Sim, publicly available datasets, Omniverse Connectors, and our framework to generate data with a simplified learning curve. The five key ingredients include (1) environments, (2) dynamic humans, (3) simulation setup and placement of the assets, (4) simplified robot control, and (5) data saving and post-processing.

It's not required, but familiarity with Isaac Sim and Python will help you better understand this talk.

https://www.nvidia.cn/on-demand/session/gtcspring23-s51570/

## Real-World Implementations of Simulation for Next-Generation Robotics
David Hall, Research Fellow, CSIRO
Dave M.S. Johnson, CEO and Co-founder, Dexai Robotics
Daniel Grieneisen, Principle Software Engineer, 6 River Systems
Kel Guerin, Co-founder & Chief Innovation Officer, READY Robotics
Liila Torabi, Director, Isaac Sim, NVIDIA
Nuzha Yakoob, Head of Technology and Innovation, Festo

Next-generation intelligent robots require new tools for responsiveness to new environments, extended behavior trees, AI, and machine handshakes with other equipment. For these reasons, future robotics simulation requires more than just a disconnected virtual robot. Instead, the simulation must train real perception pipelines, test robot software for production, and integrate with real-world environments. Learn how the latest edition of NVIDIA Isaac Sim, a photorealistic and physically accurate robotics simulation tool, is leveraged by advanced robotics practitioners. A panel of experts will share how they use NVIDIA Isaac Sim for their robotic products and projects, including robotic manipulation, machine perception, factory automation, and autonomous mobile robots.

Topics will include:
• Democratizing simulation in the robot design process.
• Successful robot integration with business requirements.
• How visual and physical high fidelity enables advancements in robotic intelligence and AI.

https://www.nvidia.cn/on-demand/session/gtcspring23-s52290/

## Training Highly Dynamic Robots for Complex Tasks in Industrial Applications
Julian Eßer, Research Associate, Fraunhofer IML
Sören Kerner, Head of AI and Autonomous Systems, Fraunhofer IML

Automation in logistics is moving from automated guided vehicles toward autonomous transport robots. With this shift in robotics, developing logistic systems becomes more and more complex. Fraunhofer IML is known for pushing the boundaries of logistics automation with projects like the highly dynamic LoadRunner, or the recently presented O³dyn pallet transport robot. In this talk, we'll preview what the next generation of autonomous transport robots will look like — agile, flexible, and intelligent. The evoBOT is a bio-inspired assistance robot combining complex skills such as dexterous object manipulation and agile transportation for a wide range of potential industrial applications. Along with this, we'll introduce the concept of guided reinforcement learning, which provides a systematic approach to accelerate training and improve performance in real-world robotics settings by incorporating additional knowledge sources. We’ll show how these technologies, interleaved with Isaac SIM, can pave the way to an industrial application of AI-based robots, both in simulation and reality, and provide deeper insights into the underlying methods used. Check out the following resources below to already get a first preview of what this talk is about. Preview evoBOT robot by clicking this link and Guided Reinforcement Learning by clicking this link .

https://www.nvidia.cn/on-demand/session/gtcspring23-s51823/

## Building a Robot Digital Twin in Isaac Sim: A Step-by-Step Example
Gennady Plyushchev, Engineer/YouTuber, Skyentific

Accurate robotic simulation allows one to design advanced control algorithms. But capturing all the robot parameters need for simulation precisely is extremely complicated (especially in the case of agile robots). We'll describe (1) how to make a digital twin starting from the CAD model of robot; (2) how to adjust all necessary robot parameters; and (3) how to synchronize the digital twin with the physical robot in real time. As an example, we'll use the robot arm controlled with the Jetson AGX Orin.

https://www.nvidia.cn/on-demand/session/gtcspring23-s51561/

## Utilizing Sensor Fusion to Improve VSLAM-based Navigation for Autonomous Mobile Robots (Presented by Arrow Electronics)
Harsh Vardhan Singh, Solutions Engineer, einfochips
Sagar Dhatrak, Solutions Engineer, einfochips

With the growing need for autonomous mobile robots (AMRs) across various industries, robust and accurate localization becomes imperative for safe and precise navigation. Having developed an AMR from the ground up, we gained first-hand experience on why localization may fail, what problems bad localization causes, and how they can be mitigated. We'll give insights on this experience with specific focus on Kalman Filter-based sensor fusion using Wheel Encoders, inertial measurement units, and visual ddometry generated from VSLAM algorithms using the robot_localization package on ROS2 (Robot Operating System) Humble. We'll also cover the role of sensor fusion in providing a robust and accurate odometry source for other modules, like navigation and mapping, with respect to ROS2. Finally, we'll present our experience in combining sensor fusion with NVIDIA ROS GEMS packages like Nvblox and VSLAM and explore RTabMap with ToF sensor.

https://www.nvidia.cn/on-demand/session/gtcspring23-s52372/

## Simulation-Based Development of Mobile Robots: Technical Hands-On to the Next-Generation Logistics Robot O³dyn
Anna Vasileva, Research Associate, AI and Autonomous Systems, Fraunhofer IML
Marvin Wiedemann, Research Associate, AI and Autonomous Systems, Fraunhofer IML

Developing a mobile robot from a first idea to an autonomous system is a complex and resource-intensive task. We, as Fraunhofer IML, take on the challenge and develop next-generation robots for logistics use cases. Within this development, simulation tools, such as Omniverse Isaac Sim,
play a more and more important role. They advance our opportunities in the field of actuators, sensors, and communication. Last year at GTC 2022, we presented O³dyn (formerly named Obelix), a sophisticated logistics robot that can drive up to 36 km/h, omnidirectional, indoor, and outdoor. It received much interest not only in real-world presentations but also virtually in the keynote of GTC as one of the most sophisticated simulation models available in Isaac Sim. This year, we present deeper insights into the model and explain technical hands-on. This is especially interesting since O³dyn is open-source and everyone can play around with its dynamics at home. We'll show its simulated sensors, such as lidar and cameras, as well as its integration in the Robot Operating System (ROS). With examples on how we use these tools in our development chain, we provide insight into the daily business of developing mobile robots with and without simulation tools.
Can’t wait to see O³dyn? The following links guide you to a video of our robot and the simulation
model.
O³dyn in action: youtube.com/watch?v=ianBiJZ-AS4

Simulation model:
https://git.openlogisticsfoundation.org/silicon-economy/simulation-model/o3dynsimmodel

## Jetson Edge AI Developer Days: Design a Complex Architecture on NVIDIA Isaac ROS
Raffaello Bonghi, Developer Relations Manager Robotics & AI, NVIDIA

We'll investigate the new Isaac ROS features and how to design a robot using NVIDIA hardware, working with the latest Jetson Orin and designing in DevOps a continuous integration and delivery solution. We'll briefly explain how to implement your robot using Nanosaur.

https://www.nvidia.cn/on-demand/session/gtcspring23-s51822/

## Accelerating Robotics from the Edge to Cloud with AI and Simulation
Amit Goel, Director of Product Management, Autonomous Machines, NVIDIA

Designing, developing, testing, deploying, and managing robots is a complex process that needs accelerated computing on the edge for deploying the AI robot brains, and in the cloud for AI training and simulation. Learn how robot developers and companies in agriculture, manufacturing, logistics, retail, and last-mile delivery are using the NVIDIA Isaac platform to develop innovative solutions while accelerating the time-to-market. 

https://www.nvidia.cn/on-demand/session/gtcspring23-s52631/

## Scaling Robotic Simulation in the Cloud with NVIDIA Isaac Sim and AWS Robomaker
Abhishek Srivastav, Senior Solutions Architect, AWS
Matt Hansen, Principal Solutions Architect, AWS
Shaun Kirby, Principal Customer Delivery Architect, AWS
Rishabh Chadha, Technical Marketing Engineer, NVIDIA

Training robots requires high-fidelity simulation. In addition, it requires a multitude of data extracted from a broad variety of scenes. Generating this training data can require considerable compute resources running simulations in parallel. In this session, get hands on experience running NVIDIA Isaac Sim and Isaac Replicator on AWS Robomaker in the cloud. You will generate Synthetic Data for segmentation in multiple parallel batches and use ROS to drive a robot in simulation and visualize the results from its perspective. The session will teach you to simulate and train robots in the cloud quickly and cost-effectively. 

Prerequisite(s):  

Some familiarity with robotic simulation and ML training concepts. 
 
Please disregard any reference to "Event Code" for access to training materials. "Event Codes" are only valid during the original live session. Explore more training options offered by the NVIDIA Deep Learning Institute (DLI). Choose from an extensive catalog of self-paced, online courses or instructor-led virtual workshops to help you develop key skills in AI, HPC, graphics & simulation, and more.

https://www.nvidia.cn/on-demand/session/gtcspring23-dlit52116/

## Re-imagining Robot Autonomy with Neural Environment Representations
Mac Schwager, Associate Professor of Aeronautics and Astronautics, Stanford University

New developments in computer vision and deep learning powered by next-generation GPUs have led to the rise of neural environment representations: 3D maps that are stored as deep networks that spatially register occupancy, color, texture, and other physical properties. These environment models can generate photorealistic synthetic images from unseen viewpoints, and can store 3D information in exquisite detail. I'll investigate the question: How can robots use neural environment representations for perception, motion planning, manipulation, and simulation? I'll show recent work from my lab in which we use neural radiance field (NeRF) representations for robot navigation and manipulation, and incorporate the NeRF model into a differentiable robot dynamics simulator. I'll conclude with future opportunities and challenges in integrating neural environment representations into the robot autonomy stack.

https://www.nvidia.cn/on-demand/session/gtcfall22-a41181/

## How to Build a Digital Twin: Bringing in Robotics
Liila Torabi, Director, Isaac Sim, NVIDIA

This session is part of a series to learn how to build a digital twin in NVIDIA Omniverse. After aggregating 3D datasets to simulate your complete industrial environment, you'll need to bring in the robots. We'll walk through the process of building a digital twin of your autonomous mobile robot (AMR). We'll cover how one imports the robot from CAD/URDF and rigs the robot so that it moves in simulation just as it moves in the real world. We'll also add the sensors (lidar/cameras) and then connect to Robot Operating System so that you can exercise the navigation stack in the digital twin. Finally, we'll show how the digital twin can be used to generate synthetic datasets that can be used to train the robot’s perception models. Check out "How to Build a Digital Twin: Full-Design Fidelity Visualization and Aggregation of 3D Data" to learn the previous step.

https://www.nvidia.cn/on-demand/session/gtcfall22-a41384/

## Building City-Scale Neural Radiance Fields for Autonomous Driving
Piotr Sokolski, Staff ML Software Engineer, Wayve

We'll share our experience with building a pipeline for constructing neural radiance fields (NeRFs) at a city scale. Recent advancements in neural rendering techniques, such as NeRFs, enable the creation of data-driven simulations for robot perception and control. We use NeRFs to build interactive environments to test and train autonomous agents that control vehicles deployed on real roads. Join us to hear more about the challenges involved in scaling this technique to be able to create city-scale reconstructions: (1) splitting the problem into parallelizable sub-tasks, (2) automating quality control, and (3) overcoming the shortcomings of using NeRFs to simulate complicated driving scenarios.

https://www.nvidia.cn/on-demand/session/gtcspring23-s51770/

## Fast-Track Next-Generation AI Robotics Deployment (Presented by Advantech)
Chen Su, Senior Technical Product Marketing Manager, NVIDIA
Nathan Wang, Product Manager, Embedded IoT Group, Advantech, Advantech
Matt Wieborg, Solution Architect Supervisor & FDA Manager, Advantech USA, Advantech

As robots deploy in dynamic and challenging environments, designing AI-empowered robots is becoming increasingly complex, especially where robots and humans share common operating areas. Therefore, the next-generation AI robots will need not only eyes to see, sense, and navigate, but also intelligence to interact and make decisions to ensure that human–robot interactions are dependable and safe. We'll discuss key factors to achieve this: (1) AI capabilities on edge robots; (2) ROS2 integration; and (3) large-scale management. We utilize NVIDIA Jetson Orin platform and RTX GPUs for AI-accelerated robot learning at scale for a variety of tasks, and we develop dedicated software to accelerate deployment and management.

https://www.nvidia.cn/on-demand/session/gtcspring23-s52429/

## Introduction to Autonomous Vehicles
Aaraadhya Narra, Senior Product Manager, Autonomous Vehicles, NVIDIA

Autonomous vehicles are revolutionizing the way we think about transportation. This session provides an entry point into this transformative technology and its effect on the future of mobility. You'll hear a comprehensive overview of fundamental concepts, including AI model development and validation, software-defined compute, and intelligent cockpits, as well as how recent technological breakthroughs such as Generative AI are shaping the industry. Whether you're new to autonomous vehicles or seeking to deepen your knowledge, this session offers a foundation for engaging with the new era of mobility.

https://www.nvidia.cn/on-demand/session/gtc24-s62558

## Open-World Segmentation and Tracking in 3D
Laura Leal-Taixe, Senior Research Manager, NVIDIA

We'll explain how we tackle the problem of open-world instance segmentation and tracking in 3D, in particular using lidar sensors found in cars. We'll also discuss potential applications.

https://www.nvidia.com/en-us/on-demand/session/gtc24-s63224/

## 推动 OpenUSD 在工业数字化和物理 AI 时代的发展
<!-- Jens Jebens，NVIDIA 高级产品经理
Aaron Luk，NVIDIA 产品管理总监

了解 NVIDIA 和我们的合作伙伴如何推动 OpenUSD 成为工业数字孪生的数据生态系统和 AI 的下一个时代标准，包括自动驾驶汽车和机器人中的物理 AI。我们将介绍传感器仿真、CFD、数据优化框架以及众多 OpenUSD 生成式 AI 模型中的首个模型。

https://www.nvidia.cn/on-demand/session/siggraph2024-sigg2404/ -->

## 将 RTX 引入 OpenUSD：从云端获得超逼真的空间体验
<!-- Max Bickley，NVIDIA Omniverse 空间框架产品经理

了解全光线追踪的工业级 OpenUSD 场景如何借助 NVIDIA Omniverse SDK 和 API 实时串流到 Apple Vision Pro。了解如何开发您自己的原生 OpenUSD 空间应用，以实现更高级别的逼真度数字孪生体验。

https://www.nvidia.cn/on-demand/session/siggraph2024-sigg2405/ -->

## Enhancing 3D Pipelines With Generative AI
<!-- 利用生成式 AI 增强 3D 工作流
Dade Orgeron，Shutterstock 3D 创新副总裁

随着生成式 AI 不断推进几乎各行各业的工作流，3D 正迅速成为这项惊人技术的下一个受益者。Shutterstock 和 TurboSquid 创新副总裁 Dade Orgeron 将分享有关如何使用下一波生成技术来改善和增强各种不同行业的 3D 工作流的经验和见解。了解 Shutterstock 和 NVIDIA 团队正在开发的突破性生成式 3D 工具，并了解它们如何被用于改变 Mattel、HP 和 WPP 等品牌的未来工作流。

https://www.nvidia.cn/on-demand/session/siggraph2024-sigg2416/ -->

## Remastering Classic Games With OpenUSD
<!-- 使用 OpenUSD 重制经典游戏
Nuno Subtil，NVIDIA 首席软件工程师
Mark Henderson，NVIDIA 首席软件工程师

RTX Remix 让 Modder 能够使用路径追踪、DLSS 和带有 PBR 材质的高多边形资产重制经典游戏。在本次演讲中，我们将讨论自发布以来的渲染改进、正在进行的项目以及我们基于 OpenUSD 的内容工作流。

https://www.nvidia.cn/on-demand/session/siggraph2024-sigg2408/ -->

## Blender and OpenUSD for 3D Projects: Full Workflow Guide
Join us for an exciting livestream celebrating Blender, the immensely popular 3D creation suite! We'll showcase film and industrial workflows in Blender, featuring special guests like Blender Bob (Blender Bob) and Emily Boehmer from SORDI.ai (Emily Boehmer). Don't miss this opportunity to learn from the experts and explore the creative possibilities within Blender!

https://blogs.nvidia.com/blog/industrial-designer-blender-openusd-ai/

## One-Click Creation of Hyper-Realistic 3D Characters from AI-Powered Conversations and Designs
<!-- Qixuan Zhang, Chief Technology Officer, Deemos Technologies, Inc.

We'll introduce a new 3D generative AI technology that enables one-click creation of hyper-realistic digital characters. Leveraging the latest advances in large language models and neural rendering and latent diffusion models, this technology can automatically generate 3D character assets directly from images or text prompts. The output 3D models come with production-ready PBR textures that are compatible with various CG engines and creative tools. We'll detail its neural architecture for multi-modal synthesis, texture mapping techniques, and 3D asset generation and optimization methods and demonstrate integration with platforms like Omniverse via OpenUSD outputs. Learn how to leverage related tools like Audio2Face.

No specialized knowledge is required, but familiarity with 3D/deep learning will help you follow technical discussions. You'll understand this cutting-edge character generation technique and its applications.

https://www.nvidia.cn/on-demand/session/gtc24-s61250/ -->

## Steering the Future: Innovative Applications of Real-Time 3D and AI in the Automotive Industry
<!-- Zhou Huang, Senior Director, Lotus Tech

We'll transport you into an entirely new world of automobiles — a world co-created by intelligent digital humans and real-time 3D technologies. Drawing upon NVIDIA's Omniverse platform, large language models (LLMs), and combined 3D application scenarios and innovative applications of intelligent digital humans, we've explored a new car configurator and pioneered the creation of digital twin factories.

The greatest challenge we faced was in constructing a unified and efficient integrated workflow platform for 3D+AI (LLMs). Omniverse proved to be the perfect solution, enabling us to manage all 3D models within Nucleus efficiently, and conveniently use modules such as Audio2Face and ASR/TTS to create digital humans.

We firmly believe that the blend of AI and 3D technologies will indeed stimulate new potentials and opportunities in the automotive industry.

https://www.nvidia.cn/on-demand/session/gtc24-s63150/ -->

## Building GPU-Accelerated Streaming AI Pipelines With Holoscan SDK
<!-- Meiran Peng (Developer Programs), Solutions Engineer, NVIDIA

Join us to deep dive on how to use Holoscan SDK to deploy an AI model conveniently to achieve an end-to-end GPU-accelerated streaming AI application. The NVIDIA Holoscan SDK is part of NVIDIA Holoscan, the AI sensor processing platform that combines hardware systems for low-latency sensor and network connectivity. With the sensor-processing capabilities for the next generation of AI applications at the edge, Holoscan SDK provides ready-to-use GPU-accelerated operators and an easy way to build streaming AI pipelines for a variety of domains, either in C++ or Python. We'll describe the core concepts of Hololscan SDK, and take Yolo Model as an example to show how to deploy an object detection application within Holoscan SDK. Besides that, we'll show how to interoperate with popular preprocessing libraries (CV-CUDA, cuCIM and OpenCV) in Holoscan SDK to create customized operators within the existing pipeline.

https://www.nvidia.cn/on-demand/session/gtc24-s61294/
Holoscan 官方介绍：
https://www.nvidia.com/en-us/clara/holoscan/ -->

## Synchronize Multiple Sensors in Autonomous Machines
<!-- Ding Steven, SENSING Co-Founder and Vice General Manager, SZ Sensing TECH CO.,LTD

Developers of autonomous machines will learn how to synchronize and align data from cameras and lidars in time. The decomposition section explains (1) how the exposure principle and timing of cameras can help developers better understand the output video data of cameras; and (2) how to synchronize the camera with other sensors, such as how to synchronize with data from lidar and GPS.

https://www.nvidia.cn/on-demand/session/gtc24-s62437/ -->

## Digitalizing the World's Largest Industries With OpenUSD and Generative AI
<!-- 利用 OpenUSD 和生成式 AI 实现全球最大产业的数字化
Christine Osik，系统运营和模拟，Amazon Robotics
Benjamin Chang，全球制造副总裁，纬创公司
Andy Pratt，新兴技术 CVP，微软
Dean Takahashi，首席撰稿人，VentureBeat
Joe Bohman，PLM 产品部执行副总裁，西门子数字工业软件
Patrick Cozzi，CEO，Cesium
Paulina Chmielarz，工业运营数字与创新总监，JLR (捷豹路虎)
Rev Lebaredian，Omniverse 和模拟技术副总裁，NVIDIA

全球大型行业竞相采用软件定义，但此类流程的数字化非常复杂。请聆听本小组的杰出人士介绍他们的工业数字化项目，这些项目将生成式 AI、新数据平台、三维互操作性和高级可视化融入到他们的整个组织中。
想要详细了解适用于工业数字孪生的 OpenUSD？欢迎参加此深度学习培训中心培训课程。

https://www.nvidia.cn/on-demand/session/gtc24-s62783/ -->

## An Introduction to OpenUSD

<!-- OpenUSD 简介

通用场景描述（Universal Scene Description）不仅仅是一种文件格式。这个开放、强大、易于扩展的世界合成框架具有用于在高度复杂的 3D 场景中创建、编辑、查询、渲染、模拟和协作的 API。NVIDIA 将继续投资，帮助发展 USD，使其适用于媒体和娱乐以外的工作流程，从而实现工业数字化工作流程以及下一波计算机视觉和生成式 AI。参加此会议，了解 NVIDIA 的最新进展以及下一个重要里程碑。

https://www.nvidia.cn/on-demand/session/gtc24-s62642/ -->

## Overview of Layers in Omniverse With OpenUSD
Note: This video may not reflect the current shipping version

Layers are a fundamental aspect of working on both Omniverse and USD.   Forming the building blocks upon which scaleable and complex scenes can be logically managed and organized, layers provide the critical pathways to things like USD Stage opinions, connected and live collaborative workflows and success in using more advanced features like live mode.  This video will begin the journey into these powerful, simple tools while building a scene.  At the end of the tutorial, you can expect to have a general working knowledge of layers, fully understand their capabilities, and have enough information to start working with layers today.

https://youtu.be/LTwmNkSDh-c

## Learn OpenUSD: An Overview of 3D Workflow Fundamentals

Get an in-depth look at NVIDIA’s new training course Learn OpenUSD, designed to accelerate 3D workflows for developers. This livestream covers foundational OpenUSD concepts, key terminology, and Python best practices.

https://www.youtube.com/watch?v=j8VdnKBu_3g&t=904s

## Navigating the Opportunity for Generative AI in Financial Services

<!-- 把握金融服务业中生成式 AI 的机遇
Belinda Neal，核心工程首席运营官兼工程合作伙伴关系主管，高盛
Benika Hall，高级解决方案架构师，NVIDIA
Prem Natarajan，执行副总裁、首席科学家兼企业 AI 负责人，Capital One
Rohit Chauhan，人工智能执行副总裁，万事达卡

本次座谈会汇聚了金融服务业的思想领袖，他们共同探讨生成式 AI 在塑造金融未来方面的变革性作用。演讲嘉宾们深入探讨 AI 驱动技术的应用，研究生成式 AI 如何重新定义金融行业的客户体验、风险管理和决策流程。座谈会成员就这项技术带来的挑战和机遇分享见解，并探讨道德因素、监管影响以及促进创新的潜力。欢迎与我们一起全面探索生成式 AI 与金融服务之间不断发展的交集领域。

https://www.nvidia.cn/on-demand/session/gtc24-s62592/ -->

## The Role of Generative AI in Modern Medicine
<!-- 生成式 AI 在现代医学中的作用
Cathie Wood，CEO /首席投资官，ARK 投资管理有限责任公司
Eric Topol，教授兼执行副总裁，斯克利普斯研究所（Scripps Research）
Kimberly Powell，医疗健康与生命科学业务总经理兼副总裁，NVIDIA
Peter Lee，微软研究院院长，微软

在技术与医学日益交融的时代，生成式 AI 在革新患者护理、医学研究和培训以及医疗保健经济学方面的影响变得至关重要。本小组座谈旨在探讨这些技术如何不仅仅是工具，而是重新定义医疗服务、发现新药和提高患者疗效的关键驱动力。在 NVIDIA 医疗健康与生命科学业务副总裁 Kimberly Powell 的主持下，本小组探讨了 AI 的进步与实际医疗保健应用之间的协同作用，并邀请了将此作为毕生事业的众多专家参加，包括 Eric Topol、Katherine Wood 以及 Peter Lee。

https://www.nvidia.cn/on-demand/session/gtc24-s62777/ -->

## Driving Innovation: Generative AI and Industrial Digitalization in the Automotive Industry
<!-- 推动创新：汽车行业中的生成式 AI 和工业数字化
Chrissie Kemp，首席数据与 AI 官，JLR (捷豹路虎)
Alex Kendall，联合创始人兼 CEO，Wayve
Norm Marks，汽车企业部副总裁，NVIDIA
Raquel Urtasun，创始人兼 CEO，Waabi

与捷豹路虎、Waabi 和 Wayve 等知名汽车公司的高层领导一起，探讨他们在其组织中整合生成式 AI 和工业数字化的经验。本次会议由 NVIDIA 主持，探索正在重塑汽车格局的尖端技术的变革力量。了解这些行业领导者如何变革从汽车设计到制造到自动驾驶和客户体验的方方面面，帮助您获得宝贵见解，以驾驭快速发展的汽车行业格局。

https://www.nvidia.cn/on-demand/session/gtc24-s62380/ -->

## Accelerating Enterprise: Tools and Techniques for Next-Generation AI Deployment
<!-- 加速企业发展：适用于新一代 AI 部署的工具和技术
Nave Algarici，生成式 AI 软件产品经理，NVIDIA
Mahan Salehi，软件产品经理，NVIDIA

在此会议中，我们将深入探讨 AI 推理的动态领域，研究旨在彻底改变开发者部署生成式 AI 模型方式的先进工具和技术。随着 AI 格局持续快速发展和时间的推移，对提高 AI 推理速度和效率的需求正变得越来越迫切。我们将重点关注新发布的 NVIDIA NIM，这是一套易于使用的运行时，旨在加速生成式 AI 的部署。这种多功能微服务支持各种 AI 模型，从开源社区模型到 NVIDIA AI Foundation 模型，以及定制的自定义 AI 模型。

https://www.nvidia.cn/on-demand/session/gtc24-s63432/ -->

## Building Virtual Worlds with Omniverse (GTC November 2021 Keynote Part 4)

In part 4 of his #GTC21 November keynote, NVIDIA CEO Jensen Huang unveils numerous demos of digital twins in action, built with #Omniverse, a virtual world simulation and collaboration platform for 3D workflows. Work from Bentley Systems, Siemens Energy, BMW Group and Ericsson is among those showcased.

## LIO-SAM demo in NVIDIA Isaac Sim

<!-- https://www.youtube.com/watch?v=I3PHup1nX1c -->

# 合成数据

## Generate Synthetic Data with Omniverse Replicator: Overview (Part 1)

<!-- Training high-performing AI models often relies on real-world data that may be scarce and often expensive to collect and label.  Synthetic data generated from custom pipelines built on NVIDIA Omniverse Replicator provides a way to generate training data from 3D simulations for use cases like visual inspection to detect defects on manufactured parts.

In this four-part tutorial, learn how to train a defect detection model with synthetic data using Omniverse Replicator. Watch this first video for an introduction to synthetic data and how to get started with a downloadable defect detection pack from NVIDIA Omniverse. 

Get started with synthetic data generation on NVIDIA Omniverse: 
https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator.html

https://www.youtube.com/watch?v=amVjqaABfU8 -->

## Generate Synthetic Data with Omniverse Replicator: Loading the Scene and Extension (Part 2)

<!-- Training high-performing AI models often relies on real-world data that may be scarce and often expensive to collect and label.  Synthetic data generated from custom pipelines built on NVIDIA Omniverse Replicator provides a way to generate training data from 3D simulations for use cases like visual inspection to detect defects on manufactured parts.

In this four-part tutorial, learn how to train a defect detection model with synthetic data using Omniverse Replicator. This second tutorial will walk you through how to efficiently load and manipulate an OpenUSD scene, add textures and generate annotated images of your CAD part with defects.

https://www.youtube.com/watch?v=KXmc2ytQIrQ -->

## Generate Synthetic Data with Omniverse Replicator: Generating Synthetic Data (Part 3)

<!-- Training high-performing AI models often relies on real-world data that may be scarce and often expensive to collect and label.  Synthetic data generated from custom pipelines built on NVIDIA Omniverse Replicator provides a way to generate training data from 3D simulations for use cases like visual inspection to detect defects on manufactured parts.

In this four-part tutorial, learn how to train a defect detection model with synthetic data using Omniverse Replicator. This third tutorial shows you how to generate data for training a model in a scene. Learn how the data generation process can vary in duration based on scene complexity and the number of objects. You will also learn ways to improve the training data set, such as adjusting camera angles and textures.

https://www.youtube.com/watch?v=8a3C2xAxHlo -->

## Generate Synthetic Data with Omniverse Replicator: Examining the Back-End Code (Part 4)

<!-- Training high-performing AI models often relies on real-world data that may be scarce and often expensive to collect and label.  Synthetic data generated from custom pipelines built on NVIDIA Omniverse Replicator provides a way to generate training data from 3D simulations for use cases like visual inspection to detect defects on manufactured parts.

In this four-part tutorial, learn how to train a defect detection model with synthetic data using Omniverse Replicator. In this fourth and final tutorial, learn how to customize defect detection using the UI and backend source code by using Python and a randomizer method to generate defects with customized camera positions.  

https://www.youtube.com/watch?v=bmg8XWU5E_Y -->

## Training Computer Vision Models with Synthetic Data in Omniverse

Synthetic data is transforming the training of AI models! In this livestream, join guests Jenny Plunkett from Edge Impulse and Rishabh Chadha from NVIDIA as they explore the remarkable potential of Omniverse Replicator and synthetic data for training object detection models in manufacturing. They’ll also discuss how this technology empowers autonomous robots with pallet detection capabilities. Don’t miss this opportunity to learn about the future of AI training and its applications!

https://www.youtube.com/watch?v=PDEFs79BjOg

## Generating Synthetic Data for Robots

https://www.youtube.com/watch?v=8h0w8Sbx4rM

## Deep Dive into Omniverse Replicator

<!-- Join us as we learn first-hand about Omniverse Replicator (https://developer.nvidia.com/nvidia-o..., an advanced, extensible SDK that enables researchers, developers, and enterprises to generate physically accurate 3D synthetic data and easily build custom synthetic data generation (SDG) tools to accelerate the training and accuracy of perception networks. We learn about the significance of synthetic data and demonstrate an example workflow using simulation-ready assets and analyze the data with a training model.

https://www.youtube.com/watch?v=AGtIV5xgpYc -->

## Isaac Sim Installation & Core Functions | "Hello World" of Omniverse Replicator

<!-- In this video, we’ll dive into the installation process of NVIDIA Isaac Sim and explore its core functionalities in Omniverse Replicator. Learn how to set up Isaac Sim, enable the Replicator extension, and create a simple scene with predefined 3D assets like a torus, sphere, and cube. We’ll also demonstrate how to randomize objects and generate synthetic data using the Synthetic Data Recorder. If you're getting started with Omniverse Replicator, this video will help you take your first steps with practical examples!

https://www.youtube.com/watch?v=_kzW6yBno6Q -->

## Omniverse Replicator Camera Guide | Randomization & Focus Explained
<!-- Ready to take your camera handling in Omniverse Replicator to the next level? In this tutorial, we'll cover randomizing camera positions, working with depth of field using the f-stop, and capturing dynamic scene views over multiple frames. Join us as we break down the essential camera functionalities for creating high-quality simulations.

https://www.youtube.com/watch?v=bDerCqejpPM -->

## Low-Code Workflows for Synthetic Data Generation Pipelines

With Omniverse Replicator, you can build custom synthetic data generation pipelines to create diverse training datasets with annotations such as depth, semantic and instance segmentation, and bounding boxes. In this demo, we use a simple YAML-based workflow to demonstrate how you generate training data by randomizing the location of objects such as a forklift, cones and wet floor signs, commonly found in a warehouse. Learn more about how you can bootstrap the training of your computer visions models with Omnivese Replicator. 

https://www.youtube.com/watch?v=IHbbOnN1bAE


# 动画
## Animation Graph Overview in Omniverse USD Composer

This tutorial will teach you how to use AnimGraph and ActionGraph to control a character blending between an idle and walk animation. We'll go over how to set up an AnimGraph from scratch, add animations and variables, and use a keyboard press node within ActionGraph to drive the AnimGraph to propel our character in the scene.

NVIDIA Omniverse™ is an open platform built for virtual collaboration and real-time photorealistic simulation. Complex creator, designer, and engineering visual workflows are transformed as users and teams connect design tools, assets, and projects for collaborative iteration in a virtual world.

https://www.youtube.com/watch?v=to1jYIBFA4c

## How to Build an AMR Training Extension in NVIDIA Omniverse | EDIO

Join Eric Bowman , Jen Borucki, and Edmar Mendizabal  live on Engineering Design in Omniverse (EDIO). The ackerman AMR training extension is done, it trains AI models that can run self-driving cars and we cannot wait to show it to you.

https://www.youtube.com/watch?v=kHh1IYqQ3n0

## A New Path to 3D: Generative AI Models for OpenUSD

Join us for an insightful livestream as we dive into NVIDIA NIM microservices and their transformative impact on OpenUSD workflows. Discover how tools like USD Code, USD Search, USD Validate, and more are revolutionizing 3D world creation through generative AI copilots and agents. Learn how these advanced technologies are reshaping industries such as manufacturing, content generation, healthcare, and beyond by enabling smarter robotics and autonomous systems. Don’t miss this chance to explore how NVIDIA's innovations are paving the way for the future of AI.

00:00 Welcome, News & Introductions
09:10 What is NVIDIA NIM 
10:30 Value of NIM 
15:00 NIM for OpenUSD Workflows
22:21 NIM for Content Generation Workflows
25:53 NIM for SDG & Digital Twin Workflow
29:20 Generative AI for Digital Twins Guide & Use Cases
30:30 Code Generation (Python USD) with USD Code
35:38 Search Asset Database with USD Search and Write Python with USD Code NIMUSD Search in Action
36:31 USD Code Interactive Overview
38:35 Use USD Code NIM for Replicator Domain Scene Randomization
42:51 Using Stable Diffusion XL and ComfyUI for Synthetic Dataset Augmentation
1:03:26 NIM API Catalog

https://www.youtube.com/watch?v=mkLUta4Fheg

https://build.nvidia.com/nvidia/usdcode

## See how APIs for GPT-4 and NVIDIA DeepSearch were combined to rapidly generate 3D objects with simple, text-based prompts in this experimental project from the Omniverse team.

To experiment with the AI Room Generator Extension Sample in Omniverse, access the code on GitHub: https://github.com/NVIDIA-Omniverse/kit-extension-sample-airoomgenerator

To learn more about how the extension was developed, read our Medium blog: https://medium.com/@nvidiaomniverse/chatgpt-and-gpt-4-for-3d-content-generation-9cbe5d17ec15#cid=ov01_so-yout_en-us

https://www.youtube.com/watch?v=mFazJsjUUSo


## Crafting the Future: Human Creativity as AI's Building Blocks
<!-- 创造未来：人类创造力是 AI 的基石
Susan Nomecos，Getty Images 高级总监兼 Web3 战略负责人

与 Getty Images 全球 AI 和 Web3 战略高级总监 Susan Nomecos 一起，就 AI 与人类创造力之间的相互作用进行富有洞察力的讨论。本次会议将深入探讨训练数据在增强 AI 生成的视觉内容的真实性和文化相关性方面的关键作用，反映当今多元化的社会。Susan 将强调使用透明和清洗后的数据集训练模型的重要性，以避免法律风险、支持创作者和维护知识产权。最后，本次会议将深入探讨在推进创意边界的同时，如何负责任地开发 AI 模型和工具。

https://www.nvidia.cn/on-demand/session/siggraph2024-sigg2418/ -->

## The Transformative Power of Accelerated Computing and AI
Bob Pette, Vice President General Manager Pro Visualization, NVIDIA

Don't miss this special address from Bob Pette, where he’ll discuss how NVIDIA's accelerated computing platform is delivering breakthrough performance in the next wave of AI, sensor processing, digital twins, cybersecurity, autonomous systems, and more.

https://www.nvidia.cn/on-demand/session/aisummitdc24-sdc1112/

## Community Spotlight: Unlocking Physical AI with Isaac Sim and OpenUSD
<!-- About our livestreams:
Please join us for our weekly livestream, which features members of the Omniverse team, partners, and special guests from the community. We will discuss the latest OpenUSD and Omniverse announcements, features, and workflows. Don't miss this opportunity to ask your questions live and get ahead of the curve with OpenUSD and NVIDIA!
https://www.youtube.com/watch?v=xzadqDxKue8

视频中提到的案例：
Beckhoff Bridge开源地址：
https://github.com/loupeteam/Omniverse_Beckhoff_Bridge_Extension
Athena 分享 DOE Digital Twin：
https://github.com/uic-evl/DOE_DigitalTwin -->

## Robotics, Generative AI, and OpenUSD: How WPP is Building the Future of Creativity
Perry Nightingale, WPP

Perry Nightingale, SVP Creative AI for WPP, the world’s largest marketing services company, shares how robotics, OpenUSD, and AI are transforming the content creation pipeline for major global brands. As operators of one of the largest content supply-chains in the media and entertainment industry, WPP works closely with partners like NVIDIA, Shutterstock, and Adobe on breakthrough technologies for enhancing human creativity. Perry will share some of the latest research in high-fidelity generative AI and WPP’s groundbreaking use of robotics in 3D capture for photo-realistic personalized film, while demonstrating how OpenUSD provides a unique standardized platform across thousands of agencies, artists and brands..

https://www.nvidia.com/en-us/on-demand/session/siggraph2024-sigg2407/

## NVIDIA Omniverse Knife Sharpening Showcase
<!-- Loupe 公司新发布的适用于 PLC 的 NVIDIA Omniverse Bridge ，为使用数字孪生开辟了新的可能性。
通过 PLC 和 Omniverse 之间的快速更新率，我们展示了使用硬件在环的强大功能。在此演示中，模拟 PLC 和自定义 IK 驱动 Applied Motion Systems, Inc. 的 4 轴刀具磨床模型的高保真模型。
项目代码见：
https://github.com/loupeteam/Omniverse_Beckhoff_Bridge_Extension
视频搬自：
https://www.youtube.com/watch?v=t02-PeAB2CA -->

## How To Build Digital Twins In Omniverse Tutorial
<!-- We're excited to release an in-depth tutorial on how you can start building digital twins using Loupe's open sourced tools. We'll have the sample project on GitHub!

https://www.youtube.com/watch?v=-Va2sXIHtaQ -->

## How We Built A Digital Twin For A Huge Vegas Show

<!-- Every automation engineer knows that when the curtain rises, your machine code needs to be ready for showtime. In this case we mean that literally. Groundbreaking work here showing how digital twins are a critical tool for shipping more reliable machines to production on incredibly tight budgets and timelines. Cheers to our friends at Michael Curry Design Inc and NVIDIA Omniverse for bringing this amazing technology to the Wynn Las Vegas. 

Also, a huge shoutout to the team at Loupe for their work on this.

https://www.youtube.com/watch?v=P1lxy2zgQuA

关注 loupe.team
https://loupe.team/ -->

## ROSA Demo: Carter with Nvidia IsaacSim

<!-- This video demonstrates the integration of ROSA with Nvidia IsaacSim. ROSA was developed at NASA Jet Propulsion Laboratory to enable advanced human-robot interaction using natural language.

Read the paper on arXiv: https://arxiv.org/abs/2410.06472v1
Check out ROSA on GitHub: https://github.com/nasa-jpl/rosa

https://www.youtube.com/watch?v=mm5525G_EfQ -->

## ROSA Demo: NeBula-Spot in JPL's Mars Yard

<!-- This video demonstrates basic command and control plus scene understanding using ROSA: The Robot Operating System Agent. ROSA was developed at NASA Jet Propulsion Laboratory to enable advanced human-robot interaction using natural language.

Read the paper on arXiv: https://arxiv.org/abs/2410.06472v1 
Check out ROSA on GitHub: https://github.com/nasa-jpl/rosa -->

## RoboPianist: Dexterous Piano Playing with Deep Reinforcement Learning
<!-- 使用深度强化学习的灵巧手钢琴演奏
开源地址：https://github.com/google-research/robopianist

在线体验地址，以在浏览器中原生运行MuJoCo，并可以使用鼠标与它进行交互：
https://kzakka.com/robopianist/#demo -->

## Train Physical AI With Real-World Simulations Using fVDB.

<!-- For physical AI to operate in 3D spaces, it must have spatial intelligence—the ability to understand and reason in the real world. Achieving spatial intelligence involves converting the world around us into AI-ready virtual representations that the model can understand. fVDB, developed by NVIDIA, is an open-source deep learning framework for sparse, large-scale, high-performance spatial intelligence. It builds NVIDIA-accelerated AI operators on top of OpenVDB to enable reality-scale simulations for training physical AI. 

Read the full blog: https://blogs.nvidia.com/blog/fvdb-bigger-digital-models/
Read the fVDB technical blog: https://developer.nvidia.com/blog/building-spatial-intelligence-from-real-world-3d-data-using-deep-learning-framework-fvdb/
Apply for the fVDB Early Access Program: https://developer.nvidia.com/fvdb/join
Check out the OpenVDB Github: https://github.com/AcademySoftwareFoundation/openvdb
Learn more about fVDB: developer.nvidia.com/fvdb

https://youtu.be/rx55wDWqSrw

用 fVDB 训练物理 AI：现实世界的超级仿真利器

想让你的机器人像科幻电影中那样聪明吗？NVIDIA 的 fVDB（Field Volume Database）将帮助你实现这一梦想！本视频将详细介绍如何使用 fVDB 进行现实世界的物理 AI 训练。fVDB 是一个开源的深度学习框架，专为处理稀疏、大规模、高性能的空间智能问题而设计。通过将现实世界的数据转换为 AI 可以理解的虚拟表示，fVDB 使物理 AI 能够在 3D 空间中具备空间智能，实现精确的导航和操作。

本视频将展示如何利用 fVDB 构建大规模、高分辨率的仿真环境，训练物理 AI 模型在现实世界中高效运行。无论您是机器人技术、自动驾驶、工业自动化还是虚拟现实领域的专业人士，fVDB 都将为您打开新的技术大门。立即观看，掌握打造未来超级机器人的终极指南！

阅读完整博客：https://blogs.nvidia.com/blog/fvdb-bigger-digital-models/
阅读fVDB技术博客：https://developer.nvidia.com/blog/building-spatial-intelligence-from-real-world-3d-data-using-deep-learning-framework-fvdb/
申请fVDB早期访问计划：https://developer.nvidia.com/fvdb/join
查看OpenVDB Github：https://github.com/AcademySoftwareFoundation/openvdb
了解更多关于fVDB的信息：developer.nvidia.com/fVDB -->


## Introducing fVDB: Deep Learning Framework for Generative Physical AI with Spatial Intelligence

<!-- fVDB (Early Access) is a GPU-optimized deep learning framework for sparse, large-scale, high-performance spatial intelligence. It builds NVIDIA accelerated AI operators on top of NanoVDB to enable reality-scale digital twins, neural radiance fields, 3D generative AI, and more. fVDB is the infrastructure for generative physical AI with spatial intelligence. 

Apply for the fVDB Early Access Program: https://developer.nvidia.com/fVDB

This video demonstrates various techniques fVDB infrastructure enables, including triangle mesh reconstruction from point clouds, large-scale neural radiance field training, high-resolution simulation upsampling, and even fully AI-generated city models.

0:00 - Digital Twins at Reality Scale
0:50 - Introducing fVDB
1:30 - Triangle Mesh from Point Clouds
1:44 - City-Scale NeRF
1:54 - Large-Scale 3D Generative AI
2:07 - Physics Super-Resolution 
2:25 - Conclusion

fVDB is essential for scaling applications in autonomous driving and 3D generative AI. The framework leverages NVIDIA core technologies including NanoVDB, CUTLASS, tensor cores, and CUDA. It’s implemented as a PyTorch extension for easy integration with other libraries and spatial intelligence algorithms. 

If you’re already using the VDB format, fVDB can read and write existing VDB datasets out of the box. It interoperates with other libraries and tools, such as Warp for Pythonic spatial computing, and the Kaolin Library for 3D deep learning. Adopting fVDB into your existing AI workflow is seamless.
Apply for the fVDB Early Access Program: developer.nvidia.com/fVDB

Dive into our announcement blog: https://blogs.nvidia.com/blog/fvdb-bigger-digital-models/
Check out the fVDB technical blog: https://developer.nvidia.com/blog/building-spatial-intelligence-from-real-world-3d-data-using-deep-learning-framework-fvdb/
Read the research paper to learn more: https://arxiv.org/abs/2407.01781


Nvidia新利器fVDB：开启生成式物理智能 AI 的新时代

NVIDIA 的 fVDB（Field Volume Database）是一款革命性的 GPU 优化深度学习框架，专为处理稀疏、大规模、高性能的空间智能问题而设计。fVDB 基于 NanoVDB 构建，支持现实规模的数字孪生、神经辐射场、3D 生成 AI 等高级应用。本视频将展示 fVDB 的强大功能，包括从点云重建三角网格、大规模神经辐射场训练、高分辨率仿真放大，甚至完全由 AI 生成的城市模型。
fVDB 是自动驾驶和 3D 生成 AI 应用的重要基础设施，充分利用了 NVIDIA 的核心技术，如 NanoVDB、CUTLASS、张量核心和 CUDA。它作为一个 PyTorch 扩展实现，便于与其他库和空间智能算法集成。如果您已经在使用 VDB 格式，fVDB 可以直接读写现有的 VDB 数据集，并与 Warp 和 Kaolin 等其他工具和库无缝互操作。
立即观看，了解如何利用 fVDB 打造未来的生成物理智能 AI，开启无限可能的新时代
深入我们的公告博客：https://blogs.nvidia.com/blog/fvdb-bigger-digital-models/
查看fVDB技术博客：https://developer.nvidia.com/blog/building-spatial-intelligence-from-real-world-3d-data-using-deep-learning-framework-fvdb/
阅读研究论文以了解更多信息：https://arxiv.org/abs/2407.01781
https://youtu.be/6JCp0hXb3S4 -->

## openusd系列
https://developer.nvidia.com/blog/new-video-series-what-developers-need-to-know-about-universal-scene-description/

## Creating Variants in OpenUSD Scenes | Learn With Me

Join Ashley in this Learn With Me stream as she creates variants in OpenUSD using Omniverse!

## Is USD the Future of 3D Animation / VFX?
<!-- USD is pretty cool. Here's why.

0:00 - USD workflows
1:25 - What is USD
4:19 - How is it used in production
7:09 - How can we take advantage of it
9:16 - Multi-application collaboration

https://youtu.be/m43l9RXJcYg -->

## OpenUSD Is Unifying 3D Tools & Here’s How.

<!-- OpenUSD is revolutionizing the 3D industry by providing a universal, open-source framework that enables seamless collaboration and interoperability.

OpenUSD：统一 3D 工具的革命性框架

OpenUSD 正在彻底改变 3D 行业，通过提供一个通用的、开源的框架，实现了不同工具之间的无缝协作和互操作性。本视频将深入探讨 OpenUSD 如何解决复杂 3D 场景交换中的诸多行业顽疾，包括互操作性、协作和数据管理等问题。无论您是 3D 艺术家、设计师还是开发者，了解 OpenUSD 的强大功能将帮助您在 3D 内容创作中取得更大的成功。立即观看，探索 OpenUSD 如何统一 3D 工具，推动行业创新。

00:00 Introduction
00:48 OpenUSD Adoption
01:24 The Unifier & Characteristics
03:16 NVIDIA Pivotal USD Role 
03:56 NVIDIA New & Free Learn OpenUSD DLI Courses
04:53 Industry Backing Alliance

https://www.youtube.com/watch?v=QsRcUreM_LY -->

## BMW Group Celebrates Opening the World's First Virtual Factory in NVIDIA Omniverse

<!-- In the age of AI, new manufacturing factory projects are going digital-first. Running real-time digital twin simulations—virtually optimizing layouts, robotics, and logistics systems years before the factory opens—is the future. With Omniverse, the BMW team can aggregate data into massive, high-performance models, connect their domain-specific software tools and enable multi-user live collaboration across locations. All of this is possible from any location on any device. See the next evolution of BMW Group's adoption of NVIDIA Omniverse come to life in the official virtual opening on the automaker's new electric vehicle plant, opening in Debrecen, Hungary, in 2025.

Learn More About NVIDIA and BMW's Partnership: https://www.nvidia.com/en-us/industries/automotive/partners/

https://youtu.be/g78YHYXXils -->

## How NVIDIA's Isaac ROS 3.0 is Shaping the Future of Robotics

<!-- In this exciting video, we explore NVIDIA's groundbreaking announcement at Computex 2024 in Taipei, where CEO Jensen Huang unveiled the Isaac ROS 3.0 Robotics Platform. This innovative platform is set to transform the development and deployment of autonomous robots across various industries.

https://youtu.be/xPzEKTZSp4g -->

## AI Warehouse: AMR Visual Navigation with Isaac Sim & Isaac ROS

<!-- Autonomous machines are forecasted to dramatically increase the efficiency of warehouses, factories, and other industrial environments. NVIDIA Isaac ROS GEMs enable novel applications by empowering robots to intelligently perceive complex 3D environments. 

In this video, we showcase a camera-based navigation pipeline in which a robot uses NVIDIA’s GPU-accelerated visual SLAM algorithm (https://github.com/NVIDIA-ISAAC-ROS/i...) to find its location in the world. GPU-accelerated, real-time 3D scene reconstruction (https://github.com/NVIDIA-ISAAC-ROS/i...) is used to map its environment and plan collision-free trajectories. Finally, we demonstrate how Replicator, which is part of NVIDIA Isaac Sim (https://developer.nvidia.com/isaac-sim), can be used to procedurally generate industrial spaces in which to develop and validate robotics systems. 

Empower your robot with GPU-accelerated robotics algorithms today!

https://youtu.be/afNVwoX_zd0 -->

## Why Open USD is a Game-Changer for 3D Animation

<!-- Universal Scene Description (USD) is the future of 3D Animation and CG. Learn more about USD's and what they are! 

What is Open USD
Originally developed by Pixar Animation Studios, USD, also referred to as OpenUSD, is more than a file format. It's an open-source 3D scene description used for 3D content creation and interchange among different tools. USD is a high-performance extensible software platform for collaboratively constructing animated 3D scenes, designed to meet the needs of large-scale film and visual effects production. USD enables robust interchange between digital content creation tools with its expanding set of schemas, covering domains like geometry, shading, lighting, and physics. USD’s unique composition ability provides rich and varied ways to combine assets into larger assemblies, enables collaborative workflows so that many creators can work together with ease, and more.

https://youtu.be/xZ8wCC9qhWE -->

## Demonstration of New Torobo and New Torobo Hand in Isaac Sim and MuJoCo

<!-- This is a demonstration of the new Torobo and new Torobo Hand in Isaac Sim and MuJoCo.
To accelerate robotics development, we've open-sourced our simulation models of the new Torobo and new Torobo Hand. These models can be used in both Isaac Sim and MuJoCo.
If you're interested in using these models yourself, please visit our GitHub repositories and our blog (available in Japanese only) for more information and setup instructions.

GitHub Repositories: 
Isaac Sim: https://github.com/TokyoRobotics/torobo_usd_models
MuJoCo: https://github.com/TokyoRobotics/torobo_mujoco
Blog posts: 
Isaac Sim: https://qiita.com/mamoru_oka/items/c07c2b4e1fdb3216d739
MuJoCo: https://qiita.com/mamoru_oka/items/d7c4f412b8e76427956c

Tokyo Robotics Inc.: https://robotics.tokyo/ -->

## OpenUSD 3D Worlds: A Community Digital Art Showcase | NVIDIA Studio Standouts

<!-- Take an adventure into 3D worlds connected by OpenUSD. We've gathered some of our favorite artists & art from around the world to feed your inspiration. Sit back, relax, and travel into these stunning creative worlds. 

OpenUSD 3D 世界：一场社区数字艺术盛宴

欢迎进入由 OpenUSD 连接的 3D 世界。本视频汇集了来自全球的多位顶尖艺术家及其作品，旨在激发您的创作灵感。通过 OpenUSD 的强大功能，这些艺术家们创造了令人叹为观止的数字艺术作品。坐下来，放松身心，一起探索这些令人震撼的创意世界。立即观看，感受 OpenUSD 如何将社区的艺术创造力汇聚一堂，呈现一场视觉盛宴。

Learn more about OpenUSD:https://www.nvidia.com/en-us/omniverse/usd/
https://youtu.be/ywNYWYZTti4 -->

## Using Isaac Sim with ROS2: A Step-by-Step Guide

<!-- In this tutorial I explain how to use Isaac Sim with ROS2. 

Ubuntu 22.04 LTS
ROS2 Humble

The project is here: https://drive.google.com/drive/folders/1fdtBgbXUCxln3wdQgWUAGIBdBf-2Qv9z

https://www.youtube.com/watch?v=L1rpxRm0Q1w -->

## [Isaac Sim Core API] Introduction to Lecture Code Setup
https://github.com/kimsooyoung/rb_isaac_edu

https://youtu.be/hP01sOYtXKM

## AI Warehouse Robot Simulator
AI Warehouse Robot Simulator is a technology demonstration of simulating, suggesting, and training robots suitable for warehouse technical constraints based on the warehouse model to achieve greater efficiency, accuracy, and flexibility in logistics, supply chain operations, and warehouse management while also enhancing workforce safety and reducing the risk of workplace injuries and other health emergencies. 
AI Warehouse Robot Simulator offers the following functionality: 
• Exporting an Autodesk® Revit® warehouse model geometry for further training and simulation of the AI-based robots in NVIDIA Isaac Sim™ software. 
• Training of AI-based robots according to warehouse technical constraints. 
• Automatic simulation of AI-based robots. 
• Obstacle-avoidance navigation of AI-based robots. 
• Interactive testing of trained robots.

https://www.amcbridge.com/technology-demos/labs/ai-warehouse-robot-simulator

## Retexturing an Animated Character from Blender on the Fly in OpenUSD
<!-- Open new possibilities with OpenUSD. In this Studio Shortcut hosted by Rafi Nizam learn how you can use OpenUSD to retexture an animated character from Blender in real time for a live show.

Learn more about OpenUSD: https://www.nvidia.com/en-us/omniverse/usd/

https://www.youtube.com/watch?v=PUnT_dVoA7U -->

## Animate & Simulate: OpenUSD Techniques with Blender and USD Composer hosted by Rafi Nizam
<!-- Open new possibilities with OpenUSD. In this Studio Shortcut hosted by Rafi Nizam learn how you can use OpenUSD and Blender to optimize your animation & simulation workflow.

https://youtu.be/_XAznb9XYAs -->


## 10 Billion Humanoid Robots by 2040? Elon Musk's Future Predictions as Tesla Bot Moves to Production

<!-- Elon Musk laid out his prediction in an interview with futurist Peter Diamandis at the Future Investment Initiative Conference in Saudi Arabia. Musk envisions the robots playing a significant role in sustaining productivity as global populations age and birth rates decline.

Analysts' projections generally fall short of Musk's vision, ranging from 1 billion to several billion humanoids globally by 2040.

Morgan Stanley projects around 8 million humanoid robots in the US alone by 2040, mainly due to automation in labor-intensive sectors like healthcare and manufacturing

Venture capitalist David Holz estimates there could be a billion humanoid robots worldwide by 2040 thanks to advancements in AI and demand driven by labor shortages and aging populations.

Financial firms like Macquarie predict that the market could exceed 3 trillion in value by 2050 as humanoid robots integrate into industry and households.
Though estimates are all over the place, experts generally agree adoption of humanoid robotics will grow significantly in the coming years.

https://www.youtube.com/watch?v=iEbcWny1eEs -->

## Quadruped Robot Basics

<!-- 这个视频解释了一些关于四足机器人的基本理论，以及四足机器人PID控制和在ROS中的代码实现，基于ROS Noetic
项目就在这里：
https://drive.google.com/drive/folders/1m5nQb7YX3kwI-CwQ-ZCpICSNdDGPRp1k

https://youtu.be/O_2swSMecB4 -->

## Simplifying Complex Scenes: The Power of Layers in OpenUSD

<!-- 了解如何在 OpenUSD 中充分利用图层。在 Rafi Nizam 主持的 Studio Shortcut 教程中，了解如何在单个 USD 文件中快速创建带有图层的布局选项。

https://www.youtube.com/watch?v=Z3fVBkvXQKM -->

## 3D 

https://adsknews.autodesk.com/en/news/autodesk-launches-wonder-animation-video-to-3d-scene-technology/


## The entire system now works in Nivdia's new toolkit IsaacLab
https://youtu.be/bNTk4gmtJxo

## maskedmimic
https://research.nvidia.com/labs/par/maskedmimic/

## The Future of Physical AI, Robotics, and Autonomy
<!-- Join us for a strategic session presented by NVIDIA's Rev Lebaredian, where we'll explore the integration of NVIDIA’s advanced technologies: OVX for simulation, DGX for AI model training, and Jetson for edge inference. These platforms collectively create a robust ecosystem for developing, simulating, and deploying cutting-edge AI solutions. Omniverse enables collaborative, real-time simulations with physically accurate environments, crucial for training AI models to perform reliably in complex, real-world scenarios. The DGX platform's computational power significantly accelerates the AI development cycle, while the edge platform supports real-time AI inference—vital for applications ranging from autonomous systems to intelligent IoT. This session will provide valuable insights into how these technologies can transform AI development and deployment, ensuring robust performance and efficiency in various applications.
https://www.nvidia.cn/on-demand/session/aisummitdc24-sdc1077/ -->

## From Research to Reality, Putting Mobile Robots to Work
<!-- Learn what mobile robots are up to in the real world and how Boston Dynamics applies emerging AI to deliver value to our customers. We are witnessing the transition from research to reality for robots capable of moving around, and interacting with, the complex world around us. Boston Dynamics has a rich history of robotics research with origins fueled by bold government investments. Join us for a glimpse of that history as we review the path from the first uncertain steps to recent success applying emerging AI tools to solve real world customer problems in a rapidly growing new market.

观看该视频，深入了解移动机器人技术从实验室到实际应用的演变过程。在这次演讲中，波士顿动力的专家将回顾公司的悠久历史，从早期的政府研究项目到如今的商业成功，展示移动机器人技术的快速发展。
历史回顾：从2005年的“大狗”项目到2020年推出的Spot机器人，波士顿动力如何逐步攻克四足和双足机器人的关键技术。
技术进展：介绍最新的AI技术，如深度学习和强化学习，如何提升机器人的感知、导航和操作能力。
实际应用：展示Spot机器人在工业检查、物流和公共安全等领域的具体应用案例，包括温度测量、图像采集和风险识别。
未来展望：探讨如何通过行为克隆、远程操作和大规模数据集，进一步提升机器人的智能化水平，解决劳动力短缺等问题。
这场演讲将为观众提供深入的技术洞察，展示波士顿动力如何将前沿的AI技术应用于移动机器人，推动自动化和智能化的未来发展。
https://www.nvidia.cn/on-demand/session/aisummitdc24-sdc1092/ -->

## Build and Deploy Visual AI Agents with Vision Language Models
<!-- Join this session to discover the transformative impacts of multimodal vision language models (VLMs) and visual AI agents on city spaces, infrastructure, and robotics. From enhancing security of public safety to improving operational efficiency for city operations teams, learn how to build and deploy advanced edge AI systems for user-adaptive alerts and analytics, remote monitoring, and intuitive automations.
We’ll dive into the evolution and optimization of VLMs and Transformer-based generative AI architectures like Llama, LLaVA, VILA, and OpenVLA. Then we’ll discuss how models are connected with powerful embeddings and vector databases to enable sophisticated and context-aware visual agents with long-term memory and reasoning abilities. Leverage NVIDIA's reference workflows to create visual agents with edge-to-cloud integration and local compute onboard Jetson Orin for bringing next-generation AI into the field.
https://www.nvidia.cn/on-demand/session/aisummitdc24-sdc1057/ -->

## Advancing OpenUSD for the Era of Industrial Digitalization and Physical AI
See how NVIDIA and our partners are advancing OpenUSD as the data ecosystem for industrial digital twins and the next era of AI, including physical AI in autonomous vehicles and robots. We’ll cover pipelines for sensor simulation, CFD, data optimization frameworks, and the first of many OpenUSD generative AI models.
https://www.nvidia.cn/on-demand/session/aisummitdc24-sdc1102/

## NVIDIA Isaac Sim - Powering Robotics Development
<!-- Isaac Sim生态系统合作伙伴在CES 2023展示了他们如何使用NVIDIA Isaac Sim来满足他们的机器人仿真需求。
https://youtu.be/lcee9ntkOjk -->

## Unleashing Intelligent, End-to-End Operational Uncrewed Systems

<!-- Northrop Grumman will discuss their use of federated modeling and simulator tools, including NVIDIA Omniverse, to create a digital twin of complex UxV scenarios. The digital twin is used in the planning, rehearsal, and execution phases of missions that comprise heterogenous multi-domain autonomous teams of uncrewed systems. The digital twin enables Northrop Grumman to develop AI-enabled autonomy and collaborative teaming behaviors, train in simulation, and operate on live platforms in real environments. Northrop Grumman will discuss the humanitarian use cases for this approach and how it can improve operational efficiency for search and rescue.

数字孪生驱动的无人系统革命：从模拟到实战

在本次演讲中，Northrop Grumman 将展示其如何利用联合建模和仿真工具，包括 NVIDIA Omniverse，创建复杂无人系统（UxV）场景的数字孪生。这一创新方法贯穿于任务规划、排练及执行的各个阶段，涵盖了异构多域自主团队的协作。通过数字孪生技术，Northrop Grumman 不仅能够开发AI赋能的自主性和协同行为，还在模拟环境中进行训练，并最终应用于现实环境中的实际平台。

特别地，Northrop Grumman 将探讨该技术在人道主义救援领域的应用案例，如搜索与营救任务，展示了如何通过这些先进技术提高操作效率和响应速度。观众将了解到如何借助数字孪生实现更高效的任务准备和执行，以及它对提升无人系统性能的关键作用。

https://www.nvidia.com/en-us/on-demand/session/aisummitdc24-sdc1087/ -->


数字无人系统（Digital Unmanned Systems, 或简称 UxV） 是指那些通过数字化技术和人工智能（AI）增强的无人操作平台，它们可以在没有人类直接干预的情况下自主执行特定任务。这些系统通常包括无人驾驶车辆（如无人机、无人地面车辆、无人水面舰艇和无人潜航器），并且可以集成多种传感器、通信设备和其他专用硬件。

## Exploring the Power of Generative AI in National Security
Isaac J. Faber, Director, U.S. Army Artificial Intelligence Integration Center (AI2C)
Schuyler Moore, CTO, U.S. Central Command
Collen Roller, Senior Computer Scientist, Air Force Research Laboratory (AFRL)

This session explores some of the Department of Defense’s most exciting science and research initiatives in Generative AI for national security. These initiatives by the Air Force Research Lab and the Army AI Integration Center (AI2C) are contributing to key learning about the potential for Generative AI and are influencing policy, infrastructure, acquisition, and other considerations as the DOD explores the potential applications of this critical technology by leveraging prototype capabilities like the AI2C’s CAMOGPT to accelerate experimentation.

https://www.nvidia.com/en-us/on-demand/session/aisummitdc24-sdc1073/

## Creating the Factory of the Future With Sensor Fusion, AI Agents, and Digital Twins
<!-- "We're witnessing the evolution of factory automation with generative AI, visual language models (VLMs), sensor fusion, and digital twins.
Learn how these technologies are transforming factory operations with automated visual inspection, sensor fusion for worker safety, and AI agents for operations managers to interact with their factory using visual Q&A and video summarization.
Paving the way through this technical intersection is Plato Systems, an NVIDIA Metropolis partner, who will be presenting example deployments that use multi-camera tracking for highly accurate visual perception to increase efficiency and safety for workers."

打造空间智能工厂：多传感器融合、AI智能体和数字孪生技术

来自Plato Systems公司的分享，探索未来工厂自动化的新趋势。
在这次演讲中，Plato Systems展示如何通过生成式AI、视觉语言模型（VLMs）、传感器融合和数字孪生技术，彻底改变工厂运营。

一、技术亮点：
  1、多摄像头和毫米波雷达跟踪：结合多摄像头和毫米波雷达系统，实现高精度的空间感知，提高工人效率和安全性。
  2、自动视觉检测：利用先进的视觉技术，实现产品质量的自动检测，提高检测精度和效率。
  3、AI智能体：通过视觉问答和视频摘要技术，帮助运营经理与工厂进行互动，简化管理和决策过程。
二、具体案例：
  1、提高工人安全性：在一家制造工厂中，Plato Systems通过多摄像头系统实时监控操作员的动作，及时发现潜在的安全隐患，减少事故发生率。
  2、优化生产流程：通过AI代理，运营经理可以使用自然语言与工厂互动，获取实时数据和视频摘要，快速做出决策，优化生产流程。

https://www.nvidia.com/en-us/on-demand/session/aisummitdc24-sdc1059/

Plato Systems 是一家领先的人工智能驱动的空间智能平台提供商，专注于通过先进的数据分析和人工智能技术，帮助企业优化制造生产力。公司总部位于旧金山，致力于为制造业客户提供创新的解决方案，以提高生产效率、减少停机时间和优化资源配置。
结合计算机视觉和毫米波成像雷达（mmWave）技术，实现无标签活动追踪（Tagless Activity Tracking™），在任何环境中精确跟踪操作员和机器的互动。
公司主页：
https://www.plato.systems/ -->


## Exploring Physics-Based Digital Twins With OpenUSD

Join NVIDIA experts for a livestream exploring how digital twins are unlocking new possibilities for the world’s industries. Discover how developers use OpenUSD to build new workflows and applications that power physics-based, AI-enabled digital twins for real-time simulation and optimization of products, facilities and processes. 

https://youtu.be/4j4doXJ3uao


## Swiss-Mile Robot与员工百米赛跑
<!-- https://youtu.be/AcNCcAsCJV8
Swiss-Mile 是一家开发自主机器人的机器人公司。该公司的机器人可以行走、驾驶、直立、开门和搬运包裹。它们旨在解决劳动力短缺、降低成本、增强洞察力和优先考虑安全性。Swiss-Mile 的机器人被用于各种行业，包括安全、监控、建筑和物流。
公司主页：
https://www.swiss-mile.com/ -->

## Avular Robotics 展示

<!-- Avular Robotics专注于移动机器人技术，致力于将复杂任务自动化，节省时间和人力。提供Autopilots自动驾驶系统、Vertex One模块化无人机及Origin One驾驶机器人，已在多个领域如危险烟囱检查、火车站安全检查等取得显著成果。我们的技术助力企业加速自动化进程，提高工作效率，降低安全隐患。Avular期待与您共创未来，探索机器人技术的无限潜力。

https://youtu.be/fRTRNIor3YQ

公司主页：
https://www.avular.com/ -->

## Omniverse and Digital Twins: A Warehouse Simulation with Real-Time Analytics

<!-- This virtual warehouse  enables users to track forklifts in real-time, monitor cargo status, and simulate operations like unloading cargo. Integrating real-time inference results from Cordatus adds another layer to the digital twin, with real-time analytics available for monitoring safety, tracking asset utilization, and ensuring operational efficiency.The demo showcases Omniverse’s practical benefits for warehouse logistics, enhancing safety and efficiency and offering valuable insights into complex workflows within a real-time, interactive digital twin environment.

https://youtu.be/_oy-N9CRq5s -->

## Fusing Real-Time AI With Digital Twins

<!-- Discover the AI that'll drive the next phase of industrial automation—how it'll be developed, refined, and first deployed in simulation in digital twins.   

Complex AI is being tested in real time inside an Omniverse digital twin of a warehouse, showcasing AI that's been developed inside this digital twin. 

It’s a workflow that developers can use to build AI gyms to train and evaluate complex AI, all in real time within the digital twin. This is something that otherwise would be incredibly costly or impossible to run in the real world—particularly for heavy industry, factories, and supply chains. 

This demo leverages NVIDIA Metropolis, Omniverse, CuOpt, and Isaac for robot perception to create an end-to-end concept of how to fully automate logistically complex co-bot spaces. 

https://youtu.be/l5M4sqaRd6w

详细博客：
https://blogs.nvidia.com/blog/ai-digital-twins-industrial-automation-demo/
NVIDIA cuOpt：
https://www.nvidia.com/en-us/ai-data-science/products/cuopt/
https://docs.omniverse.nvidia.com/isaacsim/latest/features/warehouse_logistics/logistics_tutorial_cuopt.html
NVIDIA Isaac Perceptor：
https://developer.nvidia.com/isaac/perceptor
https://github.com/NVIDIA-ISAAC-ROS/isaac_perceptor
NVIDIA Metropolis：
https://www.nvidia.com/en-us/autonomous-machines/intelligent-video-analytics-platform/ -->

## Autonomous Mobile Robot with Isaac Sim and ROS2

<!-- 我们很高兴地宣布bcr_bot现已与NVIDIA Isaac Sim兼容！
这种集成使我们能够利用Isaac Sim的高保真模拟和ROS 2来增强自主导航和地图的测试和开发。
它非常适合研究人员、开发人员和任何从事自主移动的机器人工作的人！

博客：https://www.blackcoffeerobotics.com/blog/isaac-sim-photorealistic-rendering-for-next-gen-robot-development
Github代码：https://github.com/blackcoffeerobotics/bcr_bot

https://youtu.be/gfKOj_HZiX0 -->

## Visual Navigation with NVIDIA Isaac Sim, Isaac ROS and Husky A200

A 30 second clip showing Freespace segmentation using NVIDIA Isaac ROS in NVIDIA Isaac Sim

https://youtu.be/zb5amdEbRTk

## HIL on NVIDIA Orin NX with Isaac ROS vslam and Nvblox

<!-- 使用Isaac对Orin NX进行硬件在环测试（视觉SLAM）

在NVIDIA Jetson Orin NX上运行硬件在环测试，结合NVIDIA Isaac ROS的视觉同时定位（vslam），使用NVIDIA Nvblox进行地图构建，并进行AprilTag标记检测。
硬件在环 (HIL) 测试是一种强大的工具，可用于验证和确认复杂系统（包括机器人和计算机视觉）的性能。
NVIDIA Isaac 平台通过其模拟器（Isaac Sim）和针对Jetson优化的软件（Isaac ROS）支持HIL测试，可优化机器人和计算机视觉算法性能。
NVIDIA Isaac 平台由 NVIDIA Isaac Sim（一种提供用于测试机器人算法的模拟环境的模拟器）和 NVIDIA Isaac ROS（针对 NVIDIA Jetson 优化的硬件加速软件，包含机器学习、计算机视觉和定位算法）组成。通过使用此平台进行 HIL 测试，您可以验证和优化机器人软件堆栈的性能，从而获得更安全、更可靠、更高效的产品。
Hardware-in-the-loop running on NVIDIA Jetson Orin NX with NVIDIA Isaac ROS vslam, mapping with NVIDIA Nvblox and apriltag detection.

了解更多，
英文：https://developer.nvidia.com/blog/design-your-robot-on-hardware-in-the-loop-with-nvidia-jetson/
中文：https://developer.nvidia.com/zh-cn/blog/design-your-robot-on-hardware-in-the-loop-with-nvidia-jetson/ -->

## Advancing Humanoid Robot Sight and Skill Development with NVIDIA Project GR00T
https://developer.nvidia.com/blog/advancing-humanoid-robot-sight-and-skill-development-with-nvidia-project-gr00t/

当在模拟机器人上测试GR 00 T-Mobility时，机器人成功地在混乱的环境中导航，同时避开障碍物；
在真实的机器人上测试GR 00 T-Mobility时，机器人成功地在实验室环境中导航，同时避开盒子和障碍物；

<!-- GR00T 在NVIDIA Isaac实验室接受过基于控制工作流程的策略培训。红色框是来自数据集的参考机器人身体位置。机器人在跟踪参考运动
在艾萨克实验室一起训练数千个人形机器人
https://developer.nvidia.com/blog/advancing-humanoid-robot-sight-and-skill-development-with-nvidia-project-gr00t/ -->

<!-- 解机器人如何使用生成式AI进行推理并使用ReMEmbR采取行动
https://developer.nvidia.com/blog/using-generative-ai-to-enable-robots-to-reason-and-act-with-remembr/

用于机器人导航的长视界时空记忆的构建和推理
https://github.com/NVIDIA-AI-IOT/remembr -->

## 

## Farming Robot, Row Following with Isaac Sim and ROS 2
<!-- The project features:
Powered by ROS 2 and Isaac Sim for robust simulation and control
High-Fidelity Graphics providing realistic and detailed visuals
Computer Vision and ML for accurate row detection
Path Generation with Nav2 ensuring smooth U-turns and navigation
Synthetic Dataset Generation for enhanced training and performance

The project is done as a Master Project by our student Milos Nikolic.

https://youtu.be/RxzKO8GPJqY -->

## The Future of Work - Avular Robotics Showcase

<!-- https://www.avular.com/ -->

## How to import my manipulator 【omniverse】

https://github.com/Road-Balance/RB_MirobotExample
https://github.com/Road-Balance/mirobot_ros2

https://youtu.be/kRAZZ5OPZyM

## How to Install NVIDIA Isaac Sim

<!-- "What's the difference between Omniverse and Isaac Sim?"

This a common question we receive as we continue to release new videos on digital twins.

We thought this was a good moment to show you how to start from the ground up, and answer that.

https://youtu.be/SyrsAd8WbCo -->

## Get Set Up With Demo Scenes in NVIDIA Omniverse

<!-- There's a ton of potential in the demo scenes that come with ‪@NVIDIA‬ Isaac Sim. Let's show how they can connect gaming controllers with your digital twin. It's super great that these demos include both source code and visual scripting examples to show how to get started.

一个简单的展示Isaac Sim自带的有关游戏手柄的例程，Isaac Sim的演示场景具有巨大的潜力，演示包括源代码和可视化脚本示例，以展示如何开始。
https://developer.nvidia.com/isaac/downloads

https://youtu.be/YEK10l19cAg -->

## How to Use MoveIt with Isaac Sim: A Step-by-Step Guide
<!-- In this tutorial I explain how to setup a robotic arm using “MoveIt setup assist” and how to operate a robotic arm in Isaac Sim using MoveIt.

The project is here:
https://drive.google.com/drive/folders/1Bl-ULA9X3lplRguwXPpz8ZI_T7EpyADH
Artiiculations:
https://docs.omniverse.nvidia.com/extensions/latest/ext_physics/articulations.html
Rigging Complex Structures:
https://docs.omniverse.nvidia.com/isaacsim/latest/advanced_tutorials/tutorial_advanced_rigging_complex_structures.html

https://youtu.be/pGje2slp6-s -->

## 

<!-- 厨师兼社交媒体名人Nick DiGiovanni与1X Technologies的人形机器人NEO合作制作三分熟的牛排 -->

<!-- Neo
www.instagram.com/1x.technologies
www.1x.tech -->

## https://github.com/space-station-os

## [OMNIVERSE] Ultimate Guidance for Isaac Sim Wheeled Robots

Github Repository for this video
https://github.com/Road-Balance/RB_WheeledRobotExample

https://www.youtube.com/watch?v=XEri32NaLYk

## [OMNIVERSE] Create ROS 2 Robotics Simulation with USD Composer

https://youtu.be/NsssRJpid7s

## [Isaac Sim Core API] Lecture1 Hello Isaac Sim
<!-- 使用NVIDIA Isaac Sim学习机器人仿真的教育示例集合
https://github.com/kimsooyoung/rb_isaac_edu

https://youtu.be/yJwdIx9KEfw -->

## [Isaac Sim Core API] Lecture2 Hello Assets

<!-- 使用NVIDIA Isaac Sim学习机器人仿真的教育示例集合
https://github.com/kimsooyoung/rb_isaac_edu

https://youtu.be/ZGdoFgekXO4 -->

## [Isaac Sim Core API] Lecture3 Hello Deformable Objects
<!-- 使用NVIDIA Isaac Sim学习机器人仿真的教育示例集合
https://github.com/kimsooyoung/rb_isaac_edu

Deformable-Body 官方教程和文档：
https://isaac-sim.github.io/IsaacLab/main/source/tutorials/01_assets/run_deformable_object.html
https://docs.omniverse.nvidia.com/extensions/latest/ext_physics/deformable-bodies.html

https://youtu.be/lnpb3DYyWxM -->

## [Isaac Sim Tutorial - Core API] Lecture4 Hello Replicator
<!-- 使用NVIDIA Isaac Sim学习机器人仿真的教育示例集合
https://github.com/kimsooyoung/rb_isaac_edu -->

## 

## AI Robotics: Driving Innovation for the Future of Automation
<!-- Deepu Talla, VP – AI and Robotics, NVIDIA

Transforming the world of robotics, AI is fundamentally altering the way we build and deploy robots. Join Deepu Talla, Vice President of Robotics and Edge Computing, to uncover insights into cutting-edge technologies from NVIDIA Robotics that span from the cloud to the edge. These groundbreaking innovations are poised to advance the entire robotics ecosystem by harnessing the power of AI and GPU-accelerated simulation.

https://www.nvidia.com/en-us/on-demand/session/gtc24-s63287/ -->

## Challenges in Robot Manipulation and Locomotion: Integrating Planning, Perception, and Control
Rajesh Kumar, Principal Scientist, Addverb

Challenges in Robot Manipulation and Locomotion  Integrating Planning, Perception, and Control   AI Summit India 2024   NVIDIA On-Demand NVIDIA (index) (via Skyload)

In this session, we'll discuss how Addverb tackles the main challenges in real-world robot operations, covering our use of various techniques from traditional robot control algorithms to the latest AI models and algorithms. We'll also explain how we integrate NVIDIA AI technology for 3D perception, movement, and manipulation, and NVIDIA Isaac Lab and Omniverse for training and simulation. Finally, we'll look at the future of robotics, focusing on how classical control and perception can work with AI and advanced hardware, and Addverb's role in this evolution with 
its legged robots, collaborative robots, and autonomous mobile robots.

https://www.nvidia.cn/on-demand/session/aisummitin24-siin1054/

## Accelerating AI-Enabled Robotic Application Development—Universal Robot's AI Accelerator
James Davidson, Chief AI Officer, Teradyne Robotics

In this presentation, we introduce UR's new AI Accelerator, which harnesses the power of NVIDIA Isaac Manipulator to greatly simplify and accelerate the development of AI-enabled robotics applications. We begin with a comprehensive overview of our new product, delving into the intricacies of our "smart skills" platform, which provides flexible components to rapidly assemble new applications. To demonstrate the platform's capabilities, we showcase several reference applications, illustrating how these can be efficiently constructed using our "smart skill" components. This practical demonstration will highlight the versatility and power of our solution in real-world scenarios. We conclude with an exploration of the platform's extensibility, discussing how developers can leverage and expand upon our technology to meet diverse and evolving AI challenges.

https://www.nvidia.cn/on-demand/session/aisummitin24-siin1227/

## Accelerating Robot Learning with Isaac Lab and OpenUSD

<!-- Isaac Lab is an open-source framework built on NVIDIA Isaac Sim, designed to simplify workflows for reinforcement, imitation learning and more. As the successor to Isaac Gym, it complements platforms like NVIDIA Isaac Sim and MuJoCo, making it ideal for training robots in tasks like locomotion, navigation, and manipulation.

Join our livestream to explore Isaac Lab’s features and benefits, including a live demo of real-world applications. 1X Technologies, a humanoid robot company, will also join us to discuss their use of Isaac Lab and their 1X World Model Challenge, now enhanced with the NVIDIA Cosmos tokenizer.

加入NVIDIA的专家，了解如何通过Isaac Lab和OpenUSD加速机器人学习。Isaac Lab是一个基于NVIDIA Isaac Sim的开源框架，旨在简化强化学习、模仿学习等工作流。作为Isaac Gym的继任者，Isaac Lab与NVIDIA Isaac Sim和MuJoCo等平台互补，非常适合训练机器人完成诸如移动、导航和操作等任务。
技术亮点：
Isaac Lab框架：介绍Isaac Lab的开放源代码特性，以及如何简化机器人学习的工作流。
实时演示：展示Isaac Lab在真实世界中的应用，包括强化学习和模仿学习的具体案例。
1X Technologies合作：1X Technologies公司将分享他们如何使用Isaac Lab进行人形机器人开发，并介绍他们的1X World Model Challenge，现在已通过NVIDIA Cosmos tokenizer增强。
这场直播将展示Isaac Lab如何通过先进的仿真和学习技术，提升机器人训练的效率和效果，适合对机器人技术、AI和仿真平台感兴趣的观众观看。

https://youtu.be/8NoJZkabpGA -->

## Omniverse RTX Realtime render by Cineshare
<!-- Omniverse实时光追渲染概念车展示
Nvidia Concept car rendered in Downtown West cityscape by PurePolygons.

场景来自UE Downtown West 
https://www.fab.com/zh-cn/listings/0faf8b5d-7a5f-4fee-a297-7a8efaba8896

Concept car 在 Nucleus 里 Samples/Showcases/ 目录下
或在这里了解与下载：
https://docs.omniverse.nvidia.com/auto-config/latest/overview.html

https://youtu.be/hCfSIZayXqY -->

## Actuate 2024 | Michael Laskey | Building Robot Caretakers for our Planet with End2End Learning
At Electric Sheep, we are scaling physical agents across the country to care for our parks and community spaces. Our robots perform tasks such as; mowing, string trimming, and weed treatment using a both foundational world model, ES-1, and Reinforcement Learning. ES-1 is a learned model that consumes time-series information and is capable of predicting both raw features and interpretable outputs useful for outdoor work; such as semantics, a birds-eye-view map, robot pose and traversability. Given this world representation, we can then extract embeddings to teach our agents how to act with RL. In this talk, I will detail how we leverage sim2real techniques to train our models and achieve surprisingly robust simulation transfer across thousands of real yards. I will then delve into how we can leverage language based feedback to improve performance in the real world.
https://youtu.be/9uhJRweP_7Y

## Building Our Workshop in NVIDIA Omniverse

Something we have learned through our work with digital twins, is the importance of building a high-fidelity model of the machine and space you're working with, as soon as possible. As we're continuing to build digital twins for various projects, we realized a serious need for a virtual Loupe workshop. 
So, we collaborated with ATT Metrology Solutions in an effort to scan our HQ in SE Portland. We were absolutely floored at their ability to come in and quickly scan our space, using Leica imaging technology. 

Our very own Josh Polansky brought those meshes into NVIDIA Omniverse and we'll let the results speak for themselves.  Huge shoutout to the ATT team for this collaboration. Stay tuned for more!

https://youtu.be/czHlKOu8DUI

## Spot Watches Its Step | Boston Dynamics

<!-- On a typical day, our customers are operating the Spot® robot in factories, foundries, substations, sub basements, and beyond. Spot encounters all kinds of obstacles and environmental changes, but it still needs to safely complete its mission without getting stuck, falling, or breaking anything. While there are challenges and obstacles that we can anticipate and plan for—like stairs or forklifts—there are many more that are difficult to predict. To help tackle these edge cases, we used AI foundation models to give Spot a better semantic understanding of the world.

Learn more on our blog: https://bostondynamics.com/blog/put-it-in-context-with-visual-foundation-models/

https://youtu.be/YD9EaS3VRbc -->

## I put ChatGPT on a Robot and let it explore the world
<!-- My tools: https://indystry.cc/my-tools
Interested in ML/AI robots? https://indystry.cc/ml-robot/
My robot chassis: https://indystry.cc/product/orp-robotics-chassis/
ElevenLabs: https://elevenlabs.io/?from=partnerwilson6369

https://youtu.be/U3sSp1PQtVQ -->

## Animatronic "Thing" From Netflix's Wednesday!
<!-- Adam welcomes special effects fabricator Jesse Velez to the cave alongside our longtime friend Ben Eadie to check out their collaboration in the Thing animatronic robot they made for the promotion of Netflix's Wednesday show! Jesse, who runs Raptor House FX, walks Adam through the journey of designing and making this incredible animatronic work and walk in such a lifelike way. We even peel behind the skin of this robot to check out its internal movement mechanisms. 

https://youtu.be/w3cvE8jtxuw -->

## Actuate 2024 _ Panel Discussion_ The Role of Simulation in Robotics Development
<!-- Kat Scott, OSRA, hosts a panel, featuring experts like Allison Thackston (Blue River Technology), Ilia Baranov (Polymath Robotics), Kathleen Brandes (Adagy Robotics), and Rajat Bhageria (Chef Robotics), emphasized the critical role simulation plays in robotics development. Panelists explored how simulation tools can accelerate prototyping, testing, and deployment, offering insights into how developers can leverage these platforms to refine their robotics solutions.
Nvidia一直提仿真优先，但是游戏世界与仿真世界及实际专业电影制作世界之间存在鸿沟，unreal等引擎场景很好看，但是那只是一些技巧，
https://youtu.be/RtwJLsC3bAA -->

## 

## 使用螺旋形浮筒漂浮在水面上的两栖机器人

https://mashable.com/video/helix-robots-coppertstone-technologies


## A “Simulation First” Approach to Developing Physical AI-Based Robots With OpenUSD

The future of AI-powered robots hinges on their ability to autonomously sense, plan, and act in the physical world. Achieving this level of autonomy requires a "simulation-first" approach. In this livestream, we will explore NVIDIA's Isaac Sim, a robotic simulation platform built on OpenUSD, designed for developing, simulating, and validating robots. We'll showcase workflows that developers can use to enhance their robotic systems with Isaac Sim.

Foxglove, an NVIDIA Inception partner, will also demonstrate their platform's capabilities in visualization, debugging, and managing robotics data. They will present a custom extension for visualizing Isaac Sim simulations in real time and discuss how their platform’s visualization tools complement Isaac ROS, NVIDIA’s hardware acceleration solution for ROS 2.

https://www.youtube.com/watch?v=pztkN1RFLKU

<!-- “仿真优先”驱动的具身智能机器人开发，探索Isaac Sim 与 OpenUSD 的创新应用

NVIDIA 的 Isaac Sim——一个基于 OpenUSD 构建的先进机器人仿真平台，专为具身智能机器人的开发、模拟和验证而设计。通过“仿真优先”的方法，Isaac Sim 能够显著提升机器人在物理世界中的自主感知、规划和执行能力。我们将展示一系列开发者工作流，包括如何利用合成数据生成、强化学习和高保真度模拟来优化机器人性能，并确保其在复杂环境中的可靠性。结合 OpenUSD 的灵活性和模块化特性，Isaac Sim 为构建可扩展且高效的仿真环境提供了强大支持，加速了从概念到实际部署的整个开发周期。 -->

<!-- Foxglove 实时可视化助力 Isaac Sim 与 ISAAC ROS

作为 NVIDIA Inception 计划的合作伙伴，Foxglove 将展示其平台在机器人数据可视化、调试和管理方面的强大能力。通过Isaac Sim Foxglove 插件，Foxglove 将演示如何实现实时可视化 Isaac Sim 的仿真结果，并详细讲解这些工具如何与 NVIDIA 的 ISAAC ROS（ROS 2 的硬件加速解决方案）无缝集成。结合 Foxglove 的多模态数据可视化工具和强大的数据分析功能，开发者可以更高效地调试和优化机器人系统，确保从仿真到实际部署的顺畅过渡。此次演示将为观众提供宝贵的见解，了解如何利用先进工具提升机器人开发和测试的效率与精度。
了解更多 Foxglove和该插件信息：
https://foxglove.dev/blog/technical-dive-into-the-foxglove-isaac-sim-extension -->

## Jensen Huang Special Address from NVIDIA AI Summit Japan
<!-- Watch NVIDIA CEO Jensen Huang’s Special Address from AI Summit Japan to learn how AI is driving innovation, economic growth, and global leadership for Japan. Jensen then sits down with special guest Masayoshi Son, Chairman and CEO of SoftBank Group Corp., to discuss AI’s role in transforming industries and Japan’s rise as a global AI leader.
https://youtu.be/x8O6ChAWBxs -->

## Creating Real-Time Computer-aided Engineering Digital Twins with NVIDIA Omniverse Blueprints

<!-- Everything that is manufactured is first simulated with advanced physics solvers. Real-time digital twins are the cutting edge of computer-aided engineering (CAE) simulation. They empower engineers to innovate freely and rapidly explore new designs by experiencing in real time the effects of any change in the simulation. This video shows how Luminary Cloud instantiated the Omniverse Blueprint for Real-Time Computer-Aided Engineering Digital Twins to realize a real-time virtual wind tunnel. This is one of the first applications of Omniverse Blueprints, and it illustrates how blueprints simplify creating real-time digital twins for computational fluid dynamics (CFD) simulation to advance the designs of cars, airplanes, ships, and many other products.
https://youtu.be/RIxrN7yedyQ

了解更多：https://build.nvidia.com/nvidia/digital-twins-for-fluid-simulation -->

## NVIDIA SC24 Special Address

## JETSON AI LAB | SMACC State Machines in ROS2 & Kaya sim2real workflow (11/12/2024)

视频丢失！

Topics Covered:

00:00 - Transfusion/π0 and streaming omnimodality (https://arxiv.org/html/2408.11039v1)
13:20 - SMACC in Isaac Sim (Brett Aldrich, robosoft.ai - https://github.com/robosoft-ai/SMACC2)
17:25 - Carter shows their dance moves (   • This is what an autonomous applicatio...  )
47:20 - Kaya sim2real agents (Kabilan Kb -   / kabilankb2003  )
https://youtu.be/3QxRUdgnbJw


## BEHAVIOR Vision Suite Customizable Dataset Generation via SimulationStanford 2024
BEHAVIOR Vision Suite- Customizable Dataset Generation via Simulation(Stanford 2024)
https://youtu.be/WS1EP13PraM

## Sony Research

显微外科手术既涉及极其细微的动作，也涉及更大的动作。手术辅助机器人中使用的运动缩放，简化了微小的运动，但可能会降低较大运动的效率。 

为了解决这个问题，索尼开发了一种小型，轻便的控制设备

## 3D! Generative AI with NVIDIA Edify 3D

The creation of high-quality 3D assets is critical for industries like video game design, extended reality, film production, and simulation, where 3D content must meet stringent production standards such as precise mesh structures, high-resolution textures, and material maps. Meeting these standards is time-consuming and requires specialized expertise, a demand that has fueled research into AI-driven 3D asset generation. However, the limited availability of 3D assets for model training poses challenges, highlighting the need for scalable, efficient solutions.

Edify 3D addresses these challenges by generating detailed, production-ready 3D assets within two minutes, producing organized UV maps, 4K textures, and PBR materials. Using multi-view diffusion models and Transformer-based reconstruction, Edify 3D can synthesize high-quality 3D assets from either text prompts or reference images, achieving superior efficiency and scalability.


Edify 3D
Scalable High-Quality 3D Asset Generation

Expanding dimensions with NVIDIA Research. 

Using text and images as descriptions, developers and visual content creators can use NVIDIA Edify 3D to quickly generate 3D objects to create virtual worlds and prototype ideas. 

NVIDIA Edify 3D is a foundation model for 3D asset generation. The model is for both experienced and novice 3D creators and designers who need to quickly create 3D assets for ideation, lay out scenes, and conceptualize immersive environments. 

NVIDIA Edify 3D enables diverse applications, so creative professionals can rapidly iterate on 3D content creation. It also engages amateur creators in the exciting field of 3D visual design.

https://research.nvidia.com/labs/dir/edify-3d/

## Immersive Desert World -- From Idea to 3D Scene in Three Minutes | NVIDIA Research

NVIDIA researchers used NVIDIA Edify, a multimodal architecture for visual generative AI, to build a detailed 3D desert landscape within a few minutes in a live demo at SIGGRAPH’s Real-Time Live event. 

During the event — one of the prestigious graphics conference’s top sessions — NVIDIA researchers showed how, with the support of an AI agent, they could build and edit a desert landscape from scratch within five minutes. The live demo highlighted how generative AI can act as an assistant to artists by accelerating ideation and generating custom secondary assets that would otherwise have been sourced from a repository.

https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/

## Isaac ROS Office Hours: Isaac Perceptor

https://nvidia-isaac-ros.github.io/reference_workflows/isaac_perceptor/index.html

https://developer.nvidia.com/isaac/ros

https://youtu.be/ZHivDfuEGmE

## Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight (CoRL 2024)
Learning visuomotor policies for agile quadrotor flight presents significant difficulties, primarily from inefficient policy exploration caused by high-dimensional visual inputs and the need for precise and low-latency control. To address these challenges, we propose a novel approach that combines the performance of Reinforcement Learning (RL) and the sample efficiency of Imitation Learning (IL) in the task of vision-based autonomous drone racing. While RL provides a framework for learning high-performance controllers through trial and error, it faces challenges with sample efficiency computational demands due to the high dimensionality of visual inputs. Conversely, IL efficiently learns from visual expert demonstrations, but it remains limited by the expert’s performance and state distribution. To overcome these limitations, our policy learning framework integrates the strengths of both approaches. Our framework contains three phases: training a teacher policy using RL with privileged state information, distilling it into a student policy via IL, and adaptive fine-tuning via RL. Testing in both simulated and real-world scenarios shows our approach can not only learn in scenarios where RL from scratch fails but also outperforms existing IL methods in both robustness and performance, successfully navigating a quadrotor through a race course using only visual information. 

PDF: https://arxiv.org/pdf/2403.12203
YouTube: https://youtu.be/_awJMVU0H18
Project Website: https://rpg.ifi.uzh.ch/bootstrap-rl-with-il/index.html

## Reinforcement Learning - My Algorithm vs State of the Art

In this 3rd video about inverted pendulum balancing with AI, I compare the results I had with my own algorithm with a state of the art reinforcement learning algorithm using Isaac Lab, which is part of the NVidia omniverse platform.

https://youtu.be/pJfvPMNPZAU

## Advancing robotics and touch perception | AI Research from Meta FAIR
<!-- Advancing embodied AI through progress in touch perception, dexterity, and human-robot interaction
https://ai.meta.com/blog/fair-robotics-open-source/?utm_source=youtube&utm_medium=organic_social&utm_content=youtubedesc&utm_campaign=fair

Researchers at Meta FAIR are releasing several new research artifacts that advance robotics and support our goal of reaching advanced machine intelligence (AMI). These include Meta Sparsh, the first general-purpose encoder for vision-based tactile sensing that works across many tactile sensors and many tasks; Meta Digit 360, an artificial fingertip-based tactile sensor that delivers detailed touch data with human-level precision and touch-sensing; and Meta Digit Plexus, a standardized platform for robotic sensor connections and interactions that enables seamless data collection, control and analysis over a single cable.
https://ai.meta.com/blog/fair-robotics-open-source/

https://youtu.be/eyUZX-lCj4M -->

<!-- Digit Plexus旨在创建一个标准化平台，提供硬件-软件解决方案，以在任何机器人手上集成触觉传感器。该平台将基于视觉和基于皮肤的触觉传感器（如Digit、Digit 360和ReSkin）连接到指尖、手指和手掌上的控制板上，将所有数据编码到主机上。该平台的软件集成和硬件组件允许通过单一电缆进行无缝数据收集、控制和分析。 -->

## The Next Wave of AI: Physical AI
Advancements in accelerated computing and physics-based simulation, have led us to the next frontier of AI: Physical AI. 

Physical AI models can perceive, understand, interact and navigate the physical world with generative AI.  This new frontier of AI manifests itself through the embodiment of physical systems that go beyond a traditional AMR, robot arm or humanoid robot and instead, include everything from streetlights to data centers, healthcare facilities and manufacturing plants. With Physical AI, static systems will transform from static systems to dynamic, responsive systems. 

To enable developers to build physical AI, NVIDIA has built 3 computers. NVIDIA AI and DGX to train foundation models, @NVIDIAOmniverse to simulate and enhance AIs in a physically-based virtual environment, and the NVIDIA Jetson AGX, a robot supercomputer. 

The era of physical AI is here - transforming the world's heavy industries and robotics. Join the world’s leading companies and get started with NVIDIA Robotics now. 

Learn more about the three computers enabling physical AI and the foray into humanoid robots.

https://blogs.nvidia.com/blog/three-computers-robotics/

https://youtu.be/uhLDHA9skFk

NVIDIA开启物理AI新时代：从静态系统到动态响应
英伟达提出三电脑解决方案：驱动下一代AI机器人技术
从类人机器人到工厂，基于人工智能的工业物理系统正在通过训练、模拟和推理加速发展。
加速计算和基于物理的仿真技术的发展，引领我们进入了AI的新前沿：物理AI。
物理AI模型能够感知、理解、交互和导航物理世界，通过生成式AI实现更广泛的物理系统应用，从街灯到数据中心、医疗设施和制造工厂。
NVIDIA为此构建了三台关键计算机：
1、NVIDIA AI和DGX：用于训练基础模型。
2、NVIDIA Omniverse：在物理仿真虚拟环境中模拟和增强AI。
3、NVIDIA Jetson AGX：一款机器人超级计算机。
这些技术使得传统的静态系统（如固定的传感器和设备）能够转变为动态、响应式的系统。具体来说：
1、感知和理解：物理AI模型能够通过传感器和摄像头实时感知周围环境，并理解环境中的物体和事件。
2、交互和导航：这些模型能够与环境进行交互，执行复杂的任务，如抓取物体、导航复杂地形等。
3、实时响应：系统能够根据实时数据做出快速反应，调整行为以应对环境变化，从而提高效率和安全性。
通过这些技术，物理AI将彻底改变重型工业和机器人技术，实现更高效的运作和更智能的决策。

## Deep-Dive Into Isaac Lab Workflows | Isaac Lab Office Hours
<!-- Isaac Lab is a unified framework for robot learning built on NVIDIA Isaac Sim. This open-source modular framework aims to simplify workflows for reinforcement learning, imitation learning, and more. Isaac Lab is the successor to Isaac Gym. It is complementary and can be used alongside developer simulation platforms such as NVIDIA Isaac Sim and MuJoCo.

Join our livestream to explore Isaac Lab’s features and benefits. This livestream is ideal for simulation developers and roboticists trying to train robot instances at scale by building effective robot policies for locomotion, navigation, manipulation, and more.

https://youtu.be/_UHxP0FbOws -->

## ONYX NAUT & SENTRY - Ghost Robotics V60 robotic dog
Incredibly excited to share the next evolution of our systems - NAUT amphibious capabilities with SENTRY kinetic and ISR payloads. This integration on the Ghost Robotics V60 is just one example of the many options available

https://youtu.be/OQhYJ6I9c1I

Onyx：Onyx Industries总部位于华盛顿特区，在全球范围内运营，是一家由技术企业家、特种作战老兵、经验丰富的政府领导人和在该领域拥有数十年专业经验的私营行业专业人士组成的服务残疾退伍军人拥有的小型企业。利用智能技术助力复杂任务。
https://onyx-ind.com/

## Meta Robotic Hand: Feel with Vision Haptics! (Sparsh, Digit 360, Digit Plexus)

<!-- Meta just came out with Sparsh, Digit 360, and Digit Plexus, which make up the robotic hand platform for tactile sensing with vision. Their Sparsh Model is capable of 6 main tasks (Force Estimation, Slip Detection, Pose Estimation, Grasp Stability, Textile Recognition, Bead Maze). The Digit 360 and Plexus make up the hardware and software to integrate the fingers and robotic hand.

0:00 Introduction
0:17 Meta Sparsh 
2:32  Meta Digit 360
3:54 Meta Digit Plexus

https://ai.meta.com/blog/fair-robotics-open-source/
https://github.com/facebookresearch/sparsh
https://github.com/facebookresearch/digit360
https://github.com/facebookresearch/digit-plexus

https://youtu.be/RlhjSRt6b0o -->

## Using ChatGPT and GPT-4 to Generate 3D Content in Omniverse

See how APIs for GPT-4 and NVIDIA DeepSearch were combined to rapidly generate 3D objects with simple, text-based prompts in this experimental project from the Omniverse team.

To experiment with the AI Room Generator Extension Sample in Omniverse, access the code on GitHub: https://github.com/NVIDIA-Omniverse/kit-extension-sample-airoomgenerator

To learn more about how the extension was developed, read our Medium blog: https://medium.com/@nvidiaomniverse/chatgpt-and-gpt-4-for-3d-content-generation-9cbe5d17ec15#cid=ov01_so-yout_en-us

https://youtu.be/mFazJsjUUSo

## Visualizing Wind Simulation with the SimScale Extension for AECO

<!-- 视频中的 CREATE软件 目前已经重命名为 USD Composer软件，这个插件目前在 USD Composer 里可以搜到
By following this tutorial, users can gain a comprehensive understanding of how to visualize accurate wind performance results through advanced visualization in Nvidia Omniverse using the SimScale extension.
https://www.simscale.com/blog/wind-simulation-in-nvidia-omniverse/
https://youtu.be/QXcX-MicCa4 -->

## Omniverse Physics Extension

<!-- 视频中的演示都可以在 Omniverse USD Composer 或 Isaac Sim 中体验，
物理相关例程在 Window -> Simulation -> Demo Scenes 下。 -->

### Part 3: Rigid Bodies - Mass Editing

<!-- Omniverse 刚体物理教程：质量和质心编辑、力的施加、点实例化、连续碰撞检测及运动学物体物理设置

在本系列教程中，我们将深入探讨Omniverse中刚体物理的各个方面，包括质量和质心的编辑、力的施加、点实例化、连续碰撞检测以及运动学物体物理的设置。

1、质量和质心编辑
学习如何编辑刚体的质量和质心，使物体在物理模拟中表现出预期的行为。我们将通过具体的例子展示如何调整雪人的质量和质心，使其能够在倒下后自动恢复直立状态。
2、力的施加
了解如何为刚体添加力和扭矩，使其在物理模拟中产生运动。我们将通过一个火箭飞船的示例，展示如何使用力组件使火箭起飞并旋转。
3、点实例化
掌握点实例化技术，这是一种高效创建大量相似物体的方法。我们将通过一个复杂的砖塔示例，展示如何使用点实例化创建大量砖块，并在物理模拟中实现互动效果。
4、连续碰撞检测
学习如何启用和配置连续碰撞检测（CCD），以防止高速移动的物体穿透其他物体。我们将通过一个箱子掉落的示例，展示如何使用CCD确保物体在高速运动时正确检测到碰撞。
5、运动学物体物理设置（Kinematic Body Physics）
了解如何设置运动学体，使其在物理模拟中保持动画驱动的状态。我们将通过一个带有动画桨板的示例，展示如何使动画对象与动态刚体进行正确的交互。
kinematic物理体是一种不受力或碰撞影响的物理体，但它在运动时能引起影响其他物体的碰撞。

https://youtu.be/2qXFJ580RQM -->

<!-- This series covers the basics of the physics extension in Omniverse Kit 104, from the UI to fluid dynamics. 

In Parts 3 to 7, we cover rigid body physics. In these videos, you will learn how to edit the mass and center of mass, how to assign forces to objects, how to use point instancers, and how to set up continuous collision detection as well as kinematic actors. -->

<!-- https://youtu.be/2qXFJ580RQM -->

### Part 4_ Rigid Bodies - Forces

<!-- https://youtu.be/8ZdEeN-ZM4Y -->

### Part 5: Rigid Bodies - Point Instancer
<!-- This series covers the basics of the physics extension in Omniverse Kit 104, from the UI to fluid dynamics. 

In Parts 3 to 7, we cover rigid body physics. In these videos, you will learn how to edit the mass and center of mass, how to assign forces to objects, how to use point instancers, and how to set up continuous collision detection, as well as kinematic actors. -->

<!-- https://youtu.be/JX8c6xmra1g -->

### Part 6: Rigid Bodies - Kinematic Body Physics

<!-- This series covers the basics of the physics extension in Omniverse Kit 104, from the UI to fluid dynamics.

In Parts 3 to 7, we cover rigid body physics. In these videos, you will learn how to edit the mass and center of mass, how to assign forces to objects, how to use point instancers, and how to set up continuous collision detection as well as kinematic actors. -->

<!-- https://youtu.be/pBMyCr1W1EI -->

### Part 7: Rigid Bodies - Continuous Collision Detection (CCD)

<!-- This series covers the basics of the physics extension in Omniverse Kit 104, from the UI to fluid dynamics.  -->

<!-- https://youtu.be/ljg7jBXjwbA -->

### Part 8: Materials - Density

<!-- Omniverse 物理材质应用全解析：密度、摩擦、恢复系数与多材质网格

在本视频中，我们将学习在Omniverse中应用物理材质，演示如何通过材质为3D对象赋予真实的物理属性。
您将学习到设置物体密度以影响其质量表现的方法，理解不同密度材质如何改变物体之间的交互；
同时，我们还会讲解摩擦和恢复系数的配置，这些参数决定了物体间的滑动阻力以及碰撞后的反弹效果，对模拟真实世界中的物理行为至关重要。
此外，视频还将展示多材质网格的应用，介绍如何在一个复杂的几何模型上分配不同的物理和视觉材质，实现各部分具有独特外观和物理反应的效果。
这不仅增强了视觉的真实感，还允许更精细地控制刚体模拟中的物理特性。

This series covers the basics of the physics extension in Omniverse Kit 104, from the UI to fluid dynamics. 
In Parts 8 to 10, we cover materials to assign physical properties to objects. In these videos, you will learn how to use materials to assign density, friction, restitution, and multi-material meshes.

https://youtu.be/DblE_qDSlPA -->

### Part 9: Materials - Friction Restitution and Defaults

<!-- This series covers the basics of the physics extension in Omniverse Kit 104, from the UI to fluid dynamics.

In Parts 8 to 10, we cover materials to assign physical properties to objects. In these videos, you will learn how to use materials to assign density, friction, restitution, and multi-material meshes.

https://youtu.be/tHOM-OCnBLE -->

### Part 10: Materials - Multimaterial Meshes
<!-- This series covers the basics of the physics extension in Omniverse Kit 104, from the UI to fluid dynamics.

In Parts 8 to 10, we cover materials to assign physical properties to objects. In these videos, you will learn how to use materials to assign density, friction, restitution, and multi-material meshes.

https://youtu.be/W0LvXUBqc3c -->

### Part: 11: Simulation Partitioning - Collision Groups

<!-- Parts 11 to 13 cover different methods to filter out collisions and interactions from your simulated worlds. We cover collision groups, how to use filter pairs, and finally, how to leverage multiple physics scenes.

https://youtu.be/v0VBDpEIXMw -->

### Part 12: Simulation Partitioning - Filter Pairs

<!-- https://youtu.be/tP3dOrJGn4s -->

### Part 13: Simulation Partitioning - Multiple Scenes

<!-- https://youtu.be/BqgPwkN4MkU -->

### Part: 14: Joints - Basics
In Parts 14 to 17, we look at the joint demos. We review prismatic joints, revolute joints, spherical joints used for rigid-body ropes and the D6 joint, and finally, how to break joints.
https://youtu.be/MLqalEq9l6I

### Part 15: Joints - Spherical, Joint Chains and Parenting

https://youtu.be/5bPThkJQj54

### Part 16: Joints - D6 Joint

https://youtu.be/uZwQe_9lsuk

### Part 18: Volume Deformables - Overview

In Parts 18 and 19, we learn about deformables, covering their parameters and how to create deformable attachments.

https://youtu.be/focL232tb8U

### Part 19: Volume Deformables - Attachments

https://youtu.be/QOBeRkiwGos

### Part 20: Particle Deformables - Liquids
This series covers the basics of the physics extension in Omniverse Kit 104, from the UI to fluid dynamics. 

In Parts 20 and 21, we look at two uses of particle-based deformables for simulating fluids and cloth.

https://youtu.be/eMyroevX1nA

### Part 21: Particle Deformables - Cloth

In this series, we cover the basics of the physics extension in Omniverse Kit 104, from the UI to fluid dynamics.

In Parts 20 and 21, we look at two uses of particle-based deformables for simulating fluids and cloth.

https://youtu.be/5xuckfNyzhw

### Part 22: Character Controllers

In this video, we show how to set up a character controller, which is a video-game-style movement controller that permits the player to move around a scene without being able to go through physics objects.

https://youtu.be/DnposMIB7xk

### Part 23: Vehicles
This series covers the basics of the physics extension in Omniverse Kit 104, from the UI to fluid dynamics. This video shows how to set up vehicle simulations that include tire, engine, clutch, transmission, and suspension models.

https://youtu.be/XYdxOjFADhc

### NVIDIA Omniverse: Creating Flow Collisions
<!-- A quick tutorial for one of our discord members asking about how to make an Omniverse Flow simulation collide with stage geometry.
https://youtu.be/7zn0LsJWCNE -->

### Design and Control of a Bipedal Robotic Character

Legged robots have achieved impressive feats in dynamic locomotion in challenging unstructured terrain. However, in entertainment applications, the design and control of these robots face additional challenges in appealing to human audiences. This work aims to unify expressive, artist-directed motions and robust dynamic mobility for legged robots. To this end, we introduce a new bipedal robot, designed with a focus on character-driven mechanical features. We present a reinforcement learning-based control architecture to robustly execute artistic motions conditioned on command signals. During runtime, these command signals are generated by an animation engine which composes and blends between multiple animation sources. Finally, an intuitive operator interface enables real-time show performances with the robot. The complete system results in a believable robotic character, and paves the way for enhanced human-robot engagement in various contexts, in entertainment robotics and beyond.

https://la.disneyresearch.com/publication/design-and-control-of-a-bipedal-robotic-character/

https://youtu.be/7_LW7u-nk6Q

## Backstage with BD-X Star Wars Droids and Disney Imagineering!

Disney Imagineering is sharing their BD-X droids at NVIDIA's GTC 2024. Here's a backstage look at how they get ready to perform and enchant! No edits here, just full joy.

Technically, 2 NVIDIA Jetson Orin NXs control the robot along with custom electronics. 11 brushless motors and 5 servos drive the legs and head. NVIDIA ISAAC Sim uses reinforcement learning to get the droids to walk. Generative AI in action! The magic part is that Disney animators have a special layer of control to bring the character to life.

It's pretty cool when a couple of them are running and appear to 'communicate'. It's a very convincing illusion, even when the puppeteers are standing beside you. All your attention is on the droids, which is why the magic trick works.

Should we build one on the channel?

00:06 Blue Droid start up
02:43 Green Droid start up 
02:57 Blue & Green Interactions

https://youtu.be/QuWaaNN-1hs

### Robot Dogs Competition 2024

<!-- Robot Dogs Competition in ICRA 2024, May 13-17, Yokohama, Japan
Boston Dynamics Spot with Arm vs Unitree B2. (The start image of the video shows a Unitree Go2, sorry, that is a mistake. The Unitree robot in the video is a B2.)

https://youtu.be/qdR1uJjzd2k -->

### Robots that learn from machine dreams

Paper: https://arxiv.org/abs/2411.00083
Authors: Ge Yang (MIT CSAIL), John Leonard (MIT CSAIL), Alan Yu (MIT CSAIL), Ran Choi (MIT CSAIL), Yajvan Ravan (MIT CSAIL), & Phillip Isola (MIT CSAIL). 

https://youtu.be/fgnJQrvTj70

### How do humanoid robots perceive the world?

Members of the Agility team talk about perception and how it enables Digit to work in real-world environments. As well as our approach to perception algorithms and machine learning to help overcome the challenges of encountering new environments.

-------------------------------------------------------------------------

At Agility, we make robots that are made for work. Our robot Digit works alongside us in spaces designed for people. Digit handles the boring and repetitive tasks that are meant for a machine, which allows companies and their people to focus on the work that requires the human element.

https://youtu.be/MhRd0vvdaj0

### Digit + Large Language Model = Embodied Artificial Intelligence

Is there a world where Digit can leverage a large language model (LLM) to expand its capabilities and better adapt to our world?  We had the same question.  Our innovation team developed this interactive demo to show how LLMs could make our robots more versatile and faster to deploy. The demo enables people to talk to Digit in natural language and ask it to do tasks, giving a glimpse at the future.

https://youtu.be/CnkM0AecxYA

### Using an LLM to direct our robot Digit.
In this demonstration, Digit starts out knowing there is trash on the floor and bins are used for recycling/trash. We use a voice command "clean up this mess" to have Digit help us. Digit hears the command and uses an LLM to interpret how best to achieve the stated goal with its existing physical capabilities.

At no point is Digit instructed on how to clean or what a mess is. This is an example of bridging the conversational nature of Chat GPT and other LLMS to generate real-world physical action.

https://youtu.be/Vq_DcZ_xc_E

### LLMs or Reinforcement Learning? Which is better for robot control?

Agility CEO and Co-Founder Damion Shelton talks with Pras Velagapudi, VP of Innovation and Chief Architect, about the best methods for robot control. Comparing Reinforcement Learning to what we can now do using LLMs.

https://youtu.be/SbDxrHpQTy4

### EXPLAINED: Using LLMs for controlling humanoid robots

<!-- Our Chief Technology Officer, Pras Velagapudi, explains what happens when we use natural language voice commands and tools like an LLM to get Digit to do work.

---------------------------------------

At Agility, we make robots that are made for work. Our robot Digit works alongside us in spaces designed for people. Digit handles the tedious and repetitive tasks meant for a machine, allowing companies and their people to focus on the work that requires the human element.

https://youtu.be/r0iVR1FEOxE -->


### Jim Fan from Nvidia AI Building foundation model for robots!

Join Dr. Jim Fan as he delves into the fascinating world of embodied AI! At the San Francisco GenAI Summit on May 29, Jim shared beliefs regarding future trends and covered the latest advancements from Nvidia including MineDojo, Voyager, MetaMorph, and Dr. Eureka. 

MineCLIP uses reinforcement learning from human feedback (RLHF) to train agents in the Minecraft game environment using video and text data.
Voyager is an agent that can explore Minecraft for hours, generating code snippets as skills using - GPT-4 and refining them through trial and error.
MetaMorph is a universal policy model that can control thousands of different robot morphologies by tokenizing their kinematic structures.
Eureka utilizes GPT-4 to automatically generate reward functions for reinforcement learning in physics simulations like IsaacSim, sometimes outperforming human experts.
Domain randomization allows simulation-trained policies to be transferred to real robots by randomizing parameters during training.
Nvidia aims to create a "foundation agent" that can generalize across different embodiments using language instructions, similar to how ChatGPT generalizes across text.
Project Groot focuses on developing a foundation model specifically for humanoid robots, leveraging decreasing hardware costs and simulation at scale.

Chapters:
0:00 MineDojo
2:14 Voyager
4:14 MetaMorph
6:38 Isaac Sim
7:14 Eureka
9:30 Dr. Eureka
11:02 Insights
14:14 F&A

https://youtu.be/LjgCtxcJ710

### Marvelous Designer Workflow: Marvelous Designer Omniverse Connector

With the release of Marvelous Designer 12.1, we are supporting .USD file format.  We have also partnered with Nvidia and released this Omniverse Connector feature.  In this tutorial, you can learn how to use Marvelous Designer Omniverse Connector, and find out how Universal Scene Description can improve your workflow.

https://youtu.be/r7AfPVd8WG4

### Toyota's Basketball Robot Makes Longest Ever Shot - Guinness World Records
A team at Toyota in Japan have upgraded CUE to be able to shoot basketballs from the other side of the court, after originally setting a free throw record in 2019.

This robot just sunk a seemingly impossible basketball shot – and made it look easy.

CUE6, a humanoid robot created by Toyota Motor Corporation (Japan), has earned its second Guinness World Records title with a mind-blowing shot from over 80 ft away.

In Nagakute, Aichi, Japan on 26 September, CUE6 performed the farthest basketball shot by a humanoid robot from a distance of 24.55 m (80 ft 6 in).

An NBA court is 29 m (94 ft) long.

The CUE project, led by Toyota's R&D (Frontier research team), began in 2017 with a few volunteers within the company.

It all started as a bit of a challenge, to see if artificial intelligence could achieve human-like precision and adaptability in physical tasks.

CUE went from being components made of LEGO to the incredible human likeness he is today.

But there’s much more beneath the surface than you may realize.

CUE isn’t just a catapult that can throw a basketball into a hoop – he can learn from his mistakes and adapt his aim for a better shot.

He’s trained to recognize patterns and even to correct his posture, arm position and shot strength for variables in real time, just like a human would.

The 3rd generation of the robot broke its first record in 2019, performing the most consecutive basketball free throws by a humanoid robot (assisted) – an astonishing 2,020 shots in a row.

And it only stopped because the engineers present decided to end the attempt.

Rumour has it, CUE3 would still be hitting free throws now if nobody had stepped in.

Before long, he could receive the ball and set himself up for a shot, and find the net from the three-point line and even further away.

He continued to improve with every iteration, with the team at Toyota working to allow CUE to independently pick up the ball and shoot, move on wheels and collect the ball from different shooting positions and even dribble the ball.

A lot of work was done to his hands, and foot cameras were added to make it easier for him to sense the movement of the ball.

Tomohiro Nomi, CUE project leader at Toyota, said: “With the goal of attempting a record, we set out to shoot from a significantly farther distance than before.

“We aimed to surprise the world by shooting from far away. And so, we took on this challenge to shoot from this distance.”

He added: “Using Artificial Intelligence based on robot structure, it learned and thought of the throwing style that would be the most effective.

“As a result, I think it led to the current throwing form.”

The longest basketball shot ever was made from 34.6 metres (113 ft 6 in) away by Joshua Walker (USA) in 2022.

But of course, it took him a lot of goes and a lot of practice.

There were high hope that CUE6 would land the shot on his very first attempt.

Sadly, the ball hit the rim and bounced back towards him.

But it went in on his second shot.

Nomi added: “I want to create something interesting and show the power of craftsmanship, and let the world know that Japan can still do a lot.

“I hope we can share that message.”

He shared that his dream is to ultimately create a robot that can “dunk like Michael Jordan”.

Watch this space.

https://www.guinnessworldrecords.com/news/2024/12/humanoid-robot-could-be-the-future-of-basketball-after-sinking-shot-from-record-80-ft

https://youtu.be/aBder-Ukymk

### Isaac Lab Study group 系列

#### Isaac Lab Study group - 11/29/24

https://youtu.be/OPuXzMnnAvA

#### Isaac Lab Study group 2

https://youtu.be/OYyMoEuY4Bk

#### Study Group 3 12/11/24

In this study group we go over the basics of cloning an Isaac Lab extension template, creating a Isaac Sim extension and going over the structure of the manager based workflow in Isaac Lab.

https://youtu.be/Hg-2h3S_yvs

####  Isaac Lab Tutorial 1: Creating an Empty Scene (NVIDIA Omniverse Isaac Sim)

In this Isaac Lab Tutorial I'm showing how to setup and control the simulation by creating an empty scene.

Here are the links:
Isaac Lab Documentation:
https://isaac-sim.github.io/IsaacLab/main/index.html

https://youtu.be/sL1wCfp9tRU

#### Isaac Lab Beginner Tutorial 2: Adding Prims to the Scene (NVIDIA Omniverse Isaac Sim)
In this Isaac Lab Tutorial we learn how to add prims (objects) to the scene. We will cover various forms of prims.

Here are some useful links:

Isaac Lab Tutorial 2:
https://isaac-sim.github.io/IsaacLab/main/source/tutorials/00_sim/spawn_prims.html

https://youtu.be/ksF8mDOqwy4

#### Isaac Lab Tutorial 3: Interacting with Rigid Objects

In this Isaac Lab Tutorial we learn how to interact with Rigid Objects. Isaac Lab, previously Isaac Gym, is built on top of NVIDIA Omniverse Isaac Sim.
https://isaac-sim.github.io/IsaacLab/main/source/tutorials/01_assets/run_rigid_object.html

https://youtu.be/7UVfj_mYVuM

#### Isaac Lab Tutorial 4: Interacting with Deformable Objects

In this Isaac Lab Tutorial we learn how to interact with Deformable Objects. Isaac Lab, previously Isaac Gym, is built on top of NVIDIA Omniverse Isaac Sim.
https://isaac-sim.github.io/IsaacLab/main/source/tutorials/01_assets/run_deformable_object.html

https://youtu.be/Qp-4DqUrjmY

#### Training & Registering Robots - Isaac Lab Tutorial 4 (Reinforcement Learning)

In this Isaac Lab Tutorial I'm showing how to train an agent and register an environment. 
Isaac Lab, previously Isaac Gym, is built on top of NVIDIA Omniverse Isaac Sim.

https://youtu.be/BSQEYj3Wm0Q

#### *NEW* Free Tutorials & Guides for Isaac Sim & Isaac Lab! - LycheeAI Hub

In this video I'm announcing our Isaac Sim & Isaac Lab learning platform LycheeAI Hub:
https://lycheeai.notion.site

https://youtu.be/B1EfVKRsm90

#### [Isaac Lab] Leatherback Project: Setup & Training - Part 1

This is part 1 of the Leatherback project where we setup the environment to train the car with Reinforcement Learning.

https://lycheeai.notion.site/Leatherback-Community-Project-1b828763942b818c903be4648e53f23d

https://youtu.be/bzHtZseHb34

#### Interacting with Articulations - Isaac Lab Tutorial 5 (Basics)
In this Isaac Lab Tutorial we learn how to interact with Articulations. Isaac Lab, previously Isaac Gym, is built on top of NVIDIA Omniverse Isaac Sim.

https://isaac-sim.github.io/IsaacLab/main/source/tutorials/01_assets/run_articulation.html

#### Getting Started with Isaac Lab
Let's get started with Isaac Lab, an open-source, unified framework for reinforcement learning, motion planning, and other robot training tasks.

Here are the official docs to follow along with: 
https://isaac-sim.github.io/IsaacLab/main/index.html

And here's the Isaac Lab GitHub.
https://github.com/isaac-sim/IsaacLab

#### How Isaac Sim relates to Isaac Lab

So, there's NVIDIA Isaac Lab, Issac Sim, Isaac Gym, Issac Limbs...
How do they relate?

The main idea: build and rig your robot in Isaac Sim, then train it with Isaac Lab.

https://youtu.be/NFcRirGuERI

#### Robot Programming With NOVA & NVIDIA Isaac Sim

Join us for an exclusive livestream as we explore how Wandelbots' NOVA integration with NVIDIA's Isaac Sim advanced simulation capability is reshaping robot motion planning and execution. Witness how these cutting-edge technologies bridge the gap between virtual environments and real-world applications, making robotics development more accessible, seamless, and efficient than ever before.

https://youtu.be/dCwNkjkzJcY

#### Physical AI at NVIDIA GTC Recap
GTC 2025 delivered groundbreaking advancements in Physical AI, Omniverse, and robotics—now it's time to break it all down! Join us as we recap the biggest announcements, key takeaways, and unexpected surprises from the conference. From next-gen humanoid robotics to the latest in OpenUSD and digital twins, we’ll explore what these innovations mean for developers and the future of AI-powered simulation. Whether you attended GTC or are catching up, this is your chance to dive into the highlights and discuss what’s next. Don’t miss it!

https://www.youtube.com/watch?v=qiykCYJO5FE

#### We built a GPT-4o-powered cleaning robot.

<!-- - $250 for the robot arms
- 4 days to build

https://x.com/JannikGrothusen/status/1852790503823057073 -->

#### Ansys Innovates Industrial Simulation With OpenUSDAnsys Innovates Industrial Simulation With OpenUSD

<!-- NVIDIA and Ansys are expanding our long-standing partnership to include new opportunities in AI and accelerate a new era of digital engineering.

Learn More About OpenUSD: https://www.nvidia.com/en-us/omniverse/usd
Learn More About Our Partnership: https://www.nvidia.com/en-us/industries/industrial-sector/ansys

https://youtu.be/Ho3mqEO9U1w -->

#### Borg 机器人——重振美国工业的智能力量

<!-- 在 Borg，我们的首要使命是借助先进的自主机器人技术，重新定义并强化美国的工业和制造业实力。作为这一愿景的核心，我们自豪地宣布，所有 Borg 机器人均在美国汽车工业的心脏——底特律精心打造，并计划于2025年初投入运行。

为了实现自动化产业变革，Borg 正式推出其2025年工业系列的最后一款力作——单臂重型移动机器人。这款机器人专为应对严苛的工业环境而设计，它不仅代表了我们在人工智能驱动机器人技术上的最新成就，也标志着我们在推动制造业智能化进程中的重要一步。

Borg 的单臂重型移动机器人结合了尖端的人工智能算法与强大的机械工程，能够执行复杂任务，如物料搬运、装配线支持以及仓库管理等。凭借其卓越的负载能力和灵活的操作性能，该机器人可以显著提高生产效率，降低人力成本，并确保工作场所的安全性和可靠性。

随着 Borg 机器人逐步部署到全国各地的工厂和企业中，我们将继续致力于开发创新解决方案，以满足不断变化的市场需求。通过引入这些先进的自动化工具，我们期待与合作伙伴共同开启一个更加高效、智能且可持续发展的未来。

此简介突出了 Borg 机器人的关键特点及其对美国工业复兴的重要性，同时也强调了产品的本地制造属性，这有助于增强品牌形象和市场信任度。标题简洁有力，直接传达了 Borg 机器人在助力美国工业复苏方面的作用。 
了解更多：https://www.borgrobotic.com/

-->

#### Introducing NVIDIA Jetson Orin™ Nano Super: The World’s Most Affordable Generative AI Computer

<!-- The NVIDIA Jetson Orin™ Nano Super Developer Kit’s performance, compact size, and low cost are redefining generative AI for small edge devices. 

At just $249, it provides developers, students, and builders with the most affordable and accessible platform, backed by the support of NVIDIA AI software and a broad AI software ecosystem.

Jensen just unveiled the new NVIDIA Jetson Orin Nano Super, optimized for on-device AI processing in robots, 

boasting 70 trillion operations (INT8) per second, 102GB/s memory bandwidth, 25 watts power consumption, and a price of $249.

Jetson Orin Nano Super：掌中之力，重塑边缘设备的生成式AI未来

NVIDIA 刚刚（12月17号）推出的 Jetson Orin Nano Super 开发套件，以249美元的亲民价格重新定义了边缘设备上的生成式AI处理能力。

这款手掌大小的超级计算机不仅提供了高达 70 TOPS（INT8）的运算能力和 102 GB/s 的内存带宽，还实现了比前代产品高 1.7 倍的生成式 AI 推理性能，功耗仅为 25 瓦。

凭借其强大的 NVIDIA AI 软件支持和广泛的生态系统，Jetson Orin Nano Super 成为开发者、学生及爱好者的理想选择，助力他们将创意变为现实，推动从机器人到视觉 AI 应用的创新边界。

Jetson Orin Nano Super 以 $249 的亲民价格提供高达 70 TOPS (INT8) 的运算能力和 102 GB/s 的内存带宽，功耗仅 25 瓦，结合强大的 NVIDIA AI 软件支持和广泛的生态系统，成为手掌大小的高性能生成式 AI 边缘计算理想选择。
https://blogs.nvidia.com/blog/jetson-generative-ai-supercomputer/
https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/nano-super-developer-kit/ -->

#### ChatUSD - Generative AI Copilot for OpenUSD Development

<!-- For USD developers, building, profiling, and optimizing large 3D scenes can be a very complex process.

ChatUSD is a large language model (LLM) agent for generating Python-USD code scripts from text and answering USD knowledge questions. ChatUSD is fine-tuned with NVIDIA’s USD functions and Python-USD code snippets using NVIDIA AI Workbench and the NeMo Framework. This generative AI copilot is easily accessed as an Omniverse Cloud API.

https://youtu.be/GowdBDeNo2Q

https://build.nvidia.com/nvidia/usdcode -->

#### Transform Your Business With Agentic AI

Agentic AI is transforming every enterprise, using sophisticated reasoning and iterative planning to solve complex, multi-step problems. Learn how NVIDIA AI Blueprints help turn data into knowledge and knowledge into action by automating processes, tapping into real-time insights, and improving workflow efficiency at scale.

https://youtu.be/afWBnxWQDKk

#### OpenUSD for Marketing and Advertising feat. WPP

See how developers are building solutions with OpenUSD that power content production for marketing campaigns such as car commercials and customizable product configurators. Featuring the creative transformation company WPP, we'll show how generative AI can be integrated with OpenUSD workflows in NVIDIA Omniverse, accelerating content creation and delivering seamless, scalable experiences for some of the world's most prestigious brands. 

https://youtu.be/fia6X2lC3-k

#### Develop Next-Generation 3D Product Configurators With OpenUSD and Omniverse APIs

Product configurators with pre-rendered images often limit personalization and dynamic environment representation. 
This makes it difficult to tailor product experiences based on regional or individual preferences. 

Developers can easily power next-generation 3D product configurators to enhance customers’ shopping experiences, utilizing Universal Scene Description (OpenUSD) and NVIDIA Omniverse Cloud APIs.

These configurators will make it possible to explore every color swatch, interior stitch, and viewing environment imaginable.

<!-- 用OpenUSD和Omniverse打造个性化3D产品展示，提升客户体验，打造未来的3D产品配置工具
使用 Universal Scene Description (OpenUSD) 和 NVIDIA Omniverse Cloud APIs 来开发下一代3D产品配置器，从而彻底改变客户的购物体验。传统的产品配置器依赖于预渲染图像，这限制了个性化和动态环境的表现，难以根据地区或个人偏好定制产品体验。

通过集成 OpenUSD 和 Omniverse Cloud APIs，开发者可以创建高度互动且实时更新的3D产品配置器，使客户能够探索每一个颜色选项、内部缝线细节以及各种观看环境。这种全新的配置器不仅提供了前所未有的个性化水平，还支持动态调整产品的外观和质感，以适应不同用户的特定需求。
通过这种方式，开发者不仅可以提高客户的参与度和满意度，还能促进更高效的销售流程。

https://youtu.be/dyciB0h-vmA -->

3D产品配置器（3D Product Configurator） 是一种交互式工具或应用程序，允许用户在虚拟环境中自定义和查看产品的三维模型。
它使得消费者能够在购买之前，通过调整各种属性来设计理想中的产品外观、功能和其他特性。
具体来说：
1、实时预览：用户可以即时看到他们所做的更改对产品外观的影响。
2、高度定制化：支持选择不同的颜色、材料、纹理、尺寸等选项，甚至包括内部结构细节。
3、环境模拟：可以在不同的光照条件和背景场景中展示产品，帮助用户更好地想象产品在家中的实际效果。
4、互动性：提供旋转、缩放和平移等功能，让用户从各个角度检查产品。
应用场景
1、家具行业：客户可以选择沙发的颜色、布料类型，并将其放置在自己的客厅环境中进行预览。
2、汽车行业：购车者可以定制汽车的外观颜色、轮毂样式、内饰材质等，并查看最终效果。
3、电子产品：消费者可以挑选手机壳的颜色、材质，并查看与所选配件搭配后的整体外观。
4、时尚配饰：用户可以根据个人喜好选择手表表带的材质、颜色及表盘设计。
技术实现
利用 Universal Scene Description (OpenUSD) 和 NVIDIA Omniverse Cloud APIs 等先进技术，开发者可以创建出高度逼真且易于使用的3D产品配置器。
这些工具不仅提高了用户体验的质量，还促进了更高效的销售流程，因为它们让潜在买家更容易做出购买决定。

#### Develop 3D Product Configurators With Generative AI and OpenUSD

无需翻译

Developers can easily power next-generation 3D product configurators to enhance customers’ shopping experiences, utilizing Universal Scene Description (#OpenUSD) and ‪@NVIDIAOmniverse‬ Cloud APIs. 

https://youtu.be/LAVXuDK83iA

#### Digitalization: A Game Changer for the Auto Industry

NVIDIA partners describe how digitalization is redefining productivity, cost-efficiency, and innovation in the automotive industry.

了解更多omniverse数字孪生技术：https://www.nvidia.com/en-us/omniverse/solutions/digital-twins

https://youtu.be/2J-o0tO7-98

#### Advancing physical AI: NVIDIA Isaac Lab and AWS for next-gen robotics

The future of robotics lies in physical AI—robots capable of autonomously performing intricate tasks in the real world. To meet this grand challenge, accelerating the development of AI-powered robots is essential. This requires a simulation-first approach to safely and efficiently train these advanced systems. In this developer-centric session, learn how to fast-track your robotics projects using NVIDIA Isaac Lab, an open source, modular framework designed for reinforcement learning. By integrating Isaac Lab with AWS Batch on GPU-accelerated Amazon EC2 instances, you can use scalable cloud resources to rapidly train robots against a comprehensive set of policies in a simulated environment. This presentation is brought to you by NVIDIA, an AWS Partner.

https://youtu.be/LafWpmrqahY

PPT:
https://reinvent.awsevents.com/content/dam/reinvent/2024/slides/aim/AIM113-S_Advancing-physical-AI-NVIDIA-Isaac-Lab-and-AWS-for-next-gen-robotics-sponsored-by-NVIDIA.pdf

#### Making Virtual People Follow a Target in NVIDIA Omniverse

With the new omniverse people extension, animated virtual people is easier than ever.  

In this video we walk through some simple steps to setup a scene with a navmesh and agents that can listen to commands. Unlike the initial tutorial released by nvidia, we will use a target walking extension to automatically attach a goal to each character and making them continuously follow it. 

You can move the goal in real-time or set it up on a path for repeatable animation. 

The code to the extension is available on github. 

https://youtu.be/OmXWyWGMaSY

#### Genesis project

<!-- 3D generation has suddenly exploded! The Genesis project takes it to the next level with 4D dynamic worlds:
- Complex character animations
- Robotic system behavior simulations
- Rich scene environments
- VLM agent integration
- Realistic facial animations & emotional transitions

Genesis是一款利用GPU加速并行计算的高度优化物理引擎，以前所未有的速度在各种场景中模拟，包括在特定场景下实现430,000倍于实时的模拟速度，并即将推出自动休眠功能以加速静态实体的模拟。

项目主页：https://genesis-embodied-ai.github.io/
文档：https://genesis-world.readthedocs.io/en/latest/index.html
开源地址：https://github.com/Genesis-Embodied-AI/Genesis -->

#### Nvidia cosmos

Cosmos platform:
https://www.nvidia.com/en-us/ai/cosmos/
More: https://github.com/NVIDIA/Cosmos

In this exclusive interview, we get the scoop on NVIDIA's dive into robotics and AI technology. Watch for more details on their physical AI platform, Cosmos, working in sync with their simulation technology platform, Omniverse, to facilitate humanoid robot interaction in a factory space.

#### Human-Object Interaction from Human-Level Instructions

Intelligent agents must autonomously interact with the environments to perform daily tasks based on human-level instructions. They need a foundational understanding of the world to accurately interpret these instructions, along with precise low-level movement and interaction skills to execute the derived actions. In this work, we propose the first complete system for synthesizing physically plausible, long-horizon human-object interactions for object manipulation in contextual environments, driven by human-level instructions. We leverage large language models (LLMs) to interpret the input instructions into detailed execution plans. Unlike prior work, our system is capable of generating detailed finger-object interactions, in seamless coordination with full-body movements. We also train a policy to track generated motions in physics simulation via reinforcement learning (RL) to ensure physical plausibility of the motion. Our experiments demonstrate the effectiveness of our system in synthesizing realistic interactions with diverse objects in complex environments, highlighting its potential for real-world applications.

Introducing Human-Object Interaction from Human-Level Instructions! First complete system that generates physically plausible, long-horizon human-object interactions with finger motions in contextual environments, driven by human-level instructions.
🔍 Our approach:
- LLMs transform instructions into execution plans & target layouts
- Multi-stage framework generates synchronized object, body & finger motions
- RL policy tracks generated motions in physics simulation
Project: https://hoifhli.github.io

#### CARLA 0.10.0 发布

CARLA has been developed from the ground up to support development, training, and validation of autonomous driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites, environmental conditions, full control of all static and dynamic actors, maps generation and much more.

CARLA 0.10.0 Release with Unreal Engine 5.5!
Unreal Engine 5.5 migration, brand new assets, upgraded Town 10, remodeled vehicles, open-cast mine map

The CARLA team is excited to announce the release of CARLA version 0.10.0. The biggest news is that this version delivers a monumental leap forward in visual fidelity through a migration from Unreal Engine 4.26 to Unreal Engine 5.5. CARLA 0.10.0 introduces the incredible Lumen and Nanite technologies into CARLA’s core rendering engine, enhancing realism and detail. This engine upgrade brings vastly improved visual fidelity for camera sensors through UE 5.5’s enhanced rendering technology.

To exploit the new capabilities delivered by the engine upgrade, 0.10.0 comes with upgraded environments and assets with an upgraded Town 10 and beautifully remodeled vehicles along with a spectacular off-road open-cast mine map generously provided by CARLA’s sponsor and consortium member Synkrotron.

As if that wasn’t enough, version 0.10.0 also brings state-of-the-art generative AI traffic simulation through a new integration with Inverted AI, native ROS integration, improved Python compatibility and more!

Improved camera fidelity
UE 5.5 significantly enhances CARLA’s capability to render highly detailed geometry with stunningly realistic lighting and reflections. The new Nanite virtualized geometry system dynamically scales the number of polygons used to render assets, ensuring efficient performance when viewing objects from a distance while retaining incredible detail up close. This means CARLA maps and assets now have an almost unlimited geometry budget, permitting the use of highly detailed 3D assets without sacrificing performance. Lumen brings a new level of realism to simulating scene lighting and reflections by more accurately modeling complex light behavior such as diffuse interreflections and specular highlights. This results more natural looking scenes that dynamically adapt to lighting changes in a realistic way. Altogether, UE 5.5 enhances CARLA’s visual fidelity with amazing results.

Stunning new environments and assets
Upgraded Town 10
The enhanced geometry handling system underpinning CARLA opens the door to rendering environments with much higher polygon counts and more realistic lighting. To fully exploit this, this version of CARLA presents a upgraded Town 10 map with brand new buildings, pavements, roads and vegetation all organized around the same familiar road network.

Highlighted features
Scalability via a server multi-client architecture: multiple clients in the same or in different nodes can control different actors.
Flexible API: CARLA exposes a powerful API that allows users to control all aspects related to the simulation, including traffic generation, pedestrian behaviors, weathers, sensors, and much more.
Autonomous Driving sensor suite: users can configure diverse sensor suites including LIDARs, multiple cameras, depth sensors and GPS among others.
Fast simulation for planning and control: this mode disables rendering to offer a fast execution of traffic simulation and road behaviors for which graphics are not required.
Maps generation: users can easily create their own maps following the ASAM OpenDRIVE standard via tools like RoadRunner.
Traffic scenarios simulation: our engine ScenarioRunner allows users to define and execute different traffic situations based on modular behaviors.
ROS integration: CARLA is provided with integration with ROS via our ROS-bridge
Autonomous Driving baselines: we provide Autonomous Driving baselines as runnable agents in CARLA, including an AutoWare agent and a Conditional Imitation Learning agent.
Recommended reading
CARLA's functionality is covered extensively in the documentation. Here are some highlights covering some of CARLA's most useful and requested features.

Core features
Quickstart: Getting started with CARLA is easy, this guide will show you how to install and run the simulator.
Actors: CARLA's actors are entities that interact within the simulation like vehicles, pedestrians and traffic signals. Get to know them here.
Sensors: CARLA boasts an impressive array of models of real world sensors like cameras, LIDAR and RADAR. The simulator also gives access to privileged information such as ground truth semantic segmentation and depth information.
Traffic Manager: CARLA's Traffic Manager controls NPCs to challenge your autonomous driving agent.
ROS bridge: CARLA's ROS bridge enables seamless connection with the Robot Operating System.

https://carla.org/2024/12/19/release-0.10.0/

https://carla.org/

#### Client Side Tool Calling with the OpenAI WebRTC Realtime API

OpenAI recently released the WebRTC interface for their realtime API. It supports tool calling, which means you can use it to do client side JavaScript function calling. This opens up a whole new set of use cases.

In this video, Craig shows off how he used Cloudflare Worker to relay the WebRTC call securely. Then he uses  function calling to control the style of a web page. And even a Bluetooth enabled robot hand 🤖🖐️!

Create an account on Cloudflare today for free: https://dash.cloudflare.com/sign-up

https://youtu.be/TcOytsfva0o

#### OpenUSD Digital Twins for Industrial Operations
Join NVIDIA and Microsoft Azure experts to learn how applications developed on NVIDIA Omniverse and OpenUSD integrate with Microsoft Azure IoT Operations, Fabric, and Power BI to transform industrial operations. This livestream showcases live demos and guidance on how developers can get started building their own operations digital twins, by connecting real-world operations data to physically accurate 3D models of industrial systems and production environments at scale.
https://youtu.be/CbZ9W7bm-pE

#### OpenUSD and Physical AI Highlights from CES 2025
Tune in to explore NVIDIA’s latest announcements at CES 2025, where physical AI and robotics took center stage. This livestream will showcase cutting-edge innovations in OpenUSD microservices and Omniverse blueprints driving the future of 3D workflows in autonomous vehicles and robotics development. Discover how these technologies are transforming industries and empowering developers worldwide.
https://youtu.be/Kte9EQ05BPk

## openai realtime embedded sdk

#### Openai Realtime API - Having A Realtime Conversation With Ai

Today we take a look at OpenAi's realtime api.
Diving into the demo app and having a real time coversation. 

https://youtu.be/l5R9sxdV_9g

#### Stanford Seminar - Where are the Field Robots?

Rapid advances in generative AI, reinforcement learning, and supervised learning have indeed transformed robotics across many industries. One domain that remains within reach, but still elusive, is when robots need to operate autonomously in harsh, dynamic, and unpredictable field environments over long durations. Cracking this domain is critical for solving some of the most pressing problems in sustainability, agriculture, and climate resilience. For instance, teams of autonomous robots hold the potential to address key challenges in modernizing agriculture. Yet, fully autonomous robots that operate without supervision for weeks, months, or for a whole growing season are not yet practical. In this talk I will outline a vision for the future of field robotics, highlighting key breakthroughs designed that advance robot AI without sacrificing reliability and practicality. I will explore advances in visual navigation, self-supervised learning, and robot onboard AI for increasing levels of autonomy. I will also discuss innovations in soft robotics for manipulation in cluttered environments. I will also share our progress in commercializing these technologies to benefit agriculture and solar energy industries through entrepreneurial activities. Including creating innovative products in high-throughput phenotyping, disease and pest monitoring, under-canopy cover crop planting for soil regeneration, automated spraying in orchard and tree crops for small and large farmers, and automating operations and maintenance in large solar farms. 

About the speaker: http://daslab.illinois.edu/

https://youtu.be/PWw7wDOsc4c

#### Stanford Seminar - Modeling Humans for Humanoid Robots

Having a humanoid robot operating like a human has been a long-standing goal in robotics. The humanoid robot provides a general-purpose platform to conduct diverse tasks we do in our daily lives. In this talk, I will present a 2-level learning framework designed to equip humanoid robots with robust mobility and manipulation skills, enabling them to generalize across diverse tasks, objects, and environments. The first level focuses on training Vision-Language-Action (VLA) models with human video data for both navigation and manipulation. These models can predict “mid-level” actions which predict precise movements or trajectories for the human body and hands, conditioned on language instructions. The second level involves developing low-level robot manipulation skills through teleoperation, and low-level humanoid whole-body control skills via motion imitation and Sim2Real. By combining human VLA with low-level robot skills, this framework offers a scalable pathway toward realizing general-purpose humanoid robots.

About the speaker: https://xiaolonw.github.io/

https://youtu.be/uQ-5BryUNv8

#### Debugging Made Easy: Essential Tips, Tricks, and More for VS Code | Isaac Sim Office Hours

This session will explore Visual Studio Code’s powerful debugging tools, with a special focus on troubleshooting workflows in NVIDIA Isaac Sim. We’ll share essential tips, best practices, and time-saving tricks to help you debug smarter and optimize your development process. Additionally, we’ll discuss the impact of the deprecation of Omniverse Launcher on Isaac Sim users and provide guidance on transitioning smoothly. Bring your questions—our experts are here to help you navigate these changes and level up your debugging skills!

https://youtu.be/HWdvoIC5mg4

#### Getting Started With NVIDIA Cosmos for Windows (Beginner Tutorial)

https://youtu.be/N9qvHjB1RrU

In this video, I walk you through how to install NVIDIA Cosmos on a Windows or Linux based machine. I also walk you through how to make NVIDIA Cosmos's diffusion based videos using text to video, image to video, and video to video methods. You do not need any prior programming or command prompt experience. See below for links to the modified repository I reference in the video as well as the original project and reference.

Link to my GitHub Repo for Window installation and usage: https://github.com/jonstephens85/Cosmos-WSL2
Link to NVIDIA Cosmos experience page to get 20 free prompts: https://build.nvidia.com/nvidia/cosmos-1_0-diffusion-7b
Link to Original NVIDIA Cosmos Github repository: https://github.com/NVIDIA/Cosmos

Show Timeline:
00:00 Intro & What is NVIDIA Cosmos
04:47 Installing and configuring WSL2 for Cosmos
12:57 Installing Docker for Ubuntu on WSL
15:50 Installing NVIDIA Container Toolkit
18:10 Clone and Install NVIDIA Cosmos
22:00 Downloading Model Checkpoints
29:29 Running Text to Video (Text2World)
42:57 Running Image and Video to Video (Video2World)

If you found this tutorial helpful, please follow my channel tutorials and advanced tips on 3DGS, Generative AI, and Computer Vision!

#### Using NVIDIA Cosmos World Foundation Models for Physical AI Development
Join us for an exciting livestream on how the OpenUSD ecosystem is helping advance Physical AI. Discover how OpenUSD enables high-fidelity 3D environments in NVIDIA Omniverse™ and integrates seamlessly with NVIDIA Cosmos. Learn how Cosmos world foundation models use OpenUSD data to create photorealistic, physics-aware simulations, accelerating robotics and autonomous system development. Don’t miss this deep dive into the future of AI-powered virtual worlds.

Please join us for our weekly livestream, which features members of the Omniverse team, partners, and special guests from the community. We will discuss the latest OpenUSD and Omniverse announcements, features, and workflows. Don't miss this opportunity to ask your questions live and get ahead of the curve with OpenUSD and NVIDIA!

https://youtu.be/wjCVFfmsai0


[Resources] Cosmos: https://www.nvidia.com/en-us/ai/cosmos/
[Resources] Cosmos Getting Started: https://developer.nvidia.com/cosmos?ncid=no-ncid
[Resources] Cosmos Documentation: https://research.nvidia.com/publication/2025-01_cosmos-world-foundation-model-platform-physical-ai

#### Build Custom World Foundation Models with NVIDIA Cosmos
NVIDIA Cosmos is a world foundation model platform that accelerates the development of physical AI applications like humanoid robots and autonomous vehicles.

Mehran Maghoumi, a developer advocate at NVIDIA, shows you how to get started with Cosmos world foundation models in the NVIDIA NeMo framework. He overviews model setup, inference, raw video data curation and post-training to generate customized and high-quality training data.
00:00 - Introduction
00:35 - Cosmos World Foundation Models Overview
01:35 - Getting Started
03:10 - How to Customize a Model on Your Own Data
04:39 - Curating Videos
05:30 - Tokenization
06:33 - Post-Training and Video Generation

https://youtu.be/dqP-I59wUwU

#### Hyperparameter Tuning For Robotics in NVIDIA Isaac Lab Walkthrough
The views and opinions expressed in this video are my own personal thoughts and do not reflect or represent those of my employer or any affiliated organizations or individuals. The knowledge shared in this video is for educational purposes only. Please use this information constructively and responsibly. While I aim to be helpful, you assume all responsibility for how you apply this knowledge. Use at your own discretion.

In this vid, I show how to do hyperparameter tuning for robotics in NVIDIA Isaac Lab with the newly added Ray Integration
https://github.com/isaac-sim/IsaacLab/pull/1301
If you try it and it doesn't work for you feel free to @garylvov on github

https://youtu.be/z7MDgSga2Ho

#### *NEW* Isaac Sim 4.5 & Isaac Lab 2.0 | Installation & Overview | NVIDIA Omniverse

<!-- https://youtu.be/CLFjtuH2NAQ

官方文档:
https://github.com/isaac-sim/IsaacLab/releases/
https://docs.isaacsim.omniverse.nvidia.com/latest/overview/release_notes.html

推荐中文文档：
https://docs.robotsfan.com/isaaclab
https://docs.robotsfan.com/isaacsim -->

#### Debugging Tips & Tricks: How to Leverage Visual Markers & Catch NAN Bugs | Isaac Lab Office Hours

We’ll also dive into essential debugging techniques to enhance your simulations. Learn how to use visual markers for real-time debug visualization in your environments and runtime, and discover effective strategies for tracking down challenging issues like NaN values in rewards and observations. Whether you're preparing for GTC or looking to sharpen your debugging skills, this session is packed with valuable insights! 

https://youtu.be/YbR6fFwbz6E

#### Isaac Lab Study Group | Leatherback, Tensorboard Setup, Debug | 02-19-2025

00:00 Marker visual on Leatherback by Eric
11:14 Tensorboard Setup by Lychee AI
30:15 Quaternions
74:14 Debug Gym Bridges

https://youtu.be/dhHUwf0Wzdw

#### Isaac Lab Study Group | Leatherback working!, thoughts about Extensions, some debug |Feb 26, 2025

00:00 Leatherback is chasing the cones!
11:04 Some future directions
35:15 Opportunities to help with Isaac Lab projects
40:20 Bug in custom sim? Scene vanishes when RL is incorporated
47:15 Debug, tried to convert leatherback project into Extensions
63:31 Back to looking at the leatherback, spawning rigid body, chitchat

https://youtu.be/c30HPqlwcBQ

#### Isaac Lab 4-Bar Simulation

Solving Closed Articulation Simulation.
Now We Can Reinforcement even robot has closed Articulation.

韩语

https://youtu.be/_SHI-P6B02U

#### Direct Workflow | Reinforcement Learning in Isaac Lab (Tutorial 3)

In this Isaac Lab Tutorial I'm showing the Direct Workflow, which compared to the Manager-Workflow allows for more control when creating Reinforcement Learning Environments. Isaac Lab, previously Isaac Gym, is built on top of NVIDIA Omniverse Isaac Sim.

https://youtu.be/gdIJ_FcYXvM

#### How to Import and Control Lerobot SO100 Arm in Isaac Sim

This video provides instructions on how to import the Lerobot SO100 Arm arm into NVIDIA's Isaac Sim simulation platform and control it using Python scripts.

https://youtu.be/buiqdmNQKwY


可以先查看官方资料： https://github.com/huggingface/lerobot/blob/main/examples/10_use_so100.md  
每一步骤都很详细 几乎不需要自己去二次开发代码   到时候我们也会优化里面一些繁琐的步骤出一个新的wiki 并且现场也有我们的技术人员进行辅助

其他资料请参考：

✅wiki - 机械臂组装、校准、同步示教: https://wiki.seeedstudio.com/cn/lerobot_so100m/
✅wiki - 下载LeRobot环境，将机械臂导入NVIDIA Issac Sim仿真环境：https://wiki.seeedstudio.com/lerobot_so100m_isaacsim/

✅Diffusion Policy模型 - 论文（https://diffusion-policy.cs.columbia.edu/）+ Q&A（https://www.bilibili.com/video/BV1ZaeAe7EMu/?spm_id_from=333.788.top_right_bar_window_custom_collection.content.click）
✅ACT模型 - 论文（https://tonyzhaozh.github.io/aloha/）+ Q&A（https://www.bilibili.com/video/BV1NAebePETu?spm_id_from=333.788.recommend_more_video.-1&vd_source=16f6410bb6a7c9939bfb6a93fc79b27e）

✅具身智能黑客松第一期精彩回放：https://www.bilibili.com/video/BV1GufnYeEFQ/?vd_source=450f92248a542b4caabde8a9eb383436

✅探索更多智能硬件的结合应用：
https://www.seeedstudio.com/reComputer-J4012-p-5586.html
https://www.seeedstudio.com/reCamera-2002w-64GB-p-6249.html
https://www.seeedstudio.com/ReSpeaker-Lite-Voice-Assistant-Kit-Full-Kit-of-2-Mic-Array-pre-soldered-XIAO-ESP32S3-Mono-Enclosed-Speaker-and-Enclosure.html
https://www.orbbec.com.cn/index/Product/info.html?cate=38&id=16

#### Accelerate Vision AI Model Development with NVIDIA Metropolis
NVIDIA Metropolis accelerates the development of vision AI models with curated model selection, higher accuracy, and faster inference. 

This set of developer tools makes sense of data created by trillions of sensors for some of the world's most valuable physical transactions. Learn how developers use Metropolis to bring visual data and AI together, improving efficiency, safety, and customer experiences across a variety of industries. 

Learn more about NVIDIA Metropolis: https://www.nvidia.com/en-us/autonomous-machines/intelligent-video-analytics-platform/

https://youtu.be/j3xQquRRdNU

#### Meta PARTNR: Unlocking Human-Robot Collaboration

More details on Meta FAIR's latest advancements supporting Advanced Machine Intelligence (AMI): BLOG LINK

There is an immense amount of potential for innovation and development in the field of human-robot collaboration — and we’re excited to release Meta PARTNR, a research framework that includes a large-scale benchmark, dataset and large planning model to jump start additional research in this exciting field.

PARTNR website: https://aihabitat.org/partnr/
PARTNR repo on GitHub: https://github.com/facebookresearch/partnr-planner

https://youtu.be/JJX_U35xa7k

#### Simulation took Control of my Robot Arm (NVIDIA Isaac Sim)

<!-- I have made the Robot Arm simulation inside the NVIDIA Isaac Sim, and I've let this simulation to control my physical real robot. Like this I can use advanced NVIDIA algorithms (including AI) with my DIY Robot Arm.

Files for this video: https://github.com/SkyentificGit/YellowArmSim

https://youtu.be/Eb2zuQxOBlY -->

#### NVIDIA Isaac GR00T N1: An Open Foundation Model for Humanoid Robots
<!-- NVIDIA Isaac GR00T N1 is a generalist foundation model for #humanoid #robots, built on the foundations of synthetic data generation and learning in simulation. 

GR00T N1 features a dual-system architecture for thinking fast and slow, inspired by principles of human cognitive processing. This architecture allows the robot to perceive and reason about its environment and instructions, and then translate those plans into precise and continuous actions. GR00T N1's generalization capabilities enable robots to manipulate common objects with ease and execute multi-step sequences collaboratively. 

Around the world, robotics developers are using NVIDIA's 3 computers to build the next generation of #embodiedAI.

Read the Press Release: https://nvidianews.nvidia.com/news/nvidia-isaac-gr00t-n1-open-humanoid-robot-foundation-model-simulation-frameworks?ncid=so-yout-861275-vt48
Learn more about Isaac GR00T: https://nvda.ws/41YEXCO 

https://youtu.be/m1CH-mgpdYg

https://github.com/NVIDIA/Isaac-GR00T/
https://huggingface.co/nvidia/GR00T-N1-2B -->


#### Training Surgical Robots with NVIDIA Isaac for Healthcare

<!-- Getting started with NVIDIA Isaac for Healthcare: c

NVIDIA Isaac for Healthcare is an AI robotic development platform of AI models, simulation frameworks and synthetic data generation pipelines, and accelerated runtime libraries purpose built for healthcare robots. 

Isaac for Healthcare includes:

AI Models fine-tuned for healthcare robotics, including vision language action models adapted for autonomous ultrasound and 3D diffusion model, ACT(Action chunk transformer) multi-camera models for surgical sub task automation. 

Simulation & synthetic data generation pipelines providing -developers with a physics-based digital twin framework  to import custom medical sensors, robots, instruments, and anatomies to teach robots how to respond to various scenarios through reinforcement or imitation learning. 

Seamless deployment on NVIDIA Holoscan leveraging accelerated runtime libraries to power the brains of robots in the real world. This enables seamless integration of AI training and simulation capabilities with edge AI computing, facilitating the development of medical robotic systems that require precise, real time decision making. 

https://youtu.be/v1hdsGUHAgE -->

#### Accelerating AV Development With NVIDIA Omniverse and Cosmos

#NVIDIAOmniverse and #NVIDIACosmos enable developers to build the next generation of end-to-end autonomy. With APIs and services for high-fidelity simulation and a suite of world foundation models and development tools, developers can perform model distillation, closed-loop training, and synthetic data generation to accelerate #autonomousvehicle development.

#### How to build Humanoid: NVIDIA Isaac Lab, how to walk

<!-- New project: we are going to build a real bipedal Robot! I never made legs before, but it should be similar to the arms :). And this time, I have decided to start with the simulation. For this we are going to use the state of the art software: NVIDIA Isaac Sim with Isaac Lab. For the hardware I am going to use MyActuator custom actuators (https://www.myactuator.com).

Special thanks to my special supporters Cam Smith and Cisse Majhtar!

Please subscribe. This will help me to develop other projects like this, to bring the bright future closer! 

https://youtu.be/xwOaStX0mxE -->

#### How Robots Learn to Be Robots: Training, Simulation, and Real World Deployment

<!-- Everything that moves will be autonomous and #physicalAI will embody robots of every kind. 

Explore the continuous loop of #robot AI simulation, training, testing, and real-world experience powered by 3 computers built by #NVIDIA that are driving the physical AI revolution. 

With NVIDIA Omniverse and Cosmos, developers can generate massive amounts of diverse, synthetic data for training robot policies. Amazing things are born in simulation. -->

#### NVIDIA Just Changed Robotics Forever With GR00T N1 – See It in Action!

NVIDIA has just unveiled the Isaac GR00T N1, a foundation model that is revolutionizing humanoid robotics. This AI-driven system can learn tasks, make decisions, and adapt like never before!

At GTC 2025, NVIDIA CEO Jensen Huang revealed the Isaac GR00T N1, a next-generation AI model designed to train humanoid robots with unprecedented efficiency. It uses a dual-system approach—one for instant reactions and another for strategic thinking. NVIDIA also introduced Newton, a physics engine developed in collaboration with Google DeepMind and Disney, aiming to enhance robotic motion.

Additionally, NVIDIA’s Isaac GR00T Blueprint enables large-scale training with synthetic data. In just 11 hours, the system generated over 780,000 training examples, drastically improving robot accuracy. These advancements could reshape industries by making humanoid robots more intelligent and useful in real-world applications.

What do you think of NVIDIA’s latest robotics breakthrough? Let us know in the comments! Do not forget to like, subscribe, and turn on notifications for more updates on AI and robotics.

https://youtu.be/i2S2EwNSdhA

#### [Isaac Sim] Setup VR/AR/XReality for Robotics Application

This video shows you how to setup VR/AR and Mixed Reality experiences compatible with various headsets like Apple Vision Pro, HTC Vive,and  Meta Quest. You can also use Augmented Reality on your tablet or smartphone devices.

Timestamp:
0:00 Intro
0:24 Setup
2:19 Augmented Reality
6:06 Virtual Reality
8:55 More examples

https://youtu.be/UnNun7GWFVs

#### ROS 2 Workflows | Isaac Sim Office Hours

Join us for our monthly Isaac Sim office hours, where we’ll answer your questions and guide you through ROS 2 workflows. Whether you’re new to our technology or looking to deepen your understanding, this session is designed to provide you with hands-on support and insights. Don’t miss the chance to interact with our experts and enhance your robotics skills.

https://youtu.be/XJ_b4AvYrj0

#### Sanctuary AI Sim-to-Real Transfer

Sanctuary AI has demonstrated industry-leading sim-to-real transfer of learned dexterous manipulation policies for our unique, high degree-of-freedom, high strength, and high speed hydraulic hands. 

“Our proprietary robotic hand is differentiated by a high number of active degrees of freedom, allowing for finger abduction and therefore in-hand manipulation, a feature not offered by other commercially available hands. We are unique in our use of hydraulic actuation, providing high strength, speed and controllability. Our miniaturized hydraulic valves provide a path to achieving human-level dexterity.”

https://www.sanctuary.ai/blog/sanctuary-ai-leverages-isaac-lab

https://youtu.be/2W9GbJ2yEgc

#### Apple nos presenta tu próximo Robot

西班牙语

En un movimiento nada usual por parte de Apple, hemos conocido una investigación sobre robótica que ha marcado un camino muy interesante sobre cómo Apple entiende este nuevo producto en el que, ahora, sabemos de manera "casi oficial" que está trabajando para el futuro (puede que no tan lejano).

En este programa analizamos los papers de los modelos de inteligencia artificial ELEGNT y EMOTION, que permite expresividad no verbal tanto en robots antropomórficos (con aspecto humano) como no antropomórficos (con otras formas, como un brazo robótico tipo Luxo Jr. de Pixar).

Descubre cómo lo han conseguido, en qué consisten sus investigaciones y qué puede suponer para futuros productos que se rumorean que podrían llegar incluso este mismo año.

https://youtu.be/C4qsFH4k0mM

#### ELEGNT: Expressive and Functional Movement Design for Non-anthropomorphic Robot

*Hu et al.'s paper introduces ELEGNT, a framework designed to enhance human-robot interaction by incorporating expressive movements into non-anthropomorphic robots.* The authors argue robots should not only achieve functional tasks but also express intentions, attention, and emotions. They present a lamp-like robot as a case study, exploring how movements can be designed with both functional and expressive utilities. *A user study showed expression-driven movements significantly improve user engagement and perception of the robot, particularly in social contexts.* The study also shows the importance of the robot's non-verbal actions and behaviors. The authors advocate for considering movement as a key design element, drawing inspiration from animation and behavioral science.

https://youtu.be/eeT5M3GNFSY

### Reinforcement Learning in Isaac Lab 合集
#### Reinforcement Learning in Isaac Lab: Interactive Scene (Tutorial 1)

Learn how to teach robots with Reinforcement Learning in Isaac Lab. 
Isaac Lab, previously Isaac Gym, is built on top of NVIDIA Omniverse Isaac Sim.

https://youtu.be/Y-K1cAvnSFI

#### Reinforcement Learning in Isaac Lab: Manager Workflow (Tutorial 2)

Learn how to create Reinforcement Learning Environments with the Manager Workflow. 
https://youtu.be/oss4n1NWDKo

#### Master 3D Character Animation with OpenUSD and iClone Connector

Join us for an exciting livestream with John Martin, Reallusion's VP of Marketing, as we dive into character creation and explore the Omniverse Live Sync plugin! Get ready for a fun and informative session where we’ll showcase the powerful tools and techniques for bringing your characters to life in real-time. Don't miss out!

https://youtu.be/-uQuiOWvbEk

#### OpenUSD 101 for Beginners

Universal Scene Description expert Jen Borucki invites you to a sneak preview of her upcoming OpenUSD curriculum. Learn the foundational best practices for developing with this open-source library and framework, poised to become the standard for the 3D internet. Don't miss this live preview and the chance to enroll in the course.

https://youtu.be/SPbnnSxAyKw

#### Insights Into Disney’s Robotic Character Platform [S72595]
<!-- At Disney, we're redefining entertainment robotics from the ground up and are building artist-centric tools that provide creative control of dynamic character motion. Gain insight into our robotic character platform that enables the rapid design of freely roaming robotic characters like our BDX Droids. See how our characters learn to imitate artist-specified animations with a combination of deep reinforcement learning and GPU-accelerated simulation. Learn how we get our characters to perform highly challenging motions at the limit of hardware, and see the first results of bringing engaging autonomous interactions to our characters.

GTC 2025 Session
https://register.nvidia.com/flow/nvidia/gtcs25/vap/page/vsessioncatalog/session/1727466535152001CsBe -->

#### Accelerate Robotics Development With Digital Twins: Strategic Insights for Innovators [S73891]

Explore how Idealworks builds on NVIDIA Isaac and Omniverse to revolutionize robotics through advanced simulation and synthetic data generation. We'll delve into how idealworks applies digital twins in single-robot use cases and advanced research. Learn about the practical use of vSLAM mapping, depth estimation with extended service set networks, and real-time 3D perception using nvblox in Isaac Perceptor. We’ll also discuss the creation of synthetic datasets using Omniverse extensions, accelerating our development cycles. Discover why companies should invest in simulation for industrial robotics development to save time, reduce costs, and streamline development.
Key Takeaways:
Understand the application of digital twins in robotics, from single-robot scenarios to advanced research
Learn about the latest NVIDIA technologies driving innovation in robotics
Explore how idealworks leverages digital twins within its operating system and broader ecosystem
Discover the benefits of synthetic data generation in enhancing simulation accuracy and robotics development

https://register.nvidia.com/flow/nvidia/gtcs25/vap/page/vsessioncatalog/session/1733480757026001lbmo

#### AI for Humanoid Robots

Pieter Abbeel, Professor, University of California, Berkeley

Humanoid robotics hardware is seeing a major acceleration in progress. It's reasonable to expect commercially viable humanoid hardware to exist within three years. The question is, how do we build the AI that can make these robots helpful? Humanoids have high dimensional action spaces and high dimensional multimodal observation space, posing challenges to existing paradigms. We'll explore these challenges in more depth and highlight some promising paths forward.

https://register.nvidia.com/flow/nvidia/gtcs25/vap/page/vsessioncatalog/session/1727997356608001TDTS

#### A New Path to Embodied AI

Deepak Pathak, CEO and Co-Founder, Skild AI

Building general-purpose embodied intelligence has been a core goal of AI since its inception 70 years ago. The field has evolved through multiple hypotheses, beginning with search as a solution, followed by knowledge-based systems, and, most recently, leveraging large language models for robotics. Yet, the challenge of creating generalist embodied agents capable of performing thousands of tasks across diverse environments remains unsolved. Building such a generalist robotic agent presents a "chicken-and-egg" problem: to train agents for generalization, we need vast amounts of robotic/agent data from varied environments, but gathering such data is impractical without deploying robots/agents that already generalize. I propose a new framework that leverages alternative sources of "indirect" supervision to build robotic agents from the ground up. I'll focus on three key principles: (1) How to generate self-supervised data for training robots? (2) How to bootstrap robot learning by observing human videos? and (3) How can robots learn to adapt via large-scale training in simulation? I'll demonstrate this framework's potential for scaling robotics AI through several case studies.
Bottom-Up Philosophy for Developing Intelligence: Instead of programming in a phenomenon, think about how it could have emerged
Large-scale training in simulations is a way to solve data problems in robotics AI
Videos of human interaction (from YouTube, Flickr, etc.) provide a rich and diverse signal for training embodied agents
Rapid adaptation in the real world is critical for generalization in the real world

https://register.nvidia.com/flow/nvidia/gtcs25/vap/page/vsessioncatalog/session/1727466268139001abNd

#### From Simulation to Reality: Building Wheeled Robots with Isaac Lab
In this video, I had the chance to interview Tyler from the University of Washington to dive deep into Wheeled Lab — a robotics project that brings low-cost wheeled robots from simulation to real-world deployment using Isaac Sim and Isaac Lab, part of NVIDIA’s Omniverse platform.

I had the chance to collaborate with the team, you’ll find full step-by-step tutorials on how to recreate this project yourself at:
https://lycheeai-hub.com/isaac-lab/projects/wheeled-lab
Their Github:
https://github.com/UWRobotLearning/WheeledLab

https://youtu.be/Y4b-cz2xB1w

#### Reinforcement Learning Fundamentals | Isaac Lab Office Hours

Join us for our monthly Isaac Lab Office Hours, where we’ll dive into the fundamentals of reinforcement learning and answer your questions about Isaac Lab workflows. Whether you’re just getting started or looking to deepen your understanding, this session is designed to give you hands-on support, practical insights, and direct access to our experts. Don’t miss the opportunity to boost your robotics skills and explore how reinforcement learning can be applied within Isaac Lab.

https://youtu.be/nxxzuDqJCzQ


#### Simulate Robot Fleets in Digital Twins
In this livestream, we’ll introduce the Mega NVIDIA Omniverse Blueprint for helping developers build, test, and optimize intelligent robot fleets using digital twins. You’ll learn what Mega is, why it matters, and how it brings a software-defined approach to real-world facilities. We’ll walk through a live example, show how to get started, and highlight real-world use cases from partners like Foxconn.

https://youtu.be/93Is26tQ07o

#### Delta’s AI-Integrated Robot and D-Bot With Omniverse (Presented by Delta Electronics) [S73977]

, Sr. Director, IABG at Delta Electronics EMEA, Delta Electronics

Discover how Delta leverages Omniverse Isaac Sim to design, optimize, train, and deploy autonomous control systems for its robots and D-bots, enhancing efficiency and performance in smart factory automation.
Key Takeaways:
Introduction to Delta Robot and D-Bot solution
Integration of Delta’s physical robots/D-bots in the Omniverse platform
Benefits and achievement

#### Intelligent Industrial Robotics: From Simulation to Robot Low-Code Deployment [S71957]

Head of Demonstrator Development, German Research Center for Artificial Intelligence

, Platform Architect, Yaskawa Europe GmbH

Discover how collaborative robotics transforms industrial environments through NVIDIA Isaac Sim-based vision AI training and Yaskawa robots with integrated NVIDIA Jetson. See how encapsulated "skills," deployed as containers, enable low-code creation of complex robot programs, paving the way for new business models like skill providers and software as a service. Learn how these specialized skills can be combined by AI into intelligent robot solutions based on behavior trees.
Key Takeaways:
Simulation to Reality: Vision AI trained in NVIDIA Isaac Sim and deployed on Yaskawa robots for real-world applications
Containerized Skills: Modular "skills" deployed as containers, enabling flexible and intelligent robot functionalities
Low-Code Programming: Creation of complex robot programs through low-code approaches, simplifying development for non-experts
New Business Models: Opportunities for skill providers and software as a service in collaborative robotics

https://register.nvidia.com/flow/nvidia/gtcs25/vap/page/vsessioncatalog/session/1726156736971001jzCJ


#### The Promise of Humanoid Robots: Research vs. the Real World [S72592]

Aaron Saunders, CTO, Boston Dynamics

The promise of humanoid robots performing complex tasks alongside humans has captivated imaginations for decades. We've seen remarkable progress in areas like bipedal locomotion, manipulation, and even basic task learning. However, the crucial question remains: are we truly seeing progress in terms of practical, deployable humanoid solutions, or are we primarily observing results in isolated research settings? We'll delve into this critical distinction, examining the current state of humanoid robotics and identifying the gap between laboratory demonstrations and real-world applications. We'll analyze key challenges hindering widespread adoption, including robustness in unstructured environments, cost-effectiveness, and the critical need for advanced AI capable of genuine adaptability and problem-solving. We'll also explore promising avenues for bridging this gap. By critically assessing both the achievements and the limitations of current humanoid technology, we'll foster a realistic perspective on the future of humanoids in the workforce and identify the key steps needed to translate exciting progress into tangible results.

https://register.nvidia.com/flow/nvidia/gtcs25/vap/page/vsessioncatalog/session/1727465141102001g41h

#### 加速仿真：Omniverse SimReady 3D 资产自动化构建方法研究与应用 [S72545]

, Product Director, Bytedance

在本次演讲中，我们将深入探讨 SimReady 3D 资产的自动化构建方法研究与应用。SimReady 资产是 NVIDIA 率先提出的一种资产定义，具有准确的语义标签、物理属性和关节结构等真实信息，能够广泛应用于虚拟仿真、合成数据生成等生产工作流中，是数字孪生技术的核心组成部分。为实现这一目标，我们将从现有 3D 美术资产和 Gen3D 生成资产的语义标签、基于 AI 的物理属性推理以及关节结构生成三个方面展开讨论：首先，我们将使用 VLM 技术自动化识别 3D 美术资产的语义内容，并为其添加语义标签；其次，探讨如何推理出符合物理属性的 3D 美术资产，并进一步将其应用到 Gen3D 合成资产的属性自动生成上，最后将这些属性添加至 OpenUSD 文件中；最后，我们将讨论如何通过自动化手段将 3D 美术资产加工成具备关节结构的资产，以及如何直接合成带有关节结构的 Gen3D 资产。通过这些创新方法，我们旨在为 SimReady 3D 资产的构建提供一条自动化路径，推动 NVIDIA Omniverse 在更多场景中的广泛应用。
Key Takeaways:
自动化语义标签识别与赋值：通过 VLM 等技术，自动识别 3D 美术资产的语义内容，并为其添加准确的语义标签
物理属性推理与生成：探讨如何基于 AI 推理出 3D 美术资产的物理属性，并实现 Gen3D 合成资产的属性自动生成与 OpenUSD 导入
关节结构生成与加工：研究如何通过自动化手段将 3D 美术资产加工成具备关节结构的资产，并探索如何直接生成带有关节结构的 Gen3D 资产

https://register.nvidia.com/flow/nvidia/gtcs25/vap/page/vsessioncatalog/session/1727420481648001XMBg

#### Training a Robot from Scratch in Simulation, from URDF to OpenUSD

Learn how to train robots from scratch using simulation. In this session, Muammer Bay from LycheeAI walks through each step for training the SO-100 arm — starting from a basic URDF model and transforming it into a fully functional, trainable agent using Isaac Sim and Isaac Lab. Whether you're new to robotics or looking to train custom agents, you'll gain practical insights to build your own simulation-to-training pipeline. 

https://youtu.be/_HMk7I-vSBQ


喜欢一键三连哦，关注我了解更多具身智能创作。

仔细分析和阅读这篇论文，总结文章中的方法，假如有机会向作者提问，提几个你关心的问题？

根据字幕文件和上述描述，生成简短的视频简介，并给视频起一个专业的标题。一定要仔细阅读上传的这个字幕文件。